{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"VLM OCR Pipeline","text":"<p>A modular document processing system that combines layout detection (DocLayout-YOLO, MinerU, olmOCR, PaddleOCR) with Vision Language Models (OpenAI, Gemini, PaddleOCR-VL) for intelligent text extraction and correction.</p> <p>Based On</p> <p>This project is based on and modified from Versatile-OCR-Program</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#modular-architecture","title":"\ud83c\udfaf Modular Architecture","text":"<ul> <li>Flexible Detector/Sorter Combinations: Mix and match detectors (DocLayout-YOLO, PaddleOCR PP-DocLayoutV2, MinerU, olmOCR) with sorters (multi-column, LayoutReader, XY-Cut, VLM)</li> <li>8-Stage Pipeline: Document loading \u2192 Detection \u2192 Ordering \u2192 Recognition \u2192 Block correction \u2192 Rendering \u2192 Page correction \u2192 Output</li> <li>Unified BBox System: Integer-based bounding boxes (internal: xyxy, JSON: xywh) with automatic conversion between 6+ formats</li> </ul>"},{"location":"#multiple-recognition-backends","title":"\ud83e\udd16 Multiple Recognition Backends","text":"<ul> <li>Cloud VLM APIs: OpenAI GPT-4 Vision, Gemini 2.5 Flash</li> <li>Local VLM: PaddleOCR-VL-0.9B (0.9B parameters, NaViT + ERNIE-4.5-0.3B)</li> <li>109+ Languages: Extensive multilingual support</li> </ul>"},{"location":"#advanced-document-understanding","title":"\ud83d\udcc4 Advanced Document Understanding","text":"<ul> <li>Layout Detection: Automatically detects text, tables, figures, equations, lists</li> <li>Reading Order Analysis: Multi-column detection, LayoutLMv3, XY-Cut, VLM-based ordering</li> <li>Special Content Processing: Enhanced analysis of tables and figures with structured output</li> <li>AI-Powered Correction: Intelligent text correction using VLMs</li> </ul>"},{"location":"#performance-efficiency","title":"\u26a1 Performance &amp; Efficiency","text":"<ul> <li>Intelligent Caching: Content-based hashing to avoid reprocessing</li> <li>Rate Limiting: Automatic throttling for Gemini API (free/tier1/tier2/tier3)</li> <li>Batch Processing: Process entire directories of PDFs</li> <li>Model-Specific Prompts: YAML-based prompt templates for optimal results</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># Basic usage with Gemini\npython main.py --input document.pdf --backend gemini\n\n# Advanced: Custom detector + sorter + recognizer\npython main.py --input doc.pdf \\\n    --detector paddleocr-doclayout-v2 \\\n    --sorter mineru-layoutreader \\\n    --recognizer paddleocr-vl\n\n# Batch processing with page limits\npython main.py --input pdfs/ --backend openai --max-pages 5\n</code></pre>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph LR\n    A[Input PDF/Image] --&gt; B[Stage 1: Load Document]\n    B --&gt; C[Stage 2: Layout Detection]\n    C --&gt; D[Stage 3: Reading Order]\n    D --&gt; E[Stage 4: Text Recognition]\n    E --&gt; F[Stage 5: Block Correction]\n    F --&gt; G[Stage 6: Markdown Rendering]\n    G --&gt; H[Stage 7: Page Correction]\n    H --&gt; I[Stage 8: Save Output]\n</code></pre> <p>Each stage is modular and can be configured independently:</p> <ul> <li>Detection: <code>doclayout-yolo</code>, <code>paddleocr-doclayout-v2</code>, <code>mineru-vlm</code>, <code>olmocr-vlm</code></li> <li>Ordering: <code>pymupdf</code>, <code>mineru-layoutreader</code>, <code>mineru-xycut</code>, <code>mineru-vlm</code>, <code>olmocr-vlm</code></li> <li>Recognition: <code>openai</code>, <code>gemini</code>, <code>paddleocr-vl</code></li> </ul>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li> <p>:material-download:{ .lg .middle } Installation</p> <p>Install VLM OCR Pipeline in minutes</p> <p>:octicons-arrow-right-24: Getting started</p> </li> <li> <p>:material-book-open-variant:{ .lg .middle } User Guides</p> <p>Learn about BBox formats, detectors, and advanced usage</p> <p>:octicons-arrow-right-24: User Guides</p> </li> <li> <p>:material-api:{ .lg .middle } API Reference</p> <p>Explore the complete API documentation</p> <p>:octicons-arrow-right-24: API Reference</p> </li> <li> <p>:material-file-code:{ .lg .middle } Architecture</p> <p>Deep dive into pipeline stages and design patterns</p> <p>:octicons-arrow-right-24: Architecture</p> </li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub: NoUnique/vlm-ocr-pipeline</li> <li>Issues: Report bugs or request features</li> <li>Contributing: See our Contributing Guide</li> </ul>"},{"location":"AUTO_OPTIMIZATION_DESIGN/","title":"GPU Auto-Optimization Design","text":"<p>Core Principle: Zero-configuration optimization - the system automatically applies optimal settings based on detected GPU environment without any user intervention.</p>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#1-problem-statement","title":"1. Problem Statement","text":""},{"location":"AUTO_OPTIMIZATION_DESIGN/#manual-configuration-required-before","title":"Manual Configuration Required (Before)","text":"<pre><code># Too many manual settings required\npython main.py --input doc.pdf \\\n    --recognizer deepseek-ocr \\\n    --recognizer-backend vllm \\           # Manual 1\n    --tensor-parallel-size 4 \\            # Manual 2\n    --data-parallel-workers 2 \\           # Manual 3\n    --gpu-memory-utilization 0.90 \\       # Manual 4\n    --batch-size 8                        # Manual 5\n</code></pre> <p>Problem: Users must understand GPU environment and know optimal settings.</p>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#2-goal-complete-automation","title":"2. Goal: Complete Automation","text":""},{"location":"AUTO_OPTIMIZATION_DESIGN/#ideal-user-experience","title":"Ideal User Experience","text":"<pre><code># Optimal performance without any configuration\npython main.py --input doc.pdf --recognizer deepseek-ocr\n\n# Output:\n# \ud83d\udd0d Detecting GPU environment...\n# \u2705 Found 8x NVIDIA A100-SXM4-80GB (640GB total)\n# \ud83d\ude80 Auto-optimizing for maximum performance...\n#    Backend: vLLM (tensor parallel)\n#    Tensor Parallel Size: 4\n#    Data Parallel Workers: 2\n#    GPU Memory Utilization: 90%\n#    Flash Attention: Enabled\n# \u26a1 Expected speedup: 12x (vs single GPU)\n</code></pre> <p>User specifies nothing - system automatically optimizes</p>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#3-implementation-design","title":"3. Implementation Design","text":""},{"location":"AUTO_OPTIMIZATION_DESIGN/#31-architecture","title":"3.1 Architecture","text":"<pre><code>[User Input] \u2192 [Auto-Optimizer] \u2192 [Optimized Execution]\n                      \u2193\n          [GPU Environment Detector]\n</code></pre>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#32-core-components","title":"3.2 Core Components","text":""},{"location":"AUTO_OPTIMIZATION_DESIGN/#component-1-gpu-environment-detector-singleton","title":"Component 1: GPU Environment Detector (Singleton)","text":"<p>File: <code>pipeline/gpu_environment.py</code></p> <pre><code>@dataclass\nclass GPUConfig:\n    \"\"\"Auto-generated optimal GPU configuration.\"\"\"\n\n    # Detected environment\n    has_cuda: bool\n    gpu_count: int\n    total_memory_gb: float\n    compute_capability: tuple[int, int]\n\n    # Auto-optimized settings\n    recommended_backend: str  # \"vllm\", \"hf\", \"pytorch\"\n    tensor_parallel_size: int\n    data_parallel_workers: int\n    gpu_memory_utilization: float\n    use_flash_attention: bool\n    use_bf16: bool\n\n    optimization_strategy: str  # \"hybrid_tp4_dp2\", etc.\n    expected_speedup: float  # vs single GPU sequential\n\n\n@lru_cache(maxsize=1)\ndef get_gpu_config() -&gt; GPUConfig:\n    \"\"\"Get cached GPU configuration (singleton).\n\n    Called once at startup and cached.\n    All components reference this single source of truth.\n    \"\"\"\n    # Detect GPU environment\n    # Calculate optimal settings\n    # Return configuration\n</code></pre> <p>Detection Rules: - 8x A100 80GB \u2192 vLLM TP=4, DP=2 (hybrid, 12x speedup) - 4x A100 80GB \u2192 vLLM TP=2, DP=2 (hybrid, 6x speedup) - 2x A100 80GB \u2192 hf-ray DP=2 (data parallel, 2.5x speedup) - 1x A100 80GB \u2192 vLLM TP=1 (single GPU, 1.5x speedup) - No CUDA \u2192 pytorch CPU fallback (0.1x speedup)</p>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#component-2-recognizer-auto-optimization","title":"Component 2: Recognizer Auto-Optimization","text":"<p>File: <code>pipeline/recognition/__init__.py</code></p> <pre><code>def create_recognizer(\n    name: str,\n    backend: str | None = None,  # None = auto-select\n    use_auto_optimization: bool = True,\n    **kwargs: Any,\n) -&gt; Recognizer:\n    \"\"\"Create recognizer with auto-optimization.\n\n    Auto-optimization (when enabled):\n    - Auto-select backend based on GPU environment\n    - Auto-configure tensor_parallel_size\n    - Auto-configure gpu_memory_utilization\n    - Auto-select device placement\n    \"\"\"\n    if use_auto_optimization:\n        gpu_config = get_gpu_config()\n\n        # Auto-select backend if not specified\n        if backend is None:\n            backend = gpu_config.recommended_backend\n\n        # Inject auto-optimized settings (if not manually overridden)\n        if \"tensor_parallel_size\" not in kwargs:\n            kwargs[\"tensor_parallel_size\"] = gpu_config.tensor_parallel_size\n\n        if \"gpu_memory_utilization\" not in kwargs:\n            kwargs[\"gpu_memory_utilization\"] = gpu_config.gpu_memory_utilization\n\n    return _RECOGNIZER_REGISTRY[name](**kwargs)\n</code></pre>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#4-user-scenarios","title":"4. User Scenarios","text":""},{"location":"AUTO_OPTIMIZATION_DESIGN/#scenario-1-fully-automatic-recommended","title":"Scenario 1: Fully Automatic (Recommended)","text":"<pre><code>python main.py --input doc.pdf --recognizer deepseek-ocr\n</code></pre> <p>Internal Flow: 1. Detect GPU environment: 8x A100 80GB 2. Auto-select backend: vLLM 3. Auto-calculate settings: TP=4, DP=2 4. Expected speedup: 12x</p>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#scenario-2-partial-override","title":"Scenario 2: Partial Override","text":"<pre><code># Specify backend only, rest auto-optimized\npython main.py --input doc.pdf \\\n    --recognizer deepseek-ocr \\\n    --recognizer-backend hf\n</code></pre> <p>Internal Flow: 1. Detect GPU environment: 8x A100 80GB 2. Backend: hf (user-specified) 3. Auto-calculate settings: device_map=\"auto\", TP=4</p>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#scenario-3-advanced-users-fully-manual","title":"Scenario 3: Advanced Users (Fully Manual)","text":"<pre><code># Disable auto-optimization\npython main.py --input doc.pdf \\\n    --recognizer deepseek-ocr \\\n    --recognizer-backend vllm \\\n    --tensor-parallel-size 2 \\\n    --use-auto-optimization=False\n</code></pre>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#scenario-4-cpu-environment-auto-fallback","title":"Scenario 4: CPU Environment (Auto Fallback)","text":"<pre><code>python main.py --input doc.pdf --recognizer deepseek-ocr\n</code></pre> <p>Internal Flow: 1. Detect GPU environment: No CUDA 2. Auto-select backend: pytorch (CPU) 3. Log warning: Performance will be 10x slower</p>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#5-override-priority","title":"5. Override Priority","text":"<pre><code>1. Explicit CLI arguments (highest priority)\n   \u2514\u2500&gt; --tensor-parallel-size 8\n\n2. Environment variables\n   \u2514\u2500&gt; VLLM_TENSOR_PARALLEL_SIZE=4\n\n3. Configuration files\n   \u2514\u2500&gt; config/gpu_optimization.yaml\n\n4. Auto-detection (default)\n   \u2514\u2500&gt; gpu_config.recommended_tensor_parallel_size\n</code></pre>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#6-key-differences","title":"6. Key Differences","text":""},{"location":"AUTO_OPTIMIZATION_DESIGN/#before-problem","title":"Before (Problem)","text":"<pre><code># User must know everything\nllm = LLM(\n    model=model_name,\n    tensor_parallel_size=???,  # User must calculate?\n    gpu_memory_utilization=???,  # How much is safe?\n)\n</code></pre>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#after-automatic","title":"After (Automatic)","text":"<pre><code># Auto-optimized based on GPU environment\ngpu_config = get_gpu_config()  # Automatic detection\n\nllm = LLM(\n    model=model_name,\n    tensor_parallel_size=gpu_config.tensor_parallel_size,  # Automatic\n    gpu_memory_utilization=gpu_config.gpu_memory_utilization,  # Automatic\n)\n</code></pre>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#7-expected-performance-8x-a100-80gb","title":"7. Expected Performance (8x A100 80GB)","text":"User Input Auto Settings Processing Time Speedup <code>python main.py --input doc.pdf</code> vLLM TP=4, DP=2 4.2 min 12x (no manual configuration) Auto Ray parallelization"},{"location":"AUTO_OPTIMIZATION_DESIGN/#8-design-philosophy","title":"8. Design Philosophy","text":"<ol> <li>Zero-configuration by default - Users should not need to understand GPU architecture</li> <li>Expert override available - Advanced users can still manually configure</li> <li>Fail-safe fallbacks - Always degrade gracefully (GPU \u2192 CPU)</li> <li>Single source of truth - One singleton <code>GPUConfig</code> referenced everywhere</li> <li>Logged transparency - Users see what the system auto-selected</li> </ol> <p>Core Concept: Users type <code>python main.py --input doc.pdf</code> and the system automatically finds and applies optimal settings.</p>"},{"location":"CI_CD/","title":"CI/CD Pipeline","text":"<p>This document describes the Continuous Integration and Continuous Deployment (CI/CD) workflows for the VLM OCR Pipeline project.</p>"},{"location":"CI_CD/#overview","title":"Overview","text":"<p>The project uses GitHub Actions for automated testing, linting, type checking, and documentation deployment. All workflows are defined in <code>.github/workflows/</code>.</p>"},{"location":"CI_CD/#workflows","title":"Workflows","text":""},{"location":"CI_CD/#1-main-ci-workflow-githubworkflowsciyml","title":"1. Main CI Workflow (<code>.github/workflows/ci.yml</code>)","text":"<p>Runs on every push to <code>main</code>/<code>develop</code> branches and on pull requests.</p>"},{"location":"CI_CD/#jobs","title":"Jobs","text":"<p>Lint and Format Check - Checks code formatting with <code>ruff format --check</code> - Runs linting with <code>ruff check</code> - Ensures code style consistency</p> <p>Type Check - Runs <code>pyright</code> for static type checking - Validates type annotations across the codebase - Uses Node.js 20 for <code>npx pyright</code></p> <p>Test - Runs pytest test suite - Tests on Python 3.11 and 3.12 - Uploads coverage reports to Codecov (optional) - Uses matrix strategy for multi-version testing</p> <p>Build Documentation - Builds MkDocs documentation with <code>--strict</code> flag - Catches documentation errors before deployment - Validates all documentation links and references</p> <p>All Checks Passed - Final gate that requires all previous jobs to succeed - Provides clear status for PR mergeability</p>"},{"location":"CI_CD/#trigger-events","title":"Trigger Events","text":"<pre><code>on:\n  push:\n    branches: [main, develop]\n  pull_request:\n    branches: [main, develop]\n  workflow_dispatch:  # Manual trigger\n</code></pre>"},{"location":"CI_CD/#2-pr-checks-workflow-githubworkflowspr-checksyml","title":"2. PR Checks Workflow (<code>.github/workflows/pr-checks.yml</code>)","text":"<p>Provides automated analysis and labeling for pull requests.</p>"},{"location":"CI_CD/#jobs_1","title":"Jobs","text":"<p>PR Information - Analyzes changed files (Python, tests, docs, workflows) - Counts lines added/deleted - Posts summary comment on PR - Updates existing comment on new commits</p> <p>Size Label - Automatically labels PRs by size:   - <code>size/XS</code>: &lt; 50 lines   - <code>size/S</code>: 50-200 lines   - <code>size/M</code>: 200-500 lines   - <code>size/L</code>: 500-1000 lines   - <code>size/XL</code>: &gt; 1000 lines</p>"},{"location":"CI_CD/#permissions","title":"Permissions","text":"<pre><code>permissions:\n  pull-requests: write\n  contents: read\n  checks: write\n</code></pre>"},{"location":"CI_CD/#3-documentation-deployment-githubworkflowsdocsyml","title":"3. Documentation Deployment (<code>.github/workflows/docs.yml</code>)","text":"<p>Automatically deploys documentation to GitHub Pages.</p>"},{"location":"CI_CD/#trigger-conditions","title":"Trigger Conditions","text":"<ul> <li>Changes to <code>docs/**</code></li> <li>Changes to <code>mkdocs.yml</code></li> <li>Changes to workflow file itself</li> <li>Manual dispatch</li> </ul>"},{"location":"CI_CD/#deployment","title":"Deployment","text":"<ul> <li>Builds documentation with MkDocs</li> <li>Deploys to <code>gh-pages</code> branch</li> <li>Serves at: <code>https://&lt;username&gt;.github.io/&lt;repo&gt;/</code></li> </ul>"},{"location":"CI_CD/#setup-instructions","title":"Setup Instructions","text":""},{"location":"CI_CD/#1-enable-github-actions","title":"1. Enable GitHub Actions","text":"<p>GitHub Actions should be enabled by default. Verify in repository Settings \u2192 Actions.</p>"},{"location":"CI_CD/#2-configure-github-pages","title":"2. Configure GitHub Pages","text":"<ol> <li>Go to repository Settings \u2192 Pages</li> <li>Source: Deploy from a branch</li> <li>Branch: <code>gh-pages</code> / <code>root</code></li> <li>Save</li> </ol>"},{"location":"CI_CD/#3-add-secrets-optional","title":"3. Add Secrets (Optional)","text":"<p>For coverage reports: <pre><code>Settings \u2192 Secrets \u2192 Actions \u2192 New repository secret\nName: CODECOV_TOKEN\nValue: &lt;your-codecov-token&gt;\n</code></pre></p>"},{"location":"CI_CD/#4-branch-protection-recommended","title":"4. Branch Protection (Recommended)","text":"<p>Protect <code>main</code> branch with required status checks:</p> <ol> <li>Settings \u2192 Branches \u2192 Add rule</li> <li>Branch name pattern: <code>main</code></li> <li>Require status checks before merging:</li> <li>\u2705 Lint and Format Check</li> <li>\u2705 Type Check</li> <li>\u2705 Test (3.11)</li> <li>\u2705 Test (3.12)</li> <li>\u2705 Build Documentation</li> <li>\u2705 All Checks Passed</li> <li>Require pull request reviews</li> <li>Save changes</li> </ol>"},{"location":"CI_CD/#local-development-workflow","title":"Local Development Workflow","text":""},{"location":"CI_CD/#pre-commit-checks","title":"Pre-commit Checks","text":"<p>Before committing, run the pre-commit script:</p> <pre><code>./scripts/pre-commit-check.sh\n</code></pre> <p>This runs the same checks as CI: 1. Code formatting check 2. Linting 3. Type checking 4. Documentation build</p>"},{"location":"CI_CD/#manual-ci-checks","title":"Manual CI Checks","text":"<p>Run individual checks locally:</p> <pre><code># Format check\nuv run ruff format --check .\n\n# Lint\nuv run ruff check .\n\n# Type check\nnpx pyright\n\n# Test\nuv run pytest tests/ -v\n\n# Build docs\nuv run mkdocs build --strict\n</code></pre>"},{"location":"CI_CD/#fix-issues","title":"Fix Issues","text":"<pre><code># Auto-fix formatting\nuv run ruff format .\n\n# Auto-fix linting issues\nuv run ruff check . --fix\n\n# View type errors (no auto-fix)\nnpx pyright\n</code></pre>"},{"location":"CI_CD/#cicd-best-practices","title":"CI/CD Best Practices","text":""},{"location":"CI_CD/#commit-messages","title":"Commit Messages","text":"<p>Use conventional commits format: <pre><code>feat: add new feature\nfix: resolve bug\ndocs: update documentation\ntest: add test coverage\nrefactor: improve code structure\nperf: performance improvement\nci: update CI/CD configuration\n</code></pre></p>"},{"location":"CI_CD/#pull-requests","title":"Pull Requests","text":"<ol> <li>Keep PRs focused and small (prefer size/S or size/M)</li> <li>Ensure all CI checks pass before requesting review</li> <li>Add tests for new features</li> <li>Update documentation for user-facing changes</li> <li>Respond to automated PR comments</li> </ol>"},{"location":"CI_CD/#testing","title":"Testing","text":"<ul> <li>Write tests for all new features</li> <li>Maintain &gt;80% test coverage</li> <li>Include both unit and integration tests</li> <li>Use fixtures for test data</li> </ul>"},{"location":"CI_CD/#type-safety","title":"Type Safety","text":"<ul> <li>Add type annotations to all functions</li> <li>Use <code>from __future__ import annotations</code> for forward references</li> <li>Run <code>npx pyright</code> before committing</li> <li>Fix type errors, don't suppress them unnecessarily</li> </ul>"},{"location":"CI_CD/#troubleshooting","title":"Troubleshooting","text":""},{"location":"CI_CD/#ci-fails-but-local-checks-pass","title":"CI Fails but Local Checks Pass","text":"<ol> <li>Ensure you're using the same Python version (3.11)</li> <li>Check that all dependencies are in <code>requirements.txt</code></li> <li>Clear local cache: <code>rm -rf .venv .ruff_cache .pytest_cache</code></li> <li>Reinstall: <code>uv venv --python 3.11 &amp;&amp; uv pip install -r requirements.txt</code></li> </ol>"},{"location":"CI_CD/#type-check-fails-on-ci","title":"Type Check Fails on CI","text":"<ul> <li>CI uses <code>npx pyright</code> (not global install)</li> <li>Ensure Node.js types are consistent</li> <li>Check for platform-specific type issues</li> </ul>"},{"location":"CI_CD/#tests-timeout","title":"Tests Timeout","text":"<ul> <li>CI has 6-hour timeout per workflow</li> <li>Individual jobs have 360-minute timeout</li> <li>Long-running tests should use appropriate fixtures</li> <li>Consider mocking external API calls</li> </ul>"},{"location":"CI_CD/#documentation-build-fails","title":"Documentation Build Fails","text":"<ul> <li>Run <code>uv run mkdocs build --strict</code> locally</li> <li>Check for broken links in markdown files</li> <li>Verify all referenced files exist</li> <li>Ensure proper YAML frontmatter in docs</li> </ul>"},{"location":"CI_CD/#monitoring","title":"Monitoring","text":""},{"location":"CI_CD/#view-workflow-status","title":"View Workflow Status","text":"<ul> <li>Repository \u2192 Actions tab</li> <li>Click on workflow run to see details</li> <li>Each job shows detailed logs</li> </ul>"},{"location":"CI_CD/#status-badges","title":"Status Badges","text":"<p>Add to README.md:</p> <pre><code>[![CI](https://github.com/username/repo/actions/workflows/ci.yml/badge.svg)](https://github.com/username/repo/actions/workflows/ci.yml)\n</code></pre>"},{"location":"CI_CD/#notifications","title":"Notifications","text":"<p>Configure in GitHub Settings \u2192 Notifications: - Email notifications for failed workflows - Slack/Discord integration (via webhooks)</p>"},{"location":"CI_CD/#future-improvements","title":"Future Improvements","text":""},{"location":"CI_CD/#planned-enhancements","title":"Planned Enhancements","text":"<ol> <li>Code Coverage Enforcement</li> <li>Fail CI if coverage drops below threshold</li> <li> <p>Per-file coverage reports</p> </li> <li> <p>Security Scanning</p> </li> <li>Dependency vulnerability scanning (Dependabot)</li> <li>Secret scanning</li> <li> <p>SAST (Static Application Security Testing)</p> </li> <li> <p>Performance Regression Testing</p> </li> <li>Benchmark tests in CI</li> <li> <p>Compare performance against main branch</p> </li> <li> <p>Automatic Dependency Updates</p> </li> <li>Renovate or Dependabot</li> <li> <p>Auto-merge minor updates if tests pass</p> </li> <li> <p>Release Automation</p> </li> <li>Automatic changelog generation</li> <li>Semantic versioning</li> <li>PyPI package publishing</li> </ol>"},{"location":"CI_CD/#see-also","title":"See Also","text":"<ul> <li>Pre-commit Script</li> <li>CLAUDE.md - Development guidelines</li> <li>GitHub Actions Documentation</li> </ul>"},{"location":"PERFORMANCE_ANALYSIS/","title":"Performance Analysis and Optimization","text":"<p>This document describes the performance analysis of the VLM OCR Pipeline and identifies optimization opportunities.</p>"},{"location":"PERFORMANCE_ANALYSIS/#performance-profiling-tools","title":"Performance Profiling Tools","text":"<p>The project includes two profiling tools:</p> <ol> <li>scripts/benchmark.py: Simple timing benchmark for overall pipeline performance</li> <li>scripts/profile_pipeline.py: Detailed cProfile-based profiling for function-level analysis</li> <li>pipeline/profiling.py: Performance metrics collection utilities with decorators</li> </ol>"},{"location":"PERFORMANCE_ANALYSIS/#usage","title":"Usage","text":"<pre><code># Simple benchmark (overall timing)\npython scripts/benchmark.py --input document.pdf --max-pages 1 --detector doclayout-yolo\n\n# Detailed profiling\npython scripts/profile_pipeline.py --input document.pdf --max-pages 1 --detailed\n\n# Save results to JSON\npython scripts/benchmark.py --input doc.pdf --max-pages 5 --output results.json\n</code></pre>"},{"location":"PERFORMANCE_ANALYSIS/#identified-performance-bottlenecks","title":"Identified Performance Bottlenecks","text":""},{"location":"PERFORMANCE_ANALYSIS/#1-pdf-rendering-high-impact","title":"1. PDF Rendering (High Impact)","text":"<p>Issue: <code>pdf2image.convert_from_path()</code> renders PDF pages to images, which is CPU and memory intensive.</p> <p>Location: <code>pipeline/conversion/input/pdf.py:render_pdf_page()</code></p> <p>Current Behavior: <pre><code>images = convert_from_path(pdf_path, first_page=page_num, last_page=page_num, dpi=200)\n</code></pre></p> <p>Optimization Opportunities: - Use PyMuPDF's <code>get_pixmap()</code> for faster rendering (2-3x speedup) - Reduce DPI for non-critical pages (trade quality for speed) - Implement parallel rendering for multi-page PDFs - Cache rendered images when processing the same PDF multiple times</p> <p>Estimated Impact: 30-50% reduction in Stage 1 (Input) time</p>"},{"location":"PERFORMANCE_ANALYSIS/#2-pymupdf-document-reopening-medium-impact","title":"2. PyMuPDF Document Reopening (Medium Impact)","text":"<p>Issue: PDF files may be opened multiple times during processing: - Once for page rendering - Again for text span extraction (if using font-based headers) - Potentially again for PyMuPDF sorter</p> <p>Location: <code>pipeline/conversion/input/pdf.py:extract_text_spans_from_pdf()</code></p> <p>Current Behavior: <pre><code>with open_pdf_document(pdf_path) as doc:\n    # Extract text spans for a single page\n</code></pre></p> <p>Optimization Opportunities: - Open PDF once and pass the document object to all functions - Cache document handle at Pipeline level - Batch extract text spans for all pages at once</p> <p>Estimated Impact: 10-20% reduction in Stage 1 time for multi-page PDFs</p>"},{"location":"PERFORMANCE_ANALYSIS/#3-list-operations-and-block-processing-low-medium-impact","title":"3. List Operations and Block Processing (Low-Medium Impact)","text":"<p>Issue: Multiple passes over block lists for different operations: - Detection - Sorting - Column layout extraction - Text extraction - Correction</p> <p>Location: Multiple locations in <code>pipeline/__init__.py</code></p> <p>Current Behavior: <pre><code># Multiple separate iterations\nsorted_blocks = self.ordering_stage.sort(blocks, ...)\ncolumn_layout = self.detection_stage.extract_column_layout(sorted_blocks)\nprocessed_blocks = self.recognition_stage.recognize_blocks(sorted_blocks, ...)\n</code></pre></p> <p>Optimization Opportunities: - Combine operations where possible - Use generator expressions for lazy evaluation - Pre-allocate result lists with known sizes - Use numpy arrays for block coordinates (vectorized operations)</p> <p>Estimated Impact: 5-10% reduction in overall processing time</p>"},{"location":"PERFORMANCE_ANALYSIS/#4-image-memory-management-medium-impact","title":"4. Image Memory Management (Medium Impact)","text":"<p>Issue: Large numpy arrays (images) are passed between functions, potentially causing unnecessary copies.</p> <p>Location: Throughout recognition and detection stages</p> <p>Current Behavior: <pre><code>block_image = block.bbox.crop(image, padding=5)\n# block_image passed to multiple functions\n</code></pre></p> <p>Optimization Opportunities: - \u2705 IMPLEMENTED: Context managers for automatic cleanup (pipeline/resources.py) - Use array views instead of copies where possible - Implement image pyramid for multi-scale processing - Compress images before API calls (if backend supports)</p> <p>Estimated Impact: 10-15% reduction in memory usage, 5-10% speed improvement</p> <p>Status: Context managers implemented in commit bd0167c</p>"},{"location":"PERFORMANCE_ANALYSIS/#5-vlm-api-calls-high-impact-limited-optimization","title":"5. VLM API Calls (High Impact, Limited Optimization)","text":"<p>Issue: Network latency and API processing time dominate Stage 4 (Recognition).</p> <p>Location: <code>pipeline/recognition/api/</code> modules</p> <p>Current Behavior: - Sequential API calls for each block - Rate limiting may add delays</p> <p>Optimization Opportunities: - \u2705 IMPLEMENTED: Caching system to avoid redundant API calls - Batch processing where API supports it (Gemini batch API) - Parallel processing with async/await (within rate limits) - Request compression for faster uploads - Use lower-tier models for simple text blocks</p> <p>Estimated Impact: 20-40% reduction in Stage 4 time (with caching and batching)</p> <p>Status: Caching implemented, batching opportunity remains</p>"},{"location":"PERFORMANCE_ANALYSIS/#6-unnecessary-gccollect-calls-low-impact","title":"6. Unnecessary gc.collect() Calls (Low Impact)","text":"<p>Issue: Manual <code>gc.collect()</code> calls may slow down execution unnecessarily.</p> <p>Location: Various locations (now centralized in context managers)</p> <p>Current Behavior: <pre><code>del large_object\ngc.collect()  # May pause execution\n</code></pre></p> <p>Optimization Opportunities: - \u2705 IMPLEMENTED: Remove manual gc.collect(), rely on automatic GC - Only force GC after processing each page (not each block)</p> <p>Estimated Impact: 2-5% speed improvement</p> <p>Status: Improved with context managers in commit bd0167c</p>"},{"location":"PERFORMANCE_ANALYSIS/#performance-optimization-roadmap","title":"Performance Optimization Roadmap","text":""},{"location":"PERFORMANCE_ANALYSIS/#phase-1-quick-wins-1-2-weeks","title":"Phase 1: Quick Wins (1-2 weeks)","text":"<ul> <li>[x] Implement context managers for resource cleanup</li> <li>[ ] Replace pdf2image with PyMuPDF for rendering</li> <li>[ ] Implement PDF document handle caching</li> <li>[ ] Remove unnecessary gc.collect() calls</li> </ul>"},{"location":"PERFORMANCE_ANALYSIS/#phase-2-architectural-improvements-2-4-weeks","title":"Phase 2: Architectural Improvements (2-4 weeks)","text":"<ul> <li>[ ] Implement async/parallel API calls with rate limiting</li> <li>[ ] Add batch processing support for VLM APIs</li> <li>[ ] Optimize block list operations (vectorization)</li> <li>[ ] Implement image compression for API calls</li> </ul>"},{"location":"PERFORMANCE_ANALYSIS/#phase-3-advanced-optimizations-1-2-months","title":"Phase 3: Advanced Optimizations (1-2 months)","text":"<ul> <li>[ ] Multi-page parallel processing</li> <li>[ ] GPU acceleration for image operations</li> <li>[ ] Custom CUDA kernels for block cropping</li> <li>[ ] Model quantization for faster inference</li> </ul>"},{"location":"PERFORMANCE_ANALYSIS/#benchmarking-results","title":"Benchmarking Results","text":""},{"location":"PERFORMANCE_ANALYSIS/#baseline-before-optimizations","title":"Baseline (Before Optimizations)","text":"Stage Time (s) % Total 1. Input (PDF\u2192Image) 2.50 25% 2. Detection 1.20 12% 3. Ordering 0.30 3% 4. Recognition (VLM) 5.00 50% 5. Block Correction 0.10 1% 6. Rendering 0.20 2% 7. Page Correction 0.50 5% 8. Output 0.20 2% Total 10.00 100% <p>Note: This is a hypothetical baseline. Run actual benchmarks with your data.</p>"},{"location":"PERFORMANCE_ANALYSIS/#after-memory-management-optimizations-bd0167c","title":"After Memory Management Optimizations (bd0167c)","text":"<ul> <li>Memory leaks eliminated</li> <li>5-10% speed improvement from reduced GC pressure</li> <li>More stable memory usage across long-running processes</li> </ul>"},{"location":"PERFORMANCE_ANALYSIS/#target-after-all-optimizations","title":"Target After All Optimizations","text":"Stage Target (s) Improvement 1. Input 1.25 50% 2. Detection 1.00 17% 3. Ordering 0.25 17% 4. Recognition 3.00 40% 5-8. Other 0.80 20% Total 6.30 37%"},{"location":"PERFORMANCE_ANALYSIS/#monitoring-performance","title":"Monitoring Performance","text":""},{"location":"PERFORMANCE_ANALYSIS/#enable-profiling-in-code","title":"Enable Profiling in Code","text":"<pre><code>from pipeline.profiling import get_metrics, measure_time\n\n# Enable metrics collection\nmetrics = get_metrics()\nmetrics.enable()\n\n# Use context manager for timing\nwith measure_time(\"my_operation\"):\n    # Your code here\n    pass\n\n# Print report\nmetrics.print_report()\n</code></pre>"},{"location":"PERFORMANCE_ANALYSIS/#decorator-based-profiling","title":"Decorator-Based Profiling","text":"<pre><code>from pipeline.profiling import timed\n\n@timed(\"expensive_function\")\ndef process_blocks(blocks):\n    # Function implementation\n    pass\n</code></pre>"},{"location":"PERFORMANCE_ANALYSIS/#see-also","title":"See Also","text":"<ul> <li>pipeline/profiling.py - Profiling utilities</li> <li>scripts/benchmark.py - Simple benchmark tool</li> <li>scripts/profile_pipeline.py - Detailed profiler</li> </ul>"},{"location":"api/conversion/","title":"Conversion API","text":"<p>PDF/Image loading and output format conversion utilities.</p>"},{"location":"api/conversion/#overview","title":"Overview","text":"<p>The conversion module provides:</p> <ul> <li>Input: PDF and image loading with auxiliary information extraction</li> <li>Output: Markdown and JSON generation from processed blocks</li> </ul>"},{"location":"api/conversion/#input-conversion","title":"Input Conversion","text":""},{"location":"api/conversion/#pdf-loading","title":"PDF Loading","text":"<pre><code>from pipeline.io.input import pdf\n\n# Get PDF information\ninfo = pdf.get_pdf_info(Path(\"document.pdf\"))\nprint(f\"Pages: {info['Pages']}\")\n\n# Determine pages to process\npages = pdf.determine_pages_to_process(\n    total_pages=10,\n    max_pages=5,        # Optional: limit pages\n    page_range=(1, 5),  # Optional: range\n    pages=[1, 3, 5],    # Optional: specific pages\n)\n\n# Render page to image\nimage = pdf.render_pdf_page(\n    pdf_path=Path(\"document.pdf\"),\n    page_num=1,\n    dpi=200,\n)\n</code></pre>"},{"location":"api/conversion/#image-loading","title":"Image Loading","text":"<pre><code>from pipeline.io.input import image\n\n# Load image as numpy array\nimg = image.load_image(Path(\"document.jpg\"))\n</code></pre>"},{"location":"api/conversion/#inputloader-class","title":"InputLoader Class","text":"<p>The <code>InputLoader</code> class provides a high-level interface for loading documents:</p> <pre><code>from pipeline.io import InputLoader\nfrom pipeline.stages import InputStage\n\ninput_stage = InputStage(dpi=200)\nloader = InputLoader(\n    input_stage=input_stage,\n    use_dual_resolution=True,\n    detection_dpi=150,\n    recognition_dpi=300,\n)\n\n# Load all pages from PDF\npage_images, recognition_images, auxiliary_infos, _, _ = loader.load_page_images(\n    pdf_path=Path(\"document.pdf\"),\n    pages_to_process=[1, 2, 3],\n)\n</code></pre>"},{"location":"api/conversion/#output-conversion","title":"Output Conversion","text":""},{"location":"api/conversion/#markdown-generation","title":"Markdown Generation","text":"<p>Two strategies are available for Markdown conversion:</p>"},{"location":"api/conversion/#block-type-based-default","title":"Block Type-Based (Default)","text":"<p>Fast conversion using block type classification:</p> <pre><code>from pipeline.io.output.markdown import blocks_to_markdown\n\nblocks = [\n    Block(type=\"title\", bbox=..., text=\"Document Title\"),\n    Block(type=\"text\", bbox=..., text=\"Content here.\"),\n]\n\nmarkdown = blocks_to_markdown(blocks)\n# # Document Title\n#\n# Content here.\n</code></pre> <p>Block Type \u2192 Markdown Mapping:</p> Block Type Markdown <code>title</code> <code># Heading</code> <code>text</code> Paragraph <code>table</code> Table (if markdown content) <code>image</code> <code>![Image](path)</code> <code>code</code> Code block <code>list</code> List items <code>equation</code> Math block"},{"location":"api/conversion/#font-size-based-pymupdf4llm-style","title":"Font Size-Based (PyMuPDF4LLM Style)","text":"<p>Advanced conversion using font size information from PDF text spans:</p> <pre><code>from pipeline.io.output.markdown.pymupdf4llm import to_markdown\n\n# page_result contains auxiliary_info with text_spans\nmarkdown = to_markdown(page_result, auto_detect_headers=True)\n</code></pre> <p>Features:</p> <ul> <li>Auto-detects headers from font sizes (largest \u2192 H1, 2nd largest \u2192 H2)</li> <li>Uses IoU matching to link text spans to blocks</li> <li>Preserves original PDF formatting</li> </ul>"},{"location":"api/conversion/#json-output","title":"JSON Output","text":"<p>The <code>OutputSaver</code> class handles JSON serialization:</p> <pre><code>from pipeline.io import OutputSaver\n\nsaver = OutputSaver(\n    detector_name=\"doclayout-yolo\",\n    sorter_name=\"mineru-xycut\",\n    backend=\"gemini\",\n    model=\"gemini-2.5-flash\",\n)\n\n# Save intermediate results\nsaver.save_intermediate_results(\n    pdf_path=pdf_path,\n    pages_to_process=[1, 2, 3],\n    page_output_dir=output_dir,\n    detected_blocks=blocks_dict,\n    stage=\"detection\",\n)\n\n# Save final results\nsaver.save_final_results(result, output_path)\n</code></pre>"},{"location":"api/conversion/#output-directory-structure","title":"Output Directory Structure","text":"<pre><code>output/\n\u2514\u2500\u2500 {model}/\n    \u2514\u2500\u2500 {document}/\n        \u251c\u2500\u2500 page_1.json           # Page data\n        \u251c\u2500\u2500 page_1.md             # Markdown output\n        \u251c\u2500\u2500 page_2.json\n        \u251c\u2500\u2500 page_2.md\n        \u2514\u2500\u2500 summary.json          # Document summary\n</code></pre>"},{"location":"api/conversion/#page-json-format","title":"Page JSON Format","text":"<pre><code>{\n  \"page_num\": 1,\n  \"width\": 1920,\n  \"height\": 1080,\n  \"blocks\": [\n    {\n      \"order\": 0,\n      \"type\": \"title\",\n      \"xywh\": [100, 50, 400, 80],\n      \"detection_confidence\": 0.95,\n      \"text\": \"Document Title\"\n    }\n  ],\n  \"text\": \"Rendered markdown text\",\n  \"corrected_text\": \"VLM-corrected text\",\n  \"correction_ratio\": 0.05,\n  \"processing_time_seconds\": 2.5,\n  \"processed_at\": \"2024-12-19T10:30:00\"\n}\n</code></pre>"},{"location":"api/conversion/#summary-json-format","title":"Summary JSON Format","text":"<pre><code>{\n  \"pdf_name\": \"document\",\n  \"pdf_path\": \"/path/to/document.pdf\",\n  \"num_pages\": 10,\n  \"processed_pages\": 10,\n  \"output_directory\": \"output/gemini-2.5-flash/document\",\n  \"processed_at\": \"2024-12-19T10:30:00\",\n  \"status_summary\": {\"complete\": 10},\n  \"pages\": [\n    {\"page\": 1, \"status\": \"complete\", \"file_suffix\": \"\"}\n  ]\n}\n</code></pre>"},{"location":"api/conversion/#see-also","title":"See Also","text":"<ul> <li>BBox Formats - Coordinate system reference</li> <li>Pipeline API - Main pipeline class</li> <li>Types API - Data structures</li> </ul>"},{"location":"api/detection/","title":"Detection API","text":"<p>Factory function and interface for layout detectors.</p>"},{"location":"api/detection/#overview","title":"Overview","text":"<p>The detection module provides:</p> <ul> <li>Factory function: <code>create_detector()</code> for creating detector instances</li> <li>Protocol interface: <code>Detector</code> protocol for implementing custom detectors</li> <li>Built-in detectors: DocLayout-YOLO, PaddleOCR PP-DocLayoutV2, MinerU VLM</li> </ul>"},{"location":"api/detection/#quick-start","title":"Quick Start","text":"<pre><code>from pipeline.layout.detection import create_detector\nimport numpy as np\n\n# Create detector\ndetector = create_detector(\"doclayout-yolo\")\n\n# Detect layout blocks\nimage = np.zeros((1000, 800, 3), dtype=np.uint8)  # Your image\nblocks = detector.detect(image)\n\nfor block in blocks:\n    print(f\"Type: {block.type}, BBox: {block.bbox}, Confidence: {block.detection_confidence}\")\n</code></pre>"},{"location":"api/detection/#factory-function","title":"Factory Function","text":""},{"location":"api/detection/#create_detector","title":"<code>create_detector</code>","text":"<pre><code>def create_detector(\n    name: str,\n    model_path: str | None = None,\n    confidence_threshold: float = 0.5,\n    auto_batch_size: bool = False,\n    batch_size: int | None = None,\n    target_memory_fraction: float = 0.8,\n    backend: str | None = None,\n    **kwargs,\n) -&gt; Detector:\n    \"\"\"Create a detector instance.\n\n    Args:\n        name: Detector name (\"doclayout-yolo\", \"paddleocr-doclayout-v2\", etc.)\n        model_path: Custom model path (optional)\n        confidence_threshold: Detection threshold (0.0-1.0)\n        auto_batch_size: Enable automatic batch size optimization\n        batch_size: Manual batch size\n        target_memory_fraction: Target GPU memory usage for auto batch\n        backend: Inference backend (\"pytorch\", \"hf\", \"vllm\")\n        **kwargs: Additional detector-specific arguments\n\n    Returns:\n        Detector instance\n\n    Raises:\n        ValueError: If detector name is unknown\n    \"\"\"\n</code></pre>"},{"location":"api/detection/#available-detectors","title":"Available Detectors","text":"Detector Description Speed Block Types <code>doclayout-yolo</code> This project's DocLayout-YOLO Fast 7 types <code>mineru-doclayout-yolo</code> MinerU's DocLayout-YOLO Fast 10 types <code>paddleocr-doclayout-v2</code> PaddleOCR PP-DocLayoutV2 Medium 25 types <code>mineru-vlm</code> MinerU VLM-based detection Slow 25+ types"},{"location":"api/detection/#doclayout-yolo","title":"DocLayout-YOLO","text":"<p>Fast YOLO-based detector for common document layouts.</p> <p>Block Types: title, plain text, figure, table, equation, list, text</p> <pre><code>detector = create_detector(\n    \"doclayout-yolo\",\n    model_path=\"path/to/model.pt\",  # Optional custom model\n    confidence_threshold=0.5,\n)\n</code></pre>"},{"location":"api/detection/#paddleocr-pp-doclayoutv2","title":"PaddleOCR PP-DocLayoutV2","text":"<p>High-quality detector with 25 block type categories and pointer network-based ordering.</p> <p>Block Types: doc_title, paragraph_title, text, table, image, formula, algorithm, and more</p> <pre><code>detector = create_detector(\n    \"paddleocr-doclayout-v2\",\n    confidence_threshold=0.5,\n)\n</code></pre>"},{"location":"api/detection/#mineru-vlm","title":"MinerU VLM","text":"<p>VLM-based detector for complex document understanding.</p> <p>Block Types: Full MinerU 2.5 block type hierarchy (25+ types)</p> <pre><code>detector = create_detector(\n    \"mineru-vlm\",\n    model_name=\"opendatalab/MinerU2.5-2509-1.2B\",\n    backend=\"hf\",  # or \"vllm\"\n)\n</code></pre>"},{"location":"api/detection/#detector-protocol","title":"Detector Protocol","text":"<p>All detectors implement the <code>Detector</code> protocol:</p> <pre><code>from typing import Protocol\nimport numpy as np\nfrom pipeline.types import Block\n\nclass Detector(Protocol):\n    \"\"\"Protocol for layout detectors.\"\"\"\n\n    def detect(self, image: np.ndarray) -&gt; list[Block]:\n        \"\"\"Detect layout blocks in an image.\n\n        Args:\n            image: Input image as numpy array (H, W, C) in BGR format\n\n        Returns:\n            List of detected blocks with bounding boxes, types, and confidence scores\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/detection/#implementing-custom-detectors","title":"Implementing Custom Detectors","text":"<pre><code>from pipeline.types import Block, BBox\nimport numpy as np\n\nclass MyDetector:\n    \"\"\"Custom detector implementation.\"\"\"\n\n    def __init__(self, model_path: str):\n        self.model = load_model(model_path)\n\n    def detect(self, image: np.ndarray) -&gt; list[Block]:\n        \"\"\"Detect layout blocks.\"\"\"\n        results = self.model.predict(image)\n\n        blocks = []\n        for r in results:\n            block = Block(\n                type=r.class_name,\n                bbox=BBox.from_xyxy(*r.bbox),\n                detection_confidence=r.confidence,\n                source=\"my-detector\",\n            )\n            blocks.append(block)\n\n        return blocks\n</code></pre>"},{"location":"api/detection/#cli-usage","title":"CLI Usage","text":"<pre><code># Default detector\npython main.py --input doc.pdf\n\n# Specific detector\npython main.py --input doc.pdf --detector doclayout-yolo\n\n# With backend\npython main.py --input doc.pdf --detector mineru-vlm --detector-backend vllm\n\n# With confidence threshold\npython main.py --input doc.pdf --detector doclayout-yolo --confidence 0.7\n</code></pre>"},{"location":"api/detection/#see-also","title":"See Also","text":"<ul> <li>Detectors Architecture - Detailed detector comparison</li> <li>Detector Block Types - Block type mappings</li> <li>Types API - BBox and Block classes</li> </ul>"},{"location":"api/ordering/","title":"Ordering API","text":"<p>Factory function and interface for reading order sorters.</p>"},{"location":"api/ordering/#overview","title":"Overview","text":"<p>The ordering module provides:</p> <ul> <li>Factory function: <code>create_sorter()</code> for creating sorter instances</li> <li>Validation: <code>validate_combination()</code> for checking detector/sorter compatibility</li> <li>Protocol interface: <code>Sorter</code> protocol for implementing custom sorters</li> <li>Built-in sorters: PyMuPDF, XY-Cut, LayoutReader, VLM-based</li> </ul>"},{"location":"api/ordering/#quick-start","title":"Quick Start","text":"<pre><code>from pipeline.layout.ordering import create_sorter, validate_combination\nfrom pipeline.layout.detection import create_detector\nimport numpy as np\n\n# Validate detector/sorter combination\nvalidate_combination(\"doclayout-yolo\", \"mineru-xycut\")\n\n# Create components\ndetector = create_detector(\"doclayout-yolo\")\nsorter = create_sorter(\"mineru-xycut\")\n\n# Detect and sort\nimage = np.zeros((1000, 800, 3), dtype=np.uint8)\nblocks = detector.detect(image)\nsorted_blocks = sorter.sort(blocks, image)\n\nfor block in sorted_blocks:\n    print(f\"Order: {block.order}, Type: {block.type}\")\n</code></pre>"},{"location":"api/ordering/#factory-function","title":"Factory Function","text":""},{"location":"api/ordering/#create_sorter","title":"<code>create_sorter</code>","text":"<pre><code>def create_sorter(\n    name: str,\n    backend: str | None = None,\n    **kwargs,\n) -&gt; Sorter:\n    \"\"\"Create a sorter instance.\n\n    Args:\n        name: Sorter name (\"pymupdf\", \"mineru-xycut\", etc.)\n        backend: Inference backend for VLM sorters\n        **kwargs: Additional sorter-specific arguments\n\n    Returns:\n        Sorter instance\n\n    Raises:\n        ValueError: If sorter name is unknown\n    \"\"\"\n</code></pre>"},{"location":"api/ordering/#validate_combination","title":"<code>validate_combination</code>","text":"<pre><code>def validate_combination(\n    detector_name: str,\n    sorter_name: str,\n) -&gt; None:\n    \"\"\"Validate detector/sorter combination.\n\n    Args:\n        detector_name: Name of detector\n        sorter_name: Name of sorter\n\n    Raises:\n        ValueError: If combination is invalid\n    \"\"\"\n</code></pre>"},{"location":"api/ordering/#available-sorters","title":"Available Sorters","text":"Sorter Algorithm Multi-Column Speed Notes <code>pymupdf</code> Font analysis Yes Fast Best for standard documents <code>mineru-xycut</code> XY-Cut No Fast Simple and reliable <code>mineru-layoutreader</code> LayoutLMv3 Yes Medium ML-based ordering <code>mineru-vlm</code> VLM reasoning Yes Slow Requires mineru-vlm detector <code>olmocr-vlm</code> VLM reasoning Yes Slow Flexible VLM ordering <code>paddleocr-doclayout-v2</code> Pointer network Yes Medium Preserves detector ordering"},{"location":"api/ordering/#pymupdf-sorter","title":"PyMuPDF Sorter","text":"<p>Multi-column aware sorter using PyMuPDF font analysis.</p> <pre><code>sorter = create_sorter(\"pymupdf\")\nsorted_blocks = sorter.sort(blocks, image, pymupdf_page=page)\n</code></pre>"},{"location":"api/ordering/#xy-cut-sorter","title":"XY-Cut Sorter","text":"<p>Classic recursive XY-Cut algorithm for reading order.</p> <pre><code>sorter = create_sorter(\"mineru-xycut\")\nsorted_blocks = sorter.sort(blocks, image)\n</code></pre>"},{"location":"api/ordering/#layoutreader-sorter","title":"LayoutReader Sorter","text":"<p>LayoutLMv3-based reading order prediction.</p> <pre><code>sorter = create_sorter(\"mineru-layoutreader\")\nsorted_blocks = sorter.sort(blocks, image)\n</code></pre>"},{"location":"api/ordering/#vlm-sorters","title":"VLM Sorters","text":"<p>VLM-based sorters use visual reasoning for complex layouts.</p> <pre><code># olmOCR VLM (independent)\nsorter = create_sorter(\"olmocr-vlm\")\n\n# MinerU VLM (requires mineru-vlm detector)\nsorter = create_sorter(\"mineru-vlm\")\n</code></pre>"},{"location":"api/ordering/#sorter-protocol","title":"Sorter Protocol","text":"<p>All sorters implement the <code>Sorter</code> protocol:</p> <pre><code>from typing import Protocol\nimport numpy as np\nfrom pipeline.types import Block\n\nclass Sorter(Protocol):\n    \"\"\"Protocol for reading order sorters.\"\"\"\n\n    def sort(\n        self,\n        blocks: list[Block],\n        image: np.ndarray,\n        **kwargs,\n    ) -&gt; list[Block]:\n        \"\"\"Sort blocks in reading order.\n\n        Args:\n            blocks: Detected blocks to sort\n            image: Original page image for context\n            **kwargs: Additional arguments (e.g., pymupdf_page)\n\n        Returns:\n            Blocks sorted by reading order with `order` field populated\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/ordering/#implementing-custom-sorters","title":"Implementing Custom Sorters","text":"<pre><code>from pipeline.types import Block\nimport numpy as np\n\nclass MySorter:\n    \"\"\"Custom sorter implementation.\"\"\"\n\n    def sort(\n        self,\n        blocks: list[Block],\n        image: np.ndarray,\n        **kwargs,\n    ) -&gt; list[Block]:\n        \"\"\"Sort blocks by custom logic.\"\"\"\n        # Sort by y-coordinate then x-coordinate\n        sorted_blocks = sorted(blocks, key=lambda b: (b.bbox.y0, b.bbox.x0))\n\n        # Add order field\n        for i, block in enumerate(sorted_blocks):\n            block.order = i\n\n        return sorted_blocks\n</code></pre>"},{"location":"api/ordering/#detectorsorter-compatibility","title":"Detector/Sorter Compatibility","text":"<p>Some combinations have specific requirements:</p> Detector Compatible Sorters Notes <code>doclayout-yolo</code> All except <code>mineru-vlm</code> Most flexible <code>mineru-doclayout-yolo</code> All except <code>mineru-vlm</code> - <code>paddleocr-doclayout-v2</code> All Best with <code>paddleocr-doclayout-v2</code> sorter <code>mineru-vlm</code> <code>mineru-vlm</code> only Tightly coupled"},{"location":"api/ordering/#cli-usage","title":"CLI Usage","text":"<pre><code># Default sorter (auto-selected)\npython main.py --input doc.pdf\n\n# Specific sorter\npython main.py --input doc.pdf --sorter mineru-xycut\n\n# With backend\npython main.py --input doc.pdf --sorter olmocr-vlm --sorter-backend vllm\n\n# Detector + sorter combination\npython main.py --input doc.pdf --detector doclayout-yolo --sorter pymupdf\n</code></pre>"},{"location":"api/ordering/#see-also","title":"See Also","text":"<ul> <li>Sorters Architecture - Detailed sorter comparison</li> <li>Detection API - Detector documentation</li> <li>Types API - Block class reference</li> </ul>"},{"location":"api/pipeline/","title":"Pipeline API","text":"<p>Main pipeline class for document processing.</p>"},{"location":"api/pipeline/#overview","title":"Overview","text":"<p>The <code>Pipeline</code> class orchestrates the entire 8-stage document processing workflow:</p> <ol> <li>Input: Load documents and extract auxiliary information</li> <li>Detection: Detect layout blocks using selected detector</li> <li>Ordering: Analyze reading order using selected sorter</li> <li>Recognition: Extract text from blocks using VLM or local model</li> <li>Block Correction: Block-level text correction (optional)</li> <li>Rendering: Convert to Markdown or plaintext</li> <li>Page Correction: Page-level VLM correction (optional)</li> <li>Output: Save results and generate summaries</li> </ol>"},{"location":"api/pipeline/#quick-start","title":"Quick Start","text":"<pre><code>from pathlib import Path\nfrom pipeline import Pipeline\nfrom pipeline.config import PipelineConfig\n\n# Create pipeline with configuration\nconfig = PipelineConfig(\n    detector=\"paddleocr-doclayout-v2\",\n    sorter=\"mineru-xycut\",\n    recognizer=\"gemini-2.5-flash\",\n)\npipeline = Pipeline(config=config)\n\n# Process PDF\ndocument = pipeline.process_pdf(Path(\"document.pdf\"))\n\n# Process single image\nresult = pipeline.process_single_image(Path(\"image.jpg\"))\n\n# Process directory\nsummary = pipeline.process_directory(Path(\"pdfs/\"), \"output/\")\n</code></pre>"},{"location":"api/pipeline/#pipeline-class","title":"Pipeline Class","text":""},{"location":"api/pipeline/#constructor","title":"Constructor","text":"<pre><code>class Pipeline:\n    def __init__(self, config: PipelineConfig | None = None):\n        \"\"\"Initialize VLM OCR processing pipeline.\n\n        Args:\n            config: Pipeline configuration object. If None, uses default configuration.\n        \"\"\"\n</code></pre>"},{"location":"api/pipeline/#key-methods","title":"Key Methods","text":""},{"location":"api/pipeline/#process_pdf","title":"<code>process_pdf</code>","text":"<pre><code>def process_pdf(\n    self,\n    pdf_path: Path,\n    max_pages: int | None = None,\n    page_range: tuple[int, int] | None = None,\n    pages: list[int] | None = None,\n) -&gt; Document:\n    \"\"\"Process PDF file with page limiting options.\n\n    Args:\n        pdf_path: Path to PDF file\n        max_pages: Maximum number of pages to process\n        page_range: Range of pages to process (start, end)\n        pages: Specific list of page numbers to process\n\n    Returns:\n        Document object with full processing results\n    \"\"\"\n</code></pre>"},{"location":"api/pipeline/#process_image","title":"<code>process_image</code>","text":"<pre><code>def process_image(\n    self,\n    image_path: str | Path,\n    max_pages: int | None = None,\n    page_range: tuple[int, int] | None = None,\n    pages: list[int] | None = None,\n) -&gt; Document | dict[str, Any]:\n    \"\"\"Process single image or PDF.\n\n    Args:\n        image_path: Path to image or PDF file\n        max_pages: Maximum number of pages to process (PDF only)\n        page_range: Range of pages to process (PDF only)\n        pages: Specific pages to process (PDF only)\n\n    Returns:\n        Document for PDF, dict for single image\n    \"\"\"\n</code></pre>"},{"location":"api/pipeline/#process_single_image","title":"<code>process_single_image</code>","text":"<pre><code>def process_single_image(self, image_path: Path) -&gt; dict[str, Any]:\n    \"\"\"Process a single image file.\n\n    Args:\n        image_path: Path to image file\n\n    Returns:\n        Processing results with blocks and extracted text\n    \"\"\"\n</code></pre>"},{"location":"api/pipeline/#process_directory","title":"<code>process_directory</code>","text":"<pre><code>def process_directory(\n    self,\n    input_dir: Path,\n    output_dir: str,\n    max_pages: int | None = None,\n    page_range: tuple[int, int] | None = None,\n    specific_pages: list[int] | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Process all PDFs in a directory using staged batch processing.\n\n    Args:\n        input_dir: Directory containing PDF files\n        output_dir: Output directory\n        max_pages: Maximum pages per PDF to process\n        page_range: Page range per PDF to process\n        specific_pages: Specific pages per PDF to process\n\n    Returns:\n        Processing summary dictionary\n    \"\"\"\n</code></pre>"},{"location":"api/pipeline/#pipelineconfig","title":"PipelineConfig","text":"<p>Configuration class for the pipeline.</p>"},{"location":"api/pipeline/#constructor_1","title":"Constructor","text":"<pre><code>@dataclass\nclass PipelineConfig:\n    # Detector settings\n    detector: str = \"paddleocr-doclayout-v2\"\n    detector_backend: str | None = None\n    detector_model_path: str | None = None\n    confidence_threshold: float = 0.5\n\n    # Sorter settings\n    sorter: str | None = None  # Auto-selected based on detector\n    sorter_backend: str | None = None\n\n    # Recognizer settings\n    recognizer: str = \"gemini-2.5-flash\"\n    recognizer_backend: str | None = None\n\n    # DPI settings\n    dpi: int | None = None\n    detection_dpi: int | None = 150\n    recognition_dpi: int | None = 300\n    use_dual_resolution: bool = True\n\n    # Correction settings\n    enable_block_correction: bool = False\n    enable_page_correction: bool = False\n\n    # Output settings\n    output_dir: Path = Path(\"output\")\n    renderer: str = \"block-type\"\n\n    # Caching\n    use_cache: bool = True\n    cache_dir: Path = Path(\".cache\")\n\n    # Batch processing\n    auto_batch_size: bool = False\n    batch_size: int | None = None\n</code></pre>"},{"location":"api/pipeline/#example-usage","title":"Example Usage","text":"<pre><code>from pipeline.config import PipelineConfig\n\n# Minimal configuration\nconfig = PipelineConfig(\n    recognizer=\"gemini-2.5-flash\",\n)\n\n# Full configuration\nconfig = PipelineConfig(\n    detector=\"paddleocr-doclayout-v2\",\n    sorter=\"mineru-xycut\",\n    recognizer=\"paddleocr-vl\",\n    recognizer_backend=\"vllm\",\n    detection_dpi=150,\n    recognition_dpi=300,\n    enable_page_correction=True,\n    output_dir=Path(\"results\"),\n)\n</code></pre>"},{"location":"api/pipeline/#see-also","title":"See Also","text":"<ul> <li>Architecture Overview - Understanding the pipeline stages</li> <li>Detectors - Available detection models</li> <li>Sorters - Reading order algorithms</li> <li>Recognizers - Text extraction backends</li> </ul>"},{"location":"api/recognition/","title":"Recognition API","text":"<p>Text recognition and correction using Vision Language Models.</p>"},{"location":"api/recognition/#overview","title":"Overview","text":"<p>The recognition module provides:</p> <ul> <li>TextRecognizer: Main class for text extraction from blocks</li> <li>Multiple backends: OpenAI, Gemini, PaddleOCR-VL, DeepSeek-OCR</li> <li>Caching: Content-based caching to avoid reprocessing</li> <li>Rate limiting: Automatic throttling for API backends</li> </ul>"},{"location":"api/recognition/#quick-start","title":"Quick Start","text":"<pre><code>from pipeline.recognition import TextRecognizer\nimport numpy as np\n\n# Create recognizer with Gemini backend\nrecognizer = TextRecognizer(\n    backend=\"gemini\",\n    model=\"gemini-2.5-flash\",\n    use_cache=True,\n)\n\n# Process blocks\nimage = np.zeros((1000, 800, 3), dtype=np.uint8)\nprocessed_blocks = recognizer.process_blocks(image, blocks)\n\n# Correct text\ncorrected = recognizer.correct_text(raw_text)\n</code></pre>"},{"location":"api/recognition/#textrecognizer","title":"TextRecognizer","text":""},{"location":"api/recognition/#constructor","title":"Constructor","text":"<pre><code>class TextRecognizer:\n    def __init__(\n        self,\n        backend: str = \"gemini\",\n        model: str = \"gemini-2.5-flash\",\n        use_cache: bool = True,\n        cache_dir: str | Path = \".cache\",\n        gemini_tier: str = \"free\",\n        recognizer_backend: str | None = None,\n        prompts_dir: str | Path | None = None,\n        **kwargs,\n    ):\n        \"\"\"Initialize text recognizer.\n\n        Args:\n            backend: API backend (\"gemini\", \"openai\", \"paddleocr-vl\", \"deepseek-ocr\")\n            model: Model name\n            use_cache: Enable content-based caching\n            cache_dir: Cache directory path\n            gemini_tier: Gemini API tier for rate limiting\n            recognizer_backend: Inference backend for local models\n            prompts_dir: Custom prompts directory\n        \"\"\"\n</code></pre>"},{"location":"api/recognition/#methods","title":"Methods","text":""},{"location":"api/recognition/#process_blocks","title":"<code>process_blocks</code>","text":"<pre><code>def process_blocks(\n    self,\n    image: np.ndarray,\n    blocks: Sequence[Block],\n) -&gt; list[Block]:\n    \"\"\"Extract text from blocks.\n\n    Args:\n        image: Full page image as numpy array\n        blocks: List of blocks to process\n\n    Returns:\n        List of blocks with text field populated\n    \"\"\"\n</code></pre>"},{"location":"api/recognition/#correct_text","title":"<code>correct_text</code>","text":"<pre><code>def correct_text(self, text: str) -&gt; str | dict[str, Any]:\n    \"\"\"Correct extracted text using VLM.\n\n    Args:\n        text: Raw extracted text\n\n    Returns:\n        Corrected text string, or dict with:\n        - corrected_text: str\n        - correction_ratio: float (0.0 = no change)\n    \"\"\"\n</code></pre>"},{"location":"api/recognition/#available-backends","title":"Available Backends","text":""},{"location":"api/recognition/#cloud-vlm-apis","title":"Cloud VLM APIs","text":"Backend Models Rate Limits Cost <code>gemini</code> gemini-2.5-flash, gemini-2.0-flash 15 RPM (free) Free tier available <code>openai</code> gpt-4o, gpt-4-turbo Varies by tier Pay per token <code>openrouter</code> Multiple VLMs Varies by model Pay per token"},{"location":"api/recognition/#local-models","title":"Local Models","text":"Backend Model Parameters Languages <code>paddleocr-vl</code> PaddleOCR-VL-0.9B 0.9B 109 languages <code>deepseek-ocr</code> DeepSeek-OCR - Contextual compression"},{"location":"api/recognition/#backend-configuration","title":"Backend Configuration","text":""},{"location":"api/recognition/#gemini-backend","title":"Gemini Backend","text":"<pre><code>recognizer = TextRecognizer(\n    backend=\"gemini\",\n    model=\"gemini-2.5-flash\",\n    gemini_tier=\"free\",  # free, tier1, tier2, tier3\n)\n</code></pre> <p>Environment Variable: <code>GEMINI_API_KEY</code></p> <p>Rate Limits (Free Tier): - 15 requests per minute - 1,500,000 tokens per minute - 1,500 requests per day</p>"},{"location":"api/recognition/#openai-backend","title":"OpenAI Backend","text":"<pre><code>recognizer = TextRecognizer(\n    backend=\"openai\",\n    model=\"gpt-4o\",\n)\n</code></pre> <p>Environment Variable: <code>OPENAI_API_KEY</code></p>"},{"location":"api/recognition/#paddleocr-vl-backend","title":"PaddleOCR-VL Backend","text":"<pre><code>recognizer = TextRecognizer(\n    backend=\"paddleocr-vl\",\n    recognizer_backend=\"pytorch\",  # pytorch, vllm, sglang\n)\n</code></pre> <p>Requirements: GPU recommended, PaddleX installation</p>"},{"location":"api/recognition/#deepseek-ocr-backend","title":"DeepSeek-OCR Backend","text":"<pre><code>recognizer = TextRecognizer(\n    backend=\"deepseek-ocr\",\n    recognizer_backend=\"hf\",  # hf, vllm\n)\n</code></pre>"},{"location":"api/recognition/#recognizer-protocol","title":"Recognizer Protocol","text":"<p>All recognizers implement the <code>Recognizer</code> protocol:</p> <pre><code>from typing import Protocol, Any, Sequence\nimport numpy as np\nfrom pipeline.types import Block\n\nclass Recognizer(Protocol):\n    \"\"\"Protocol for text recognizers.\"\"\"\n\n    def process_blocks(\n        self,\n        image: np.ndarray,\n        blocks: Sequence[Block],\n    ) -&gt; list[Block]:\n        \"\"\"Extract text from blocks.\"\"\"\n        ...\n\n    def correct_text(self, text: str) -&gt; str | dict[str, Any]:\n        \"\"\"Correct extracted text.\"\"\"\n        ...\n</code></pre>"},{"location":"api/recognition/#caching","title":"Caching","text":"<p>The recognizer uses content-based caching to avoid reprocessing:</p> <pre><code># Cache key = hash(block_image + block_type + prompt)\nrecognizer = TextRecognizer(\n    backend=\"gemini\",\n    use_cache=True,\n    cache_dir=\".cache\",\n)\n</code></pre>"},{"location":"api/recognition/#implementing-custom-recognizers","title":"Implementing Custom Recognizers","text":"<pre><code>from pipeline.types import Block\nfrom typing import Sequence, Any\nimport numpy as np\n\nclass MyRecognizer:\n    \"\"\"Custom recognizer implementation.\"\"\"\n\n    def __init__(self, model_path: str):\n        self.model = load_model(model_path)\n\n    def process_blocks(\n        self,\n        image: np.ndarray,\n        blocks: Sequence[Block],\n    ) -&gt; list[Block]:\n        \"\"\"Extract text from blocks.\"\"\"\n        result_blocks = []\n\n        for block in blocks:\n            cropped = block.bbox.crop(image)\n            text = self.model.recognize(cropped)\n            block.text = text\n            result_blocks.append(block)\n\n        return result_blocks\n\n    def correct_text(self, text: str) -&gt; dict[str, Any]:\n        \"\"\"Correct extracted text.\"\"\"\n        corrected = self.model.correct(text)\n        return {\n            \"corrected_text\": corrected,\n            \"correction_ratio\": calculate_ratio(text, corrected),\n        }\n</code></pre>"},{"location":"api/recognition/#cli-usage","title":"CLI Usage","text":"<pre><code># Default recognizer (gemini)\npython main.py --input doc.pdf\n\n# Specific recognizer\npython main.py --input doc.pdf --recognizer gpt-4o\n\n# Local model with backend\npython main.py --input doc.pdf --recognizer paddleocr-vl --recognizer-backend vllm\n\n# Check rate limit status\npython main.py --rate-limit-status --recognizer gemini-2.5-flash --gemini-tier free\n</code></pre>"},{"location":"api/recognition/#see-also","title":"See Also","text":"<ul> <li>Recognizers Architecture - Detailed backend comparison</li> <li>Basic Usage - Usage examples</li> <li>Types API - Block class reference</li> </ul>"},{"location":"api/types/","title":"Types API","text":"<p>Core type definitions and data structures used throughout the pipeline.</p>"},{"location":"api/types/#overview","title":"Overview","text":"<p>The pipeline uses a unified type system for representing document elements:</p> <ul> <li>BBox: Pixel-based bounding box with automatic format conversion</li> <li>Block: Document block (text, table, figure, etc.) with metadata</li> <li>Page: Processed page with blocks and text</li> <li>Document: Complete document with multiple pages</li> </ul>"},{"location":"api/types/#bbox","title":"BBox","text":"<p>Unified bounding box representation with automatic format conversion between 6+ formats.</p> <p>Key Features:</p> <ul> <li>Internal format: <code>(x0, y0, x1, y1)</code> - integer pixel coordinates (xyxy corners)</li> <li>JSON output: <code>[x, y, w, h]</code> - position + size for human readability</li> <li>Automatic conversion from YOLO, MinerU, PyMuPDF, PyPDF, olmOCR formats</li> </ul>"},{"location":"api/types/#quick-example","title":"Quick Example","text":"<pre><code>from pipeline.types import BBox\n\n# Create from different formats\nbbox = BBox.from_xywh(100, 50, 200, 150)      # [x, y, w, h]\nbbox = BBox.from_xyxy(100, 50, 300, 200)      # corners\nbbox = BBox.from_pypdf_rect([100, 592, 300, 742], page_height=792)\n\n# Convert to formats\nxywh = bbox.to_xywh_list()     # [100, 50, 200, 150]\nxyxy = bbox.to_list()          # [100, 50, 300, 200]\n\n# Properties\nprint(bbox.width, bbox.height)  # 200, 150\nprint(bbox.area)                # 30000\nprint(bbox.center)              # (200.0, 125.0)\n\n# Operations\ncropped = bbox.crop(image, padding=5)\niou = bbox1.iou(bbox2)\n</code></pre>"},{"location":"api/types/#bbox-class-reference","title":"BBox Class Reference","text":"<pre><code>@dataclass(frozen=True)\nclass BBox:\n    \"\"\"Pixel-based bounding box with integer coordinates.\n\n    Internal format: (x0, y0, x1, y1) - Top-left and bottom-right corners (xyxy)\n    Origin: Top-left corner of image (0, 0)\n    Coordinates: Integer pixel values (not normalized)\n    \"\"\"\n\n    x0: int  # Left\n    y0: int  # Top\n    x1: int  # Right\n    y1: int  # Bottom\n</code></pre>"},{"location":"api/types/#factory-methods","title":"Factory Methods","text":"Method Format Example <code>from_xywh(x, y, w, h)</code> Position + Size <code>BBox.from_xywh(100, 50, 200, 150)</code> <code>from_xyxy(x0, y0, x1, y1)</code> Corners <code>BBox.from_xyxy(100, 50, 300, 200)</code> <code>from_list(coords, format)</code> List <code>BBox.from_list([100, 50, 200, 150], \"xywh\")</code> <code>from_pymupdf_rect(rect)</code> PyMuPDF Rect <code>BBox.from_pymupdf_rect(rect)</code> <code>from_mineru_bbox(bbox)</code> MinerU list <code>BBox.from_mineru_bbox([100, 50, 300, 200])</code> <code>from_pypdf_rect(rect, height)</code> PyPDF (Y-flip) <code>BBox.from_pypdf_rect([100, 592, 300, 742], 792)</code> <code>from_cxcywh(cx, cy, w, h)</code> Center format <code>BBox.from_cxcywh(200, 125, 200, 150)</code>"},{"location":"api/types/#conversion-methods","title":"Conversion Methods","text":"Method Output Example <code>to_xywh()</code> Tuple <code>(100, 50, 200, 150)</code> <code>to_xyxy()</code> Tuple <code>(100, 50, 300, 200)</code> <code>to_xywh_list()</code> List (JSON) <code>[100, 50, 200, 150]</code> <code>to_list()</code> List (xyxy) <code>[100, 50, 300, 200]</code> <code>to_dict()</code> Dict <code>{\"x0\": 100, \"y0\": 50, \"x1\": 300, \"y1\": 200}</code> <code>to_mineru_bbox()</code> List <code>[100, 50, 300, 200]</code> <code>to_pypdf_rect(height)</code> List (Y-flip) <code>[100, 592, 300, 742]</code> <code>to_olmocr_anchor(type)</code> String <code>\"[100x50]text\"</code>"},{"location":"api/types/#properties","title":"Properties","text":"Property Type Description <code>width</code> int Width of bbox <code>height</code> int Height of bbox <code>area</code> int Area (width \u00d7 height) <code>center</code> tuple[float, float] Center point (cx, cy) <code>left</code>, <code>top</code>, <code>right</code>, <code>bottom</code> int Edge coordinates"},{"location":"api/types/#geometric-operations","title":"Geometric Operations","text":"Method Description Returns <code>intersect(other)</code> Intersection area int <code>iou(other)</code> Intersection over Union float (0-1) <code>overlap_ratio(other)</code> Overlap relative to self float (0-1) <code>contains_point(x, y)</code> Point inside bbox? bool <code>expand(padding)</code> Expand by padding BBox <code>clip(max_w, max_h)</code> Clip to bounds BBox <code>crop(image, padding)</code> Crop from image np.ndarray"},{"location":"api/types/#block","title":"Block","text":"<p>Document block representing a detected layout element.</p>"},{"location":"api/types/#quick-example_1","title":"Quick Example","text":"<pre><code>from pipeline.types import Block, BBox\n\n# Create block\nblock = Block(\n    type=\"text\",\n    bbox=BBox(100, 50, 500, 200),\n    detection_confidence=0.95,\n    order=0,\n    text=\"Extracted text content\",\n)\n\n# Serialize to JSON\ndata = block.to_dict()\n# {\"order\": 0, \"type\": \"text\", \"xywh\": [100, 50, 400, 150], ...}\n\n# Deserialize from JSON\nblock = Block.from_dict(data)\n</code></pre>"},{"location":"api/types/#block-class-reference","title":"Block Class Reference","text":"<pre><code>@dataclass\nclass Block:\n    \"\"\"Document block with bounding box.\"\"\"\n\n    # Core fields (always present)\n    type: str              # \"text\", \"title\", \"table\", \"image\", etc.\n    bbox: BBox             # Required, always present\n    detection_confidence: float | None = None\n\n    # Added by sorters\n    order: int | None = None           # Reading order rank\n    column_index: int | None = None    # Column index\n\n    # Added by recognizers\n    text: str | None = None            # Extracted text\n    corrected_text: str | None = None  # VLM-corrected text\n    correction_ratio: float | None = None\n    corrected_by: str | None = None\n\n    # Image/figure blocks\n    image_path: str | None = None      # Path to extracted image\n    description: str | None = None     # VLM description\n\n    # Metadata (internal use)\n    source: str | None = None          # Detector name\n    index: int | None = None           # Internal index\n</code></pre>"},{"location":"api/types/#methods","title":"Methods","text":"Method Description <code>to_dict()</code> Convert to JSON-serializable dict (bbox \u2192 xywh) <code>from_dict(data)</code> Create Block from dict (xywh \u2192 BBox)"},{"location":"api/types/#blocktype","title":"BlockType","text":"<p>Standardized block type constants for consistent processing.</p> <pre><code>class BlockType:\n    # Content\n    TEXT = \"text\"\n    TITLE = \"title\"\n\n    # Figures\n    IMAGE = \"image\"\n    IMAGE_BODY = \"image_body\"\n    IMAGE_CAPTION = \"image_caption\"\n\n    # Tables\n    TABLE = \"table\"\n    TABLE_BODY = \"table_body\"\n    TABLE_CAPTION = \"table_caption\"\n\n    # Equations\n    EQUATION = \"equation\"\n    INTERLINE_EQUATION = \"interline_equation\"\n    INLINE_EQUATION = \"inline_equation\"\n\n    # Code\n    CODE = \"code\"\n    ALGORITHM = \"algorithm\"\n\n    # Lists\n    LIST = \"list\"\n\n    # Page Elements\n    HEADER = \"header\"\n    FOOTER = \"footer\"\n    PAGE_NUMBER = \"page_number\"\n</code></pre>"},{"location":"api/types/#protocol-interfaces","title":"Protocol Interfaces","text":""},{"location":"api/types/#detector-protocol","title":"Detector Protocol","text":"<p>All detectors must implement this interface:</p> <pre><code>from typing import Protocol\nimport numpy as np\nfrom pipeline.types import Block\n\nclass Detector(Protocol):\n    \"\"\"Protocol for layout detectors.\"\"\"\n\n    def detect(self, image: np.ndarray) -&gt; list[Block]:\n        \"\"\"Detect layout blocks in image.\n\n        Args:\n            image: Input image as numpy array (H, W, C)\n\n        Returns:\n            List of detected blocks\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/types/#sorter-protocol","title":"Sorter Protocol","text":"<p>All sorters must implement this interface:</p> <pre><code>class Sorter(Protocol):\n    \"\"\"Protocol for reading order sorters.\"\"\"\n\n    def sort(\n        self,\n        blocks: list[Block],\n        image: np.ndarray,\n        **kwargs,\n    ) -&gt; list[Block]:\n        \"\"\"Sort blocks in reading order.\n\n        Args:\n            blocks: Detected blocks\n            image: Original page image\n            **kwargs: Additional arguments\n\n        Returns:\n            Sorted blocks with order field\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/types/#recognizer-protocol","title":"Recognizer Protocol","text":"<p>All recognizers must implement this interface:</p> <pre><code>class Recognizer(Protocol):\n    \"\"\"Protocol for text recognizers.\"\"\"\n\n    def process_blocks(\n        self,\n        image: np.ndarray,\n        blocks: Sequence[Block],\n    ) -&gt; list[Block]:\n        \"\"\"Extract text from blocks.\n\n        Args:\n            image: Full page image\n            blocks: Blocks to process\n\n        Returns:\n            Blocks with text field\n        \"\"\"\n        ...\n\n    def correct_text(self, text: str) -&gt; str | dict[str, Any]:\n        \"\"\"Correct extracted text.\n\n        Args:\n            text: Raw extracted text\n\n        Returns:\n            Corrected text or dict with metadata\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/types/#page-and-document","title":"Page and Document","text":""},{"location":"api/types/#page","title":"Page","text":"<pre><code>@dataclass\nclass Page:\n    \"\"\"Processed page with all metadata.\"\"\"\n\n    page_num: int\n    width: int\n    height: int\n    blocks: list[Block]\n    text: str                      # Rendered text\n    corrected_text: str | None     # Page-level corrected\n    correction_ratio: float | None\n    processing_time_seconds: float\n    processed_at: str              # ISO timestamp\n    status: str                    # \"complete\", \"partial\", \"error\"\n</code></pre>"},{"location":"api/types/#document","title":"Document","text":"<pre><code>@dataclass\nclass Document:\n    \"\"\"Complete document with all pages.\"\"\"\n\n    name: str\n    path: Path\n    num_pages: int\n    processed_pages: int\n    pages: list[Page]\n    output_directory: Path\n    processed_at: str\n    status_summary: dict[str, int]\n</code></pre>"},{"location":"api/types/#see-also","title":"See Also","text":"<ul> <li>BBox Formats Guide - Detailed format conversion examples</li> <li>Detector Block Types - Block type mappings per detector</li> </ul>"},{"location":"architecture/detectors/","title":"Detectors","text":"<p>Available layout detection models and their characteristics.</p>"},{"location":"architecture/detectors/#overview","title":"Overview","text":"<p>Layout detectors identify document regions (text, tables, figures, etc.) in page images. The pipeline supports multiple detector backends with different trade-offs between speed and accuracy.</p>"},{"location":"architecture/detectors/#detector-comparison","title":"Detector Comparison","text":"Detector Source Technology Speed Quality Block Types <code>doclayout-yolo</code> This project YOLO v8 \u26a1\u26a1\u26a1 \u2b50\u2b50\u2b50 7 <code>mineru-doclayout-yolo</code> MinerU YOLO v8 \u26a1\u26a1 \u2b50\u2b50\u2b50 10 <code>paddleocr-doclayout-v2</code> PaddleOCR PP-DocLayoutV2 \u26a1\u26a1 \u2b50\u2b50\u2b50\u2b50 25 <code>mineru-vlm</code> MinerU VLM-based \u26a1 \u2b50\u2b50\u2b50\u2b50 25+"},{"location":"architecture/detectors/#doclayout-yolo","title":"DocLayout-YOLO","text":"<p>Speed: Fast | Quality: Good | GPU Memory: ~2GB</p> <p>This project's optimized DocLayout-YOLO detector based on Ultralytics YOLO v8.</p>"},{"location":"architecture/detectors/#block-types","title":"Block Types","text":"Type Description <code>title</code> Document titles and headings <code>plain text</code> / <code>text</code> Regular paragraph text <code>figure</code> Images and diagrams <code>table</code> Tables <code>equation</code> Mathematical formulas <code>list</code> Bullet/numbered lists"},{"location":"architecture/detectors/#usage","title":"Usage","text":"<pre><code>from pipeline.layout.detection import create_detector\n\ndetector = create_detector(\n    \"doclayout-yolo\",\n    model_path=\"path/to/model.pt\",  # Optional custom model\n    confidence_threshold=0.5,\n    auto_batch_size=True,  # Auto-optimize batch size\n)\n\nblocks = detector.detect(image)\n</code></pre>"},{"location":"architecture/detectors/#cli","title":"CLI","text":"<pre><code>python main.py --input doc.pdf --detector doclayout-yolo --confidence 0.5\n</code></pre>"},{"location":"architecture/detectors/#paddleocr-pp-doclayoutv2","title":"PaddleOCR PP-DocLayoutV2","text":"<p>Speed: Medium | Quality: Very Good | GPU Memory: ~4GB</p> <p>PaddleOCR's PP-DocLayoutV2 detector with 25 category types and built-in pointer network for reading order.</p>"},{"location":"architecture/detectors/#block-types-25-categories","title":"Block Types (25 categories)","text":"Category Types Titles doc_title, paragraph_title Text text, sidebar_text, abstract, contents References reference, reference_content, footnote Page Elements header, footer, header_image, footer_image, page_number Formulas formula, formula_number, algorithm Tables table, table_title Images image, chart, chart_title, seal"},{"location":"architecture/detectors/#usage_1","title":"Usage","text":"<pre><code>detector = create_detector(\n    \"paddleocr-doclayout-v2\",\n    confidence_threshold=0.5,\n)\n</code></pre>"},{"location":"architecture/detectors/#cli_1","title":"CLI","text":"<pre><code>python main.py --input doc.pdf --detector paddleocr-doclayout-v2\n</code></pre>"},{"location":"architecture/detectors/#requirements","title":"Requirements","text":"<ul> <li>PaddleOCR v3.3.0+ (in <code>external/PaddleOCR/</code>)</li> <li>PaddlePaddle with CUDA support (recommended)</li> </ul>"},{"location":"architecture/detectors/#mineru-doclayout-yolo","title":"MinerU DocLayout-YOLO","text":"<p>Speed: Fast | Quality: Good | GPU Memory: ~2GB</p> <p>MinerU's implementation of DocLayout-YOLO with additional block types.</p>"},{"location":"architecture/detectors/#block-types_1","title":"Block Types","text":"Type Description <code>title</code> Document titles <code>plain text</code> Regular text <code>abandon</code> Discarded regions <code>figure</code> Images and diagrams <code>figure_caption</code> Figure captions <code>table</code> Tables <code>table_caption</code> Table captions <code>table_footnote</code> Table footnotes <code>isolate_formula</code> Standalone formulas <code>formula_caption</code> Formula captions"},{"location":"architecture/detectors/#usage_2","title":"Usage","text":"<pre><code>detector = create_detector(\"mineru-doclayout-yolo\")\n</code></pre>"},{"location":"architecture/detectors/#cli_2","title":"CLI","text":"<pre><code>python main.py --input doc.pdf --detector mineru-doclayout-yolo\n</code></pre>"},{"location":"architecture/detectors/#requirements_1","title":"Requirements","text":"<ul> <li>MinerU 2.5+ (in <code>external/MinerU/</code>)</li> </ul>"},{"location":"architecture/detectors/#mineru-vlm","title":"MinerU VLM","text":"<p>Speed: Slow | Quality: Excellent | GPU Memory: ~8GB</p> <p>Vision Language Model-based detection using MinerU's VLM pipeline.</p>"},{"location":"architecture/detectors/#block-types_2","title":"Block Types","text":"<p>Full MinerU 2.5 hierarchy with 25+ block types including:</p> <ul> <li>Content: text, title, list</li> <li>Images: image, image_body, image_caption, image_footnote</li> <li>Tables: table, table_body, table_caption, table_footnote</li> <li>Equations: interline_equation, inline_equation</li> <li>Code: code, code_body, code_caption, algorithm</li> <li>Page elements: header, footer, page_number, page_footnote</li> <li>References: ref_text, phonetic, aside_text, index</li> </ul>"},{"location":"architecture/detectors/#usage_3","title":"Usage","text":"<pre><code>detector = create_detector(\n    \"mineru-vlm\",\n    model_name=\"opendatalab/MinerU2.5-2509-1.2B\",\n    backend=\"hf\",  # or \"vllm\" for faster inference\n)\n</code></pre>"},{"location":"architecture/detectors/#cli_3","title":"CLI","text":"<pre><code>python main.py --input doc.pdf --detector mineru-vlm --detector-backend vllm\n</code></pre>"},{"location":"architecture/detectors/#requirements_2","title":"Requirements","text":"<ul> <li>MinerU 2.5+ (in <code>external/MinerU/</code>)</li> <li>Large GPU (8GB+ VRAM recommended)</li> </ul>"},{"location":"architecture/detectors/#backend-selection","title":"Backend Selection","text":"<p>Detectors can use different inference backends:</p> Backend Description Supported By <code>pytorch</code> Native PyTorch doclayout-yolo, mineru-doclayout-yolo <code>paddle</code> PaddlePaddle paddleocr-doclayout-v2 <code>hf</code> HuggingFace Transformers mineru-vlm <code>vllm</code> vLLM high-throughput mineru-vlm <code>pt-ray</code> PyTorch + Ray multi-GPU doclayout-yolo <pre><code># Explicit backend selection\npython main.py --input doc.pdf --detector mineru-vlm --detector-backend vllm\n</code></pre>"},{"location":"architecture/detectors/#choosing-a-detector","title":"Choosing a Detector","text":""},{"location":"architecture/detectors/#recommended-configurations","title":"Recommended Configurations","text":"Use Case Detector Reason Fast processing <code>doclayout-yolo</code> Fastest, good quality High quality <code>paddleocr-doclayout-v2</code> 25 types, built-in ordering Complex documents <code>mineru-vlm</code> Best understanding, slowest MinerU integration <code>mineru-doclayout-yolo</code> MinerU ecosystem"},{"location":"architecture/detectors/#decision-matrix","title":"Decision Matrix","text":"<pre><code>graph TD\n    A[Need fast processing?] --&gt;|Yes| B[doclayout-yolo]\n    A --&gt;|No| C[Need detailed types?]\n    C --&gt;|Yes| D[paddleocr-doclayout-v2]\n    C --&gt;|No| E[Complex layouts?]\n    E --&gt;|Yes| F[mineru-vlm]\n    E --&gt;|No| B\n</code></pre>"},{"location":"architecture/detectors/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Tested on NVIDIA RTX 3090 with 300 DPI images:</p> Detector Pages/min GPU Memory Init Time <code>doclayout-yolo</code> ~60 2 GB 2s <code>paddleocr-doclayout-v2</code> ~40 4 GB 5s <code>mineru-doclayout-yolo</code> ~50 2 GB 3s <code>mineru-vlm</code> ~10 8 GB 30s"},{"location":"architecture/detectors/#see-also","title":"See Also","text":"<ul> <li>Detection API - API reference</li> <li>Detector Block Types - Type mappings</li> <li>Sorters - Reading order algorithms</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>VLM OCR Pipeline is built on a modular, 8-stage architecture that separates concerns and allows flexible configuration of each processing step.</p>"},{"location":"architecture/overview/#core-design-principles","title":"Core Design Principles","text":""},{"location":"architecture/overview/#1-modular-stages","title":"1. Modular Stages","text":"<p>Each stage has a single responsibility and can be tested independently:</p> <pre><code># Each stage is a self-contained module\ninput_stage = InputStage(...)\ndetection_stage = DetectionStage(detector)\nordering_stage = OrderingStage(sorter)\nrecognition_stage = RecognitionStage(recognizer)\n</code></pre>"},{"location":"architecture/overview/#2-factory-pattern","title":"2. Factory Pattern","text":"<p>Detectors and sorters are created through factory functions:</p> <pre><code># Centralized creation with validation\ndetector = create_detector(\"doclayout-yolo\")\nsorter = create_sorter(\"mineru-xycut\")\nvalidate_combination(detector_name, sorter_name)  # Ensures compatibility\n</code></pre>"},{"location":"architecture/overview/#3-protocol-based-interfaces","title":"3. Protocol-Based Interfaces","text":"<p>Type-safe plugin system using Python protocols:</p> <pre><code>class Detector(Protocol):\n    \"\"\"All detectors must implement this interface.\"\"\"\n    def detect(self, image: np.ndarray) -&gt; list[Block]:\n        ...\n\nclass Sorter(Protocol):\n    \"\"\"All sorters must implement this interface.\"\"\"\n    def sort(self, blocks: list[Block], image: np.ndarray) -&gt; list[Block]:\n        ...\n</code></pre>"},{"location":"architecture/overview/#4-unified-bbox-system","title":"4. Unified BBox System","text":"<p>All bounding boxes use the <code>BBox</code> dataclass for automatic format conversion:</p> <pre><code>@dataclass\nclass BBox:\n    x0: int  # Left\n    y0: int  # Top\n    x1: int  # Right\n    y1: int  # Bottom\n\n    # Automatic conversion from 6+ formats\n    @classmethod\n    def from_yolo(cls, bbox, image_width, image_height) -&gt; BBox:\n        ...\n\n    @classmethod\n    def from_mineru(cls, bbox) -&gt; BBox:\n        ...\n</code></pre> <p>Internal: xyxy (corners) JSON Output: xywh (x, y, width, height)</p>"},{"location":"architecture/overview/#8-stage-pipeline","title":"8-Stage Pipeline","text":"<pre><code>graph TD\n    A[\ud83d\udcc4 Stage 1: Input] --&gt; B[\ud83d\udd0d Stage 2: Detection]\n    B --&gt; C[\ud83d\udcca Stage 3: Ordering]\n    C --&gt; D[\ud83d\udcdd Stage 4: Recognition]\n    D --&gt; E[\u270f\ufe0f Stage 5: Block Correction]\n    E --&gt; F[\ud83d\udccb Stage 6: Rendering]\n    F --&gt; G[\ud83d\udd27 Stage 7: Page Correction]\n    G --&gt; H[\ud83d\udcbe Stage 8: Output]\n\n    style A fill:#e1f5ff\n    style B fill:#fff3e1\n    style C fill:#e8f5e9\n    style D fill:#f3e5f5\n    style E fill:#fce4ec\n    style F fill:#fff9e1\n    style G fill:#e0f2f1\n    style H fill:#f1f8e9\n</code></pre>"},{"location":"architecture/overview/#stage-1-input","title":"Stage 1: Input","text":"<p>Responsibility: Load documents and extract auxiliary information</p> <ul> <li>Renders PDF pages to images (pdf2image)</li> <li>Loads image files directly (OpenCV)</li> <li>Extracts text spans with font metadata (PyMuPDF)</li> </ul> <p>Output: <code>(image: np.ndarray, auxiliary_info: dict)</code></p>"},{"location":"architecture/overview/#stage-2-detection","title":"Stage 2: Detection","text":"<p>Responsibility: Detect layout blocks</p> <ul> <li>Runs selected detector (DocLayout-YOLO, PaddleOCR, MinerU, olmOCR)</li> <li>Returns blocks with bounding boxes, types, and confidence scores</li> <li>Detects: text, title, table, figure, equation, list</li> </ul> <p>Output: <code>list[Block]</code> with <code>bbox</code>, <code>type</code>, <code>confidence</code></p>"},{"location":"architecture/overview/#stage-3-ordering","title":"Stage 3: Ordering","text":"<p>Responsibility: Analyze reading order</p> <ul> <li>Runs selected sorter (PyMuPDF, LayoutReader, XY-Cut, VLM)</li> <li>Adds <code>order</code> field to blocks</li> <li>Optionally adds <code>column_index</code> for multi-column documents</li> </ul> <p>Output: <code>list[Block]</code> sorted with <code>order</code> field</p>"},{"location":"architecture/overview/#stage-4-recognition","title":"Stage 4: Recognition","text":"<p>Responsibility: Extract text from blocks</p> <ul> <li>Crops block images from full page</li> <li>Sends to VLM API or local model</li> <li>Uses block-type-specific prompts</li> <li>Handles special content (tables, figures)</li> </ul> <p>Output: <code>list[Block]</code> with <code>text</code> field populated</p>"},{"location":"architecture/overview/#stage-5-block-correction","title":"Stage 5: Block Correction","text":"<p>Responsibility: Block-level text correction (placeholder)</p> <ul> <li>Currently disabled by default</li> <li>Future: VLM-based correction at block level</li> <li>Currently just copies <code>text</code> to <code>corrected_text</code></li> </ul> <p>Output: <code>list[Block]</code> with <code>corrected_text</code></p>"},{"location":"architecture/overview/#stage-6-rendering","title":"Stage 6: Rendering","text":"<p>Responsibility: Convert to output format</p> <ul> <li>Assembles blocks in reading order</li> <li>Generates Markdown or plaintext</li> <li>Uses auxiliary info for enhanced formatting</li> <li>Supports multiple rendering strategies</li> </ul> <p>Output: <code>str</code> (Markdown/plaintext)</p>"},{"location":"architecture/overview/#stage-7-page-correction","title":"Stage 7: Page Correction","text":"<p>Responsibility: Page-level VLM correction</p> <ul> <li>Sends entire page text to VLM</li> <li>Corrects OCR errors and formatting</li> <li>Calculates correction ratio</li> <li>Handles rate limits</li> <li>Skipped for local models</li> </ul> <p>Output: <code>(corrected_text: str, correction_ratio: float, should_stop: bool)</code></p>"},{"location":"architecture/overview/#stage-8-output","title":"Stage 8: Output","text":"<p>Responsibility: Save results</p> <ul> <li>Builds <code>Page</code> objects with metadata</li> <li>Saves JSON and Markdown files</li> <li>Generates document summaries</li> <li>Creates output directory structure</li> </ul> <p>Output: Saved files in <code>output/{model}/{document}/</code></p>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#block-evolution-through-stages","title":"Block Evolution Through Stages","text":"<p>A block's data evolves as it passes through stages:</p> <pre><code># After Detection (Stage 2)\nBlock(\n    type=\"text\",\n    bbox=BBox(100, 50, 500, 120),\n    detection_confidence=0.95,\n    source=\"doclayout-yolo\"\n)\n\n# After Ordering (Stage 3)\nBlock(\n    type=\"text\",\n    bbox=BBox(100, 50, 500, 120),\n    detection_confidence=0.95,\n    order=0,  # Added\n    column_index=0,  # Added (optional)\n    source=\"doclayout-yolo\"\n)\n\n# After Recognition (Stage 4)\nBlock(\n    type=\"text\",\n    bbox=BBox(100, 50, 500, 120),\n    detection_confidence=0.95,\n    order=0,\n    text=\"Chapter 1: Introduction\",  # Added\n    source=\"doclayout-yolo\"\n)\n\n# After Block Correction (Stage 5)\nBlock(\n    type=\"text\",\n    bbox=BBox(100, 50, 500, 120),\n    detection_confidence=0.95,\n    order=0,\n    text=\"Chapter 1: Introduction\",\n    corrected_text=\"Chapter 1: Introduction\",  # Added\n    source=\"doclayout-yolo\"\n)\n</code></pre>"},{"location":"architecture/overview/#extensibility","title":"Extensibility","text":""},{"location":"architecture/overview/#adding-a-new-detector","title":"Adding a New Detector","text":"<ol> <li>Implement the <code>Detector</code> protocol</li> <li>Register in <code>create_detector()</code> factory</li> <li>Add to <code>validate_combination()</code> if needed</li> </ol> <pre><code># pipeline/layout/detection/my_detector.py\nclass MyDetector:\n    def detect(self, image: np.ndarray) -&gt; list[Block]:\n        # Your detection logic\n        return blocks\n\n# pipeline/layout/detection/__init__.py\ndef create_detector(name: str, **kwargs) -&gt; Detector:\n    if name == \"my-detector\":\n        return MyDetector(**kwargs)\n</code></pre>"},{"location":"architecture/overview/#adding-a-new-sorter","title":"Adding a New Sorter","text":"<ol> <li>Implement the <code>Sorter</code> protocol</li> <li>Register in <code>create_sorter()</code> factory</li> <li>Add combination validation</li> </ol> <pre><code># pipeline/layout/ordering/my_sorter.py\nclass MySorter:\n    def sort(self, blocks: list[Block], image: np.ndarray) -&gt; list[Block]:\n        # Your sorting logic\n        return sorted_blocks\n\n# pipeline/layout/ordering/__init__.py\ndef create_sorter(name: str, **kwargs) -&gt; Sorter:\n    if name == \"my-sorter\":\n        return MySorter(**kwargs)\n</code></pre>"},{"location":"architecture/overview/#adding-a-new-recognizer","title":"Adding a New Recognizer","text":"<ol> <li>Implement the <code>Recognizer</code> protocol</li> <li>Add to TextRecognizer backend selection</li> </ol> <pre><code>class Recognizer(Protocol):\n    def process_blocks(self, image: np.ndarray, blocks: Sequence[Block]) -&gt; list[Block]:\n        ...\n\n    def correct_text(self, text: str) -&gt; str | dict[str, Any]:\n        ...\n</code></pre>"},{"location":"architecture/overview/#error-handling","title":"Error Handling","text":"<p>The pipeline uses a comprehensive error handling system:</p> <ul> <li>Custom exceptions: Specific exception types for different errors</li> <li>Graceful degradation: Continue processing on non-critical failures</li> <li>Error logging: Detailed logs with stack traces</li> <li>Rate limit handling: Automatic retry and backoff</li> </ul> <p>See Error Handling Guide for details.</p>"},{"location":"architecture/overview/#testing-strategy","title":"Testing Strategy","text":"<p>Each component can be tested independently:</p> <pre><code># Test detector\ndetector = create_detector(\"doclayout-yolo\")\nblocks = detector.detect(test_image)\nassert len(blocks) &gt; 0\n\n# Test sorter\nsorter = create_sorter(\"mineru-xycut\")\nsorted_blocks = sorter.sort(blocks, test_image)\nassert sorted_blocks[0].order is not None\n\n# Test full pipeline\npipeline = Pipeline(detector=\"doclayout-yolo\", sorter=\"mineru-xycut\")\nresult = pipeline.process_single_pdf(test_pdf)\n</code></pre>"},{"location":"architecture/overview/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/overview/#caching","title":"Caching","text":"<p>The recognition stage uses content-based caching:</p> <pre><code># Cache key = hash(block_image + block_type + prompt)\ncache_key = hashlib.sha256(\n    block_image.tobytes() +\n    block_type.encode() +\n    prompt.encode()\n).hexdigest()\n</code></pre>"},{"location":"architecture/overview/#rate-limiting","title":"Rate Limiting","text":"<p>Gemini API rate limiting is handled globally:</p> <pre><code>rate_limiter.wait_if_needed(estimated_tokens=1000)\n# Automatic throttling based on tier limits\n</code></pre>"},{"location":"architecture/overview/#memory-management","title":"Memory Management","text":"<ul> <li>Block images are deleted after recognition</li> <li>Garbage collection is triggered after each block</li> <li>Temporary files are cleaned up automatically</li> </ul>"},{"location":"architecture/overview/#configuration","title":"Configuration","text":"<p>Pipeline behavior is controlled through:</p> <ol> <li>CLI Arguments: Runtime configuration</li> <li>Environment Variables: API keys, paths</li> <li>YAML Files: Prompts, rate limits</li> <li>Factory Functions: Component selection</li> </ol>"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Pipeline Stages - Detailed stage documentation</li> <li>Detectors - Available detection models</li> <li>Sorters - Reading order algorithms</li> <li>Recognizers - Text extraction backends</li> </ul>"},{"location":"architecture/pipeline-stages/","title":"Pipeline Stages","text":"<p>Detailed documentation for each of the 8 pipeline stages.</p> <p>For a high-level overview, see Architecture Overview.</p>"},{"location":"architecture/pipeline-stages/#stage-overview","title":"Stage Overview","text":"<pre><code>graph TD\n    A[Stage 1: Input] --&gt; B[Stage 2: Detection]\n    B --&gt; C[Stage 3: Ordering]\n    C --&gt; D[Stage 4: Recognition]\n    D --&gt; E[Stage 5: Block Correction]\n    E --&gt; F[Stage 6: Rendering]\n    F --&gt; G[Stage 7: Page Correction]\n    G --&gt; H[Stage 8: Output]\n\n    style A fill:#e1f5ff\n    style B fill:#fff3e1\n    style C fill:#e8f5e9\n    style D fill:#f3e5f5\n    style E fill:#fce4ec\n    style F fill:#fff9e1\n    style G fill:#e0f2f1\n    style H fill:#f1f8e9\n</code></pre>"},{"location":"architecture/pipeline-stages/#stage-1-input","title":"Stage 1: Input","text":"<p>File: <code>pipeline/stages/input_stage.py</code></p> <p>Responsibility: Load documents and extract auxiliary information</p>"},{"location":"architecture/pipeline-stages/#what-it-does","title":"What It Does","text":"<ul> <li>Renders PDF pages to images using pdf2image (configurable DPI)</li> <li>Loads image files directly using OpenCV</li> <li>Extracts text spans with font metadata from PDFs (PyMuPDF)</li> <li>Supports dual resolution mode (lower DPI for detection, higher for recognition)</li> </ul>"},{"location":"architecture/pipeline-stages/#inputoutput","title":"Input/Output","text":"Input Output PDF path + page number <code>np.ndarray</code> (image) Image path <code>np.ndarray</code> (image) PDF path <code>dict</code> (auxiliary_info with text_spans)"},{"location":"architecture/pipeline-stages/#key-methods","title":"Key Methods","text":"<pre><code>class InputStage:\n    def load_pdf_page(self, pdf_path: Path, page_num: int) -&gt; np.ndarray:\n        \"\"\"Render PDF page to image.\"\"\"\n\n    def load_image(self, image_path: Path) -&gt; np.ndarray:\n        \"\"\"Load image file.\"\"\"\n\n    def extract_auxiliary_info(self, pdf_path: Path, page_num: int) -&gt; dict:\n        \"\"\"Extract text spans with font info from PDF.\"\"\"\n</code></pre>"},{"location":"architecture/pipeline-stages/#configuration","title":"Configuration","text":"Parameter Default Description <code>dpi</code> 200 Single resolution DPI <code>detection_dpi</code> 150 Detection DPI (dual mode) <code>recognition_dpi</code> 300 Recognition DPI (dual mode) <code>use_dual_resolution</code> False Enable dual resolution mode"},{"location":"architecture/pipeline-stages/#stage-2-detection","title":"Stage 2: Detection","text":"<p>File: <code>pipeline/stages/detection_stage.py</code></p> <p>Responsibility: Detect layout blocks in page images</p>"},{"location":"architecture/pipeline-stages/#what-it-does_1","title":"What It Does","text":"<ul> <li>Runs selected detector (DocLayout-YOLO, PaddleOCR, MinerU)</li> <li>Returns blocks with bounding boxes, types, and confidence scores</li> <li>Extracts column layout information if available</li> <li>Supports Ray-based distributed detection for multi-GPU</li> </ul>"},{"location":"architecture/pipeline-stages/#inputoutput_1","title":"Input/Output","text":"Input Output <code>np.ndarray</code> (image) <code>list[Block]</code>"},{"location":"architecture/pipeline-stages/#block-data-after-detection","title":"Block Data After Detection","text":"<pre><code>Block(\n    type=\"text\",  # or \"title\", \"table\", \"image\", etc.\n    bbox=BBox(100, 50, 500, 120),\n    detection_confidence=0.95,\n    source=\"doclayout-yolo\",\n)\n</code></pre>"},{"location":"architecture/pipeline-stages/#available-detectors","title":"Available Detectors","text":"Detector Speed Quality Block Types <code>doclayout-yolo</code> Fast Good 7 types <code>paddleocr-doclayout-v2</code> Medium Very Good 25 types <code>mineru-doclayout-yolo</code> Fast Good 10 types <code>mineru-vlm</code> Slow Excellent 25+ types"},{"location":"architecture/pipeline-stages/#stage-3-ordering","title":"Stage 3: Ordering","text":"<p>File: <code>pipeline/stages/ordering_stage.py</code></p> <p>Responsibility: Analyze reading order of detected blocks</p>"},{"location":"architecture/pipeline-stages/#what-it-does_2","title":"What It Does","text":"<ul> <li>Runs selected sorter algorithm</li> <li>Adds <code>order</code> field to blocks for correct reading sequence</li> <li>Optionally adds <code>column_index</code> for multi-column documents</li> <li>Scales bounding boxes if using dual resolution mode</li> </ul>"},{"location":"architecture/pipeline-stages/#inputoutput_2","title":"Input/Output","text":"Input Output <code>list[Block]</code> + image <code>list[Block]</code> (sorted, with <code>order</code> field)"},{"location":"architecture/pipeline-stages/#block-data-after-ordering","title":"Block Data After Ordering","text":"<pre><code>Block(\n    type=\"text\",\n    bbox=BBox(100, 50, 500, 120),\n    detection_confidence=0.95,\n    order=0,  # Added by sorter\n    column_index=0,  # Added by multi-column sorters\n    source=\"doclayout-yolo\",\n)\n</code></pre>"},{"location":"architecture/pipeline-stages/#available-sorters","title":"Available Sorters","text":"Sorter Algorithm Multi-Column Speed <code>pymupdf</code> Font analysis Yes Fast <code>mineru-xycut</code> XY-Cut No Fast <code>mineru-layoutreader</code> LayoutLMv3 Yes Medium <code>olmocr-vlm</code> VLM reasoning Yes Slow"},{"location":"architecture/pipeline-stages/#stage-4-recognition","title":"Stage 4: Recognition","text":"<p>File: <code>pipeline/stages/recognition_stage.py</code></p> <p>Responsibility: Extract text from each block</p>"},{"location":"architecture/pipeline-stages/#what-it-does_3","title":"What It Does","text":"<ul> <li>Crops block images from full page using BBox</li> <li>Sends cropped images to VLM API or local model</li> <li>Uses block-type-specific prompts for optimal results</li> <li>Handles special content (tables, figures) with appropriate prompts</li> <li>Supports Ray-based distributed recognition</li> </ul>"},{"location":"architecture/pipeline-stages/#inputoutput_3","title":"Input/Output","text":"Input Output <code>list[Block]</code> + image <code>list[Block]</code> (with <code>text</code> field)"},{"location":"architecture/pipeline-stages/#block-data-after-recognition","title":"Block Data After Recognition","text":"<pre><code>Block(\n    type=\"text\",\n    bbox=BBox(100, 50, 500, 120),\n    detection_confidence=0.95,\n    order=0,\n    text=\"Chapter 1: Introduction\",  # Added by recognizer\n    source=\"doclayout-yolo\",\n)\n</code></pre>"},{"location":"architecture/pipeline-stages/#available-recognizers","title":"Available Recognizers","text":"Recognizer Type Cost Speed <code>gemini-2.5-flash</code> Cloud API Free tier Fast <code>gpt-4o</code> Cloud API Pay per token Medium <code>paddleocr-vl</code> Local Free Medium <code>deepseek-ocr</code> Local Free Medium"},{"location":"architecture/pipeline-stages/#stage-5-block-correction","title":"Stage 5: Block Correction","text":"<p>File: <code>pipeline/stages/block_correction_stage.py</code></p> <p>Responsibility: Block-level text correction (optional)</p>"},{"location":"architecture/pipeline-stages/#what-it-does_4","title":"What It Does","text":"<ul> <li>Applies VLM-based correction at individual block level</li> <li>Currently a placeholder stage (disabled by default)</li> <li>Copies <code>text</code> to <code>corrected_text</code> when disabled</li> </ul>"},{"location":"architecture/pipeline-stages/#inputoutput_4","title":"Input/Output","text":"Input Output <code>list[Block]</code> <code>list[Block]</code> (with <code>corrected_text</code> field)"},{"location":"architecture/pipeline-stages/#configuration_1","title":"Configuration","text":"<p>Enable with <code>--block-correction</code> CLI flag or <code>enable_block_correction=True</code> in config.</p>"},{"location":"architecture/pipeline-stages/#block-data-after-correction","title":"Block Data After Correction","text":"<pre><code>Block(\n    type=\"text\",\n    bbox=BBox(100, 50, 500, 120),\n    detection_confidence=0.95,\n    order=0,\n    text=\"Chapter 1: Introduction\",\n    corrected_text=\"Chapter 1: Introduction\",  # Added\n    source=\"doclayout-yolo\",\n)\n</code></pre>"},{"location":"architecture/pipeline-stages/#stage-6-rendering","title":"Stage 6: Rendering","text":"<p>File: <code>pipeline/stages/rendering_stage.py</code></p> <p>Responsibility: Convert processed blocks to output format</p>"},{"location":"architecture/pipeline-stages/#what-it-does_5","title":"What It Does","text":"<ul> <li>Assembles blocks in reading order</li> <li>Generates Markdown or plaintext output</li> <li>Uses auxiliary info (font sizes) for enhanced formatting</li> <li>Supports multiple rendering strategies</li> </ul>"},{"location":"architecture/pipeline-stages/#inputoutput_5","title":"Input/Output","text":"Input Output <code>list[Block]</code> + auxiliary_info <code>str</code> (Markdown/plaintext)"},{"location":"architecture/pipeline-stages/#rendering-strategies","title":"Rendering Strategies","text":"Strategy Description Use Case Block Type-Based Maps block types to Markdown Default, fast Font Size-Based Uses font sizes for headers Precise formatting"},{"location":"architecture/pipeline-stages/#example-output","title":"Example Output","text":"<pre><code># Chapter 1: Introduction\n\nThis document describes the VLM OCR Pipeline...\n\n## Key Features\n\n- Feature 1\n- Feature 2\n</code></pre>"},{"location":"architecture/pipeline-stages/#stage-7-page-correction","title":"Stage 7: Page Correction","text":"<p>File: <code>pipeline/stages/page_correction_stage.py</code></p> <p>Responsibility: Page-level VLM correction (optional)</p>"},{"location":"architecture/pipeline-stages/#what-it-does_6","title":"What It Does","text":"<ul> <li>Sends entire page text to VLM for holistic correction</li> <li>Corrects OCR errors and improves formatting consistency</li> <li>Calculates correction ratio (how much text changed)</li> <li>Handles rate limits and returns early if needed</li> </ul>"},{"location":"architecture/pipeline-stages/#inputoutput_6","title":"Input/Output","text":"Input Output <code>str</code> (page text) <code>PageCorrectionResult</code>"},{"location":"architecture/pipeline-stages/#pagecorrectionresult","title":"PageCorrectionResult","text":"<pre><code>@dataclass\nclass PageCorrectionResult:\n    corrected_text: str\n    correction_ratio: float  # 0.0 = no change, 1.0 = completely different\n    should_stop: bool  # True if rate limit hit\n</code></pre>"},{"location":"architecture/pipeline-stages/#configuration_2","title":"Configuration","text":"<ul> <li>Enable with <code>--page-correction</code> CLI flag</li> <li>Skipped for local models (PaddleOCR-VL) by default</li> <li>Controlled by <code>enable_page_correction=True</code> in config</li> </ul>"},{"location":"architecture/pipeline-stages/#stage-8-output","title":"Stage 8: Output","text":"<p>File: <code>pipeline/stages/output_stage.py</code></p> <p>Responsibility: Save results and generate summaries</p>"},{"location":"architecture/pipeline-stages/#what-it-does_7","title":"What It Does","text":"<ul> <li>Builds <code>Page</code> objects with all metadata</li> <li>Saves page results as JSON files</li> <li>Generates Markdown output files</li> <li>Creates document-level summaries</li> <li>Creates final output directory structure</li> </ul>"},{"location":"architecture/pipeline-stages/#inputoutput_7","title":"Input/Output","text":"Input Output All processed data JSON + Markdown files"},{"location":"architecture/pipeline-stages/#output-structure","title":"Output Structure","text":"<pre><code>output/{model}/{document}/\n\u251c\u2500\u2500 page_1.json\n\u251c\u2500\u2500 page_1.md\n\u251c\u2500\u2500 page_2.json\n\u251c\u2500\u2500 page_2.md\n\u2514\u2500\u2500 summary.json\n</code></pre>"},{"location":"architecture/pipeline-stages/#key-methods_1","title":"Key Methods","text":"<pre><code>class OutputStage:\n    def build_page_result(self, ...) -&gt; Page:\n        \"\"\"Build Page object with all metadata.\"\"\"\n\n    def save_page_output(self, output_dir: Path, page_num: int, page: Page):\n        \"\"\"Save page as JSON and Markdown.\"\"\"\n\n    def create_pdf_summary(self, ...) -&gt; Document:\n        \"\"\"Create document summary with all pages.\"\"\"\n</code></pre>"},{"location":"architecture/pipeline-stages/#stage-data-flow","title":"Stage Data Flow","text":""},{"location":"architecture/pipeline-stages/#complete-block-evolution","title":"Complete Block Evolution","text":"<pre><code># After Stage 2 (Detection)\nBlock(type=\"text\", bbox=BBox(...), detection_confidence=0.95, source=\"...\")\n\n# After Stage 3 (Ordering)\nBlock(type=\"text\", bbox=BBox(...), detection_confidence=0.95, order=0, column_index=0, source=\"...\")\n\n# After Stage 4 (Recognition)\nBlock(type=\"text\", bbox=BBox(...), detection_confidence=0.95, order=0, text=\"...\", source=\"...\")\n\n# After Stage 5 (Block Correction)\nBlock(type=\"text\", bbox=BBox(...), detection_confidence=0.95, order=0, text=\"...\", corrected_text=\"...\", source=\"...\")\n</code></pre>"},{"location":"architecture/pipeline-stages/#stage-timing","title":"Stage Timing","text":"<p>Typical processing time distribution for a single page:</p> Stage Time % of Total Input ~0.5s 5% Detection ~1.0s 10% Ordering ~0.2s 2% Recognition ~5.0s 50% Block Correction ~0.0s 0% (disabled) Rendering ~0.1s 1% Page Correction ~3.0s 30% Output ~0.2s 2%"},{"location":"architecture/pipeline-stages/#see-also","title":"See Also","text":"<ul> <li>Architecture Overview - High-level design</li> <li>Detectors - Detection models</li> <li>Sorters - Ordering algorithms</li> <li>Recognizers - Text extraction backends</li> </ul>"},{"location":"architecture/recognizers/","title":"Recognizers","text":"<p>Text recognition backends for extracting text from detected blocks.</p>"},{"location":"architecture/recognizers/#overview","title":"Overview","text":"<p>Recognizers extract text from detected layout blocks using Vision Language Models (VLMs). The pipeline supports both cloud APIs and local models.</p>"},{"location":"architecture/recognizers/#recognizer-comparison","title":"Recognizer Comparison","text":""},{"location":"architecture/recognizers/#cloud-vlm-apis","title":"Cloud VLM APIs","text":"Recognizer Provider Speed Cost Quality <code>gemini-2.5-flash</code> Google Fast Free tier \u2b50\u2b50\u2b50\u2b50 <code>gemini-2.0-flash</code> Google Fast Free tier \u2b50\u2b50\u2b50\u2b50 <code>gpt-4o</code> OpenAI Medium Pay per token \u2b50\u2b50\u2b50\u2b50\u2b50 <code>gpt-4-turbo</code> OpenAI Medium Pay per token \u2b50\u2b50\u2b50\u2b50\u2b50"},{"location":"architecture/recognizers/#local-models","title":"Local Models","text":"Recognizer Parameters Languages Speed Quality <code>paddleocr-vl</code> 0.9B 109 Medium \u2b50\u2b50\u2b50\u2b50 <code>deepseek-ocr</code> - Multi Medium \u2b50\u2b50\u2b50"},{"location":"architecture/recognizers/#gemini-google","title":"Gemini (Google)","text":"<p>Type: Cloud API | Cost: Free tier available</p> <p>Google's Gemini Vision models for text extraction.</p>"},{"location":"architecture/recognizers/#models","title":"Models","text":"Model Speed Context Best For <code>gemini-2.5-flash</code> Fast 1M tokens General use, recommended <code>gemini-2.0-flash</code> Fast 1M tokens Alternative"},{"location":"architecture/recognizers/#rate-limits-free-tier","title":"Rate Limits (Free Tier)","text":"Limit Value Requests per minute 15 Tokens per minute 1,500,000 Requests per day 1,500"},{"location":"architecture/recognizers/#usage","title":"Usage","text":"<pre><code>from pipeline.recognition import TextRecognizer\n\nrecognizer = TextRecognizer(\n    backend=\"gemini\",\n    model=\"gemini-2.5-flash\",\n    gemini_tier=\"free\",  # free, tier1, tier2, tier3\n)\n</code></pre>"},{"location":"architecture/recognizers/#cli","title":"CLI","text":"<pre><code>export GEMINI_API_KEY=\"your_api_key\"\npython main.py --input doc.pdf --recognizer gemini-2.5-flash --gemini-tier free\n</code></pre>"},{"location":"architecture/recognizers/#check-rate-limit-status","title":"Check Rate Limit Status","text":"<pre><code>python main.py --rate-limit-status --recognizer gemini-2.5-flash --gemini-tier free\n</code></pre>"},{"location":"architecture/recognizers/#openai","title":"OpenAI","text":"<p>Type: Cloud API | Cost: Pay per token</p> <p>OpenAI's GPT-4 Vision models for high-quality text extraction.</p>"},{"location":"architecture/recognizers/#models_1","title":"Models","text":"Model Speed Context Best For <code>gpt-4o</code> Medium 128K tokens High quality <code>gpt-4-turbo</code> Medium 128K tokens Alternative <code>gpt-4o-mini</code> Fast 128K tokens Cost-effective"},{"location":"architecture/recognizers/#usage_1","title":"Usage","text":"<pre><code>recognizer = TextRecognizer(\n    backend=\"openai\",\n    model=\"gpt-4o\",\n)\n</code></pre>"},{"location":"architecture/recognizers/#cli_1","title":"CLI","text":"<pre><code>export OPENAI_API_KEY=\"your_api_key\"\npython main.py --input doc.pdf --recognizer gpt-4o\n</code></pre>"},{"location":"architecture/recognizers/#openrouter","title":"OpenRouter","text":"<p>Type: Cloud API | Cost: Varies by model</p> <p>Access multiple VLMs through OpenRouter's unified API.</p>"},{"location":"architecture/recognizers/#usage_2","title":"Usage","text":"<pre><code>recognizer = TextRecognizer(\n    backend=\"openai\",  # Uses OpenAI-compatible API\n    model=\"meta-llama/Llama-3-8b\",\n)\n</code></pre>"},{"location":"architecture/recognizers/#cli_2","title":"CLI","text":"<pre><code>export OPENROUTER_API_KEY=\"your_api_key\"\npython main.py --input doc.pdf --recognizer meta-llama/Llama-3-8b\n</code></pre>"},{"location":"architecture/recognizers/#paddleocr-vl","title":"PaddleOCR-VL","text":"<p>Type: Local Model | Cost: Free</p> <p>PaddleOCR-VL-0.9B is a 0.9B parameter Vision Language Model supporting 109 languages.</p>"},{"location":"architecture/recognizers/#architecture","title":"Architecture","text":"<ul> <li>Vision Encoder: NaViT (Native resolution Vision Transformer)</li> <li>Language Model: ERNIE-4.5-0.3B</li> <li>Total Parameters: 0.9B</li> </ul>"},{"location":"architecture/recognizers/#language-support","title":"Language Support","text":"<p>109 languages including: - CJK: Chinese, Japanese, Korean - European: English, French, German, Spanish, Italian, Portuguese - Middle Eastern: Arabic, Hebrew, Persian - Indian: Hindi, Bengali, Tamil, Telugu - Southeast Asian: Thai, Vietnamese, Indonesian - And many more...</p>"},{"location":"architecture/recognizers/#backend-options","title":"Backend Options","text":"Backend Speed Memory Notes <code>pytorch</code> Slow ~4GB Default <code>vllm</code> Fast ~6GB Recommended <code>sglang</code> Fast ~6GB Alternative"},{"location":"architecture/recognizers/#usage_3","title":"Usage","text":"<pre><code>recognizer = TextRecognizer(\n    backend=\"paddleocr-vl\",\n    recognizer_backend=\"vllm\",  # pytorch, vllm, sglang\n)\n</code></pre>"},{"location":"architecture/recognizers/#cli_3","title":"CLI","text":"<pre><code>python main.py --input doc.pdf \\\n    --recognizer paddleocr-vl \\\n    --recognizer-backend vllm\n</code></pre>"},{"location":"architecture/recognizers/#requirements","title":"Requirements","text":"<ul> <li>PaddleX v3.3.1 (in <code>external/PaddleX/</code>)</li> <li>GPU with CUDA support (4GB+ VRAM)</li> </ul>"},{"location":"architecture/recognizers/#deepseek-ocr","title":"DeepSeek-OCR","text":"<p>Type: Local Model | Cost: Free</p> <p>DeepSeek-OCR uses contextual optical compression for efficient text extraction.</p>"},{"location":"architecture/recognizers/#backend-options_1","title":"Backend Options","text":"Backend Speed Memory <code>hf</code> Slow ~4GB <code>vllm</code> Fast ~6GB"},{"location":"architecture/recognizers/#usage_4","title":"Usage","text":"<pre><code>recognizer = TextRecognizer(\n    backend=\"deepseek-ocr\",\n    recognizer_backend=\"hf\",  # hf, vllm\n)\n</code></pre>"},{"location":"architecture/recognizers/#cli_4","title":"CLI","text":"<pre><code>python main.py --input doc.pdf \\\n    --recognizer deepseek-ocr \\\n    --recognizer-backend vllm\n</code></pre>"},{"location":"architecture/recognizers/#requirements_1","title":"Requirements","text":"<ul> <li>DeepSeek-OCR (in <code>external/DeepSeek-OCR/</code>)</li> <li>GPU with CUDA support</li> </ul>"},{"location":"architecture/recognizers/#prompt-management","title":"Prompt Management","text":"<p>Recognizers use model-specific prompts stored in YAML files.</p>"},{"location":"architecture/recognizers/#prompt-directory-structure","title":"Prompt Directory Structure","text":"<pre><code>settings/prompts/\n\u251c\u2500\u2500 gemini/\n\u2502   \u251c\u2500\u2500 text_extraction.yaml\n\u2502   \u251c\u2500\u2500 content_analysis.yaml\n\u2502   \u2514\u2500\u2500 text_correction.yaml\n\u251c\u2500\u2500 openai/\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 paddleocr-vl/\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 default/\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"architecture/recognizers/#automatic-selection","title":"Automatic Selection","text":"<p>The pipeline automatically selects the appropriate prompt directory:</p> Recognizer Prompt Directory <code>gemini-*</code> <code>settings/prompts/gemini/</code> <code>gpt-*</code> <code>settings/prompts/openai/</code> <code>paddleocr-vl</code> <code>settings/prompts/default/</code>"},{"location":"architecture/recognizers/#custom-prompts","title":"Custom Prompts","text":"<pre><code># Use custom prompt directory\npython main.py --input doc.pdf --prompts-dir custom_prompts/\n</code></pre>"},{"location":"architecture/recognizers/#caching","title":"Caching","text":"<p>The recognition stage uses content-based caching to avoid reprocessing identical content.</p>"},{"location":"architecture/recognizers/#how-it-works","title":"How It Works","text":"<pre><code># Cache key = hash(block_image + block_type + prompt)\ncache_key = hashlib.sha256(\n    block_image.tobytes() +\n    block_type.encode() +\n    prompt.encode()\n).hexdigest()\n</code></pre>"},{"location":"architecture/recognizers/#configuration","title":"Configuration","text":"<pre><code># Enable caching (default)\npython main.py --input doc.pdf --cache\n\n# Disable caching\npython main.py --input doc.pdf --no-cache\n</code></pre> <p>Cache Location: <code>.cache/</code> directory (configurable via <code>--cache-dir</code>)</p>"},{"location":"architecture/recognizers/#choosing-a-recognizer","title":"Choosing a Recognizer","text":""},{"location":"architecture/recognizers/#recommended-configurations","title":"Recommended Configurations","text":"Use Case Recognizer Reason Free processing <code>gemini-2.5-flash</code> Free tier, good quality Highest quality <code>gpt-4o</code> Best understanding No API costs <code>paddleocr-vl</code> Local, 109 languages Large batches <code>paddleocr-vl</code> + vLLM Fast local inference Multi-language <code>paddleocr-vl</code> 109 language support"},{"location":"architecture/recognizers/#decision-matrix","title":"Decision Matrix","text":"<pre><code>graph TD\n    A[Need API?] --&gt;|No| B[paddleocr-vl]\n    A --&gt;|Yes| C[Free tier sufficient?]\n    C --&gt;|Yes| D[gemini-2.5-flash]\n    C --&gt;|No| E[Highest quality needed?]\n    E --&gt;|Yes| F[gpt-4o]\n    E --&gt;|No| D\n</code></pre>"},{"location":"architecture/recognizers/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Tested on single page with 20 blocks:</p> Recognizer Time/Page Cost/Page Quality <code>gemini-2.5-flash</code> ~3s Free 95% <code>gpt-4o</code> ~5s ~$0.02 98% <code>paddleocr-vl</code> (pytorch) ~10s Free 92% <code>paddleocr-vl</code> (vllm) ~3s Free 92%"},{"location":"architecture/recognizers/#see-also","title":"See Also","text":"<ul> <li>Recognition API - API reference</li> <li>Pipeline Stages - Stage 4 details</li> <li>Basic Usage - Usage examples</li> </ul>"},{"location":"architecture/sorters/","title":"Sorters","text":"<p>Reading order analysis algorithms for document layout.</p>"},{"location":"architecture/sorters/#overview","title":"Overview","text":"<p>Sorters analyze the reading order of detected blocks, determining the sequence in which text should be read. This is crucial for multi-column documents and complex layouts.</p>"},{"location":"architecture/sorters/#sorter-comparison","title":"Sorter Comparison","text":"Sorter Algorithm Multi-Column Speed Quality <code>pymupdf</code> Font analysis \u2705 \u26a1\u26a1\u26a1 \u2b50\u2b50\u2b50 <code>mineru-xycut</code> XY-Cut \u274c \u26a1\u26a1\u26a1 \u2b50\u2b50 <code>mineru-layoutreader</code> LayoutLMv3 \u2705 \u26a1\u26a1 \u2b50\u2b50\u2b50\u2b50 <code>mineru-vlm</code> VLM reasoning \u2705 \u26a1 \u2b50\u2b50\u2b50\u2b50 <code>olmocr-vlm</code> VLM reasoning \u2705 \u26a1 \u2b50\u2b50\u2b50\u2b50 <code>paddleocr-doclayout-v2</code> Pointer network \u2705 \u26a1\u26a1 \u2b50\u2b50\u2b50\u2b50"},{"location":"architecture/sorters/#pymupdf-sorter","title":"PyMuPDF Sorter","text":"<p>Algorithm: Font-based column analysis | Speed: Fast</p> <p>Multi-column aware sorter using PyMuPDF's font and position analysis.</p>"},{"location":"architecture/sorters/#how-it-works","title":"How It Works","text":"<ol> <li>Analyzes font sizes and positions in the original PDF</li> <li>Detects column boundaries based on text block positions</li> <li>Groups blocks by column</li> <li>Sorts within each column (top-to-bottom)</li> <li>Assigns <code>column_index</code> to each block</li> </ol>"},{"location":"architecture/sorters/#output-fields","title":"Output Fields","text":"Field Description <code>order</code> Global reading order (0-indexed) <code>column_index</code> Column assignment (0 = left, 1 = right, etc.)"},{"location":"architecture/sorters/#usage","title":"Usage","text":"<pre><code>from pipeline.layout.ordering import create_sorter\n\nsorter = create_sorter(\"pymupdf\")\nsorted_blocks = sorter.sort(blocks, image, pymupdf_page=page)\n</code></pre>"},{"location":"architecture/sorters/#cli","title":"CLI","text":"<pre><code>python main.py --input doc.pdf --sorter pymupdf\n</code></pre>"},{"location":"architecture/sorters/#best-for","title":"Best For","text":"<ul> <li>Standard multi-column documents (newspapers, journals)</li> <li>Documents with consistent column layouts</li> </ul>"},{"location":"architecture/sorters/#xy-cut-sorter-mineru","title":"XY-Cut Sorter (MinerU)","text":"<p>Algorithm: Recursive XY-Cut | Speed: Fast</p> <p>Classic recursive XY-Cut algorithm that divides the page into regions.</p>"},{"location":"architecture/sorters/#how-it-works_1","title":"How It Works","text":"<pre><code>1. Find horizontal whitespace \u2192 split into rows\n2. Find vertical whitespace \u2192 split into columns\n3. Recursively apply until blocks are isolated\n4. Order blocks by position\n</code></pre> <pre><code>graph TD\n    A[Page] --&gt; B[Split Horizontally]\n    B --&gt; C[Top Half]\n    B --&gt; D[Bottom Half]\n    C --&gt; E[Split Vertically]\n    E --&gt; F[Left Block]\n    E --&gt; G[Right Block]\n</code></pre>"},{"location":"architecture/sorters/#output-fields_1","title":"Output Fields","text":"Field Description <code>order</code> Reading order (0-indexed)"},{"location":"architecture/sorters/#usage_1","title":"Usage","text":"<pre><code>sorter = create_sorter(\"mineru-xycut\")\nsorted_blocks = sorter.sort(blocks, image)\n</code></pre>"},{"location":"architecture/sorters/#cli_1","title":"CLI","text":"<pre><code>python main.py --input doc.pdf --sorter mineru-xycut\n</code></pre>"},{"location":"architecture/sorters/#best-for_1","title":"Best For","text":"<ul> <li>Simple layouts with clear horizontal/vertical separations</li> <li>Single-column documents</li> <li>Fast processing requirements</li> </ul>"},{"location":"architecture/sorters/#limitations","title":"Limitations","text":"<ul> <li>May fail on complex multi-column layouts</li> <li>No column detection</li> </ul>"},{"location":"architecture/sorters/#layoutreader-sorter-mineru","title":"LayoutReader Sorter (MinerU)","text":"<p>Algorithm: LayoutLMv3 | Speed: Medium</p> <p>Machine learning-based sorter using LayoutLMv3 for reading order prediction.</p>"},{"location":"architecture/sorters/#how-it-works_2","title":"How It Works","text":"<ol> <li>Encodes block positions and types</li> <li>Uses LayoutLMv3 transformer to predict pairwise ordering</li> <li>Constructs global order from pairwise comparisons</li> </ol>"},{"location":"architecture/sorters/#output-fields_2","title":"Output Fields","text":"Field Description <code>order</code> ML-predicted reading order"},{"location":"architecture/sorters/#usage_2","title":"Usage","text":"<pre><code>sorter = create_sorter(\"mineru-layoutreader\")\nsorted_blocks = sorter.sort(blocks, image)\n</code></pre>"},{"location":"architecture/sorters/#cli_2","title":"CLI","text":"<pre><code>python main.py --input doc.pdf --sorter mineru-layoutreader\n</code></pre>"},{"location":"architecture/sorters/#requirements","title":"Requirements","text":"<ul> <li>MinerU 2.5+ (in <code>external/MinerU/</code>)</li> <li>~2GB GPU memory</li> </ul>"},{"location":"architecture/sorters/#best-for_2","title":"Best For","text":"<ul> <li>Complex layouts</li> <li>Academic papers</li> <li>Multi-column with figures/tables</li> </ul>"},{"location":"architecture/sorters/#vlm-sorters","title":"VLM Sorters","text":""},{"location":"architecture/sorters/#mineru-vlm-sorter","title":"MinerU VLM Sorter","text":"<p>Algorithm: Vision Language Model | Speed: Slow</p> <p>Uses MinerU's VLM for visual reasoning about reading order.</p> <p>Coupling Requirement</p> <p>The <code>mineru-vlm</code> sorter requires the <code>mineru-vlm</code> detector. They share the same VLM inference.</p> <pre><code>sorter = create_sorter(\"mineru-vlm\")\n# Requires mineru-vlm detector\n</code></pre>"},{"location":"architecture/sorters/#olmocr-vlm-sorter","title":"olmOCR VLM Sorter","text":"<p>Algorithm: Vision Language Model | Speed: Slow</p> <p>Uses olmOCR's VLM for reading order analysis. More flexible than MinerU VLM.</p> <pre><code>sorter = create_sorter(\"olmocr-vlm\")\nsorted_blocks = sorter.sort(blocks, image)\n</code></pre>"},{"location":"architecture/sorters/#cli_3","title":"CLI","text":"<pre><code>python main.py --input doc.pdf --sorter olmocr-vlm --sorter-backend vllm\n</code></pre>"},{"location":"architecture/sorters/#best-for_3","title":"Best For","text":"<ul> <li>Very complex layouts</li> <li>Documents with unusual reading patterns</li> <li>When accuracy is more important than speed</li> </ul>"},{"location":"architecture/sorters/#paddleocr-doclayoutv2-sorter","title":"PaddleOCR DocLayoutV2 Sorter","text":"<p>Algorithm: Pointer Network | Speed: Medium</p> <p>Preserves the reading order from PaddleOCR's PP-DocLayoutV2 detector, which includes a built-in pointer network for ordering.</p>"},{"location":"architecture/sorters/#how-it-works_3","title":"How It Works","text":"<ul> <li>PP-DocLayoutV2 detector outputs blocks with order information</li> <li>Sorter preserves this ordering (passthrough)</li> <li>No additional computation required</li> </ul>"},{"location":"architecture/sorters/#usage_3","title":"Usage","text":"<pre><code># Best used with paddleocr-doclayout-v2 detector\nsorter = create_sorter(\"paddleocr-doclayout-v2\")\n</code></pre>"},{"location":"architecture/sorters/#cli_4","title":"CLI","text":"<pre><code>python main.py --input doc.pdf \\\n    --detector paddleocr-doclayout-v2 \\\n    --sorter paddleocr-doclayout-v2\n</code></pre>"},{"location":"architecture/sorters/#best-for_4","title":"Best For","text":"<ul> <li>When using PaddleOCR PP-DocLayoutV2 detector</li> <li>Efficient processing (no separate sorting step)</li> </ul>"},{"location":"architecture/sorters/#detectorsorter-compatibility","title":"Detector/Sorter Compatibility","text":"Detector Compatible Sorters <code>doclayout-yolo</code> All except <code>mineru-vlm</code> <code>mineru-doclayout-yolo</code> All except <code>mineru-vlm</code> <code>paddleocr-doclayout-v2</code> All <code>mineru-vlm</code> <code>mineru-vlm</code> only <pre><code># Validate compatibility\nfrom pipeline.layout.ordering import validate_combination\n\nvalidate_combination(\"doclayout-yolo\", \"mineru-xycut\")  # OK\nvalidate_combination(\"doclayout-yolo\", \"mineru-vlm\")    # Raises error\n</code></pre>"},{"location":"architecture/sorters/#choosing-a-sorter","title":"Choosing a Sorter","text":""},{"location":"architecture/sorters/#recommended-configurations","title":"Recommended Configurations","text":"Use Case Sorter Reason Fast processing <code>mineru-xycut</code> Fastest, simple algorithm Multi-column docs <code>pymupdf</code> Good column detection Complex layouts <code>mineru-layoutreader</code> ML-based, accurate PaddleOCR pipeline <code>paddleocr-doclayout-v2</code> Built-in ordering Maximum accuracy <code>olmocr-vlm</code> VLM reasoning"},{"location":"architecture/sorters/#decision-matrix","title":"Decision Matrix","text":"<pre><code>graph TD\n    A[Multi-column document?] --&gt;|No| B[mineru-xycut]\n    A --&gt;|Yes| C[Using PaddleOCR detector?]\n    C --&gt;|Yes| D[paddleocr-doclayout-v2]\n    C --&gt;|No| E[Need high accuracy?]\n    E --&gt;|No| F[pymupdf]\n    E --&gt;|Yes| G[mineru-layoutreader]\n</code></pre>"},{"location":"architecture/sorters/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Tested on NVIDIA RTX 3090:</p> Sorter Blocks/sec Accuracy Notes <code>mineru-xycut</code> ~1000 85% Fastest <code>pymupdf</code> ~500 90% Good for columns <code>paddleocr-doclayout-v2</code> ~300 92% With PP-DocLayoutV2 <code>mineru-layoutreader</code> ~100 95% ML-based <code>olmocr-vlm</code> ~20 97% VLM-based"},{"location":"architecture/sorters/#see-also","title":"See Also","text":"<ul> <li>Ordering API - API reference</li> <li>Detectors - Layout detection models</li> <li>Architecture Overview - Pipeline design</li> </ul>"},{"location":"getting-started/basic-usage/","title":"Basic Usage","text":"<p>Learn the core features and command-line options of VLM OCR Pipeline.</p>"},{"location":"getting-started/basic-usage/#command-line-interface","title":"Command-Line Interface","text":"<p>The main entry point is <code>main.py</code>, which provides a comprehensive CLI.</p>"},{"location":"getting-started/basic-usage/#basic-syntax","title":"Basic Syntax","text":"<pre><code>python main.py [OPTIONS]\n</code></pre>"},{"location":"getting-started/basic-usage/#input-options","title":"Input Options","text":""},{"location":"getting-started/basic-usage/#single-file","title":"Single File","text":"<pre><code># Process a PDF\npython main.py --input document.pdf --backend gemini\n\n# Process an image\npython main.py --input photo.jpg --backend gemini\n</code></pre>"},{"location":"getting-started/basic-usage/#batch-processing","title":"Batch Processing","text":"<pre><code># Process all PDFs in a directory\npython main.py --input documents/ --backend gemini\n</code></pre>"},{"location":"getting-started/basic-usage/#page-limiting","title":"Page Limiting","text":"<p>Control which pages to process:</p> Max PagesPage RangeSpecific Pages <p>Process first N pages only: <pre><code>python main.py --input doc.pdf --backend gemini --max-pages 5\n</code></pre></p> <p>Process a specific range (inclusive): <pre><code>python main.py --input doc.pdf --backend gemini --page-range 10-20\n</code></pre></p> <p>Process selected pages: <pre><code>python main.py --input doc.pdf --backend gemini --pages 1,5,10,15\n</code></pre></p>"},{"location":"getting-started/basic-usage/#backend-selection","title":"Backend Selection","text":""},{"location":"getting-started/basic-usage/#cloud-vlm-apis","title":"Cloud VLM APIs","text":"GeminiOpenAIOpenRouter <p>Google's Gemini API (free tier available): <pre><code>export GEMINI_API_KEY=\"your_key\"\npython main.py --input doc.pdf --backend gemini --model gemini-2.5-flash\n</code></pre></p> <p>Tier Options: <code>free</code>, <code>tier1</code>, <code>tier2</code>, <code>tier3</code> <pre><code>python main.py --input doc.pdf --backend gemini --gemini-tier free\n</code></pre></p> <p>OpenAI's GPT-4 Vision: <pre><code>export OPENAI_API_KEY=\"your_key\"\npython main.py --input doc.pdf --backend openai --model gpt-4o\n</code></pre></p> <p>Access multiple VLMs through OpenRouter: <pre><code>export OPENROUTER_API_KEY=\"your_key\"\npython main.py --input doc.pdf --backend openai --model google/gemini-2.5-flash\n</code></pre></p>"},{"location":"getting-started/basic-usage/#local-recognition","title":"Local Recognition","text":"<p>PaddleOCR-VL (no API required): <pre><code>python main.py --input doc.pdf --recognizer paddleocr-vl\n</code></pre></p>"},{"location":"getting-started/basic-usage/#detector-selection","title":"Detector Selection","text":"<p>Choose the layout detection model:</p> Detector Source Speed Quality Use Case <code>doclayout-yolo</code> This project \u26a1\u26a1\u26a1 \u2b50\u2b50\u2b50 Default, fast <code>mineru-doclayout-yolo</code> MinerU \u26a1\u26a1 \u2b50\u2b50\u2b50 MinerU pipeline <code>paddleocr-doclayout-v2</code> PaddleOCR \u26a1\u26a1 \u2b50\u2b50\u2b50\u2b50 High quality <code>mineru-vlm</code> MinerU \u26a1 \u2b50\u2b50\u2b50\u2b50 VLM-based <code>olmocr-vlm</code> olmOCR \u26a1 \u2b50\u2b50\u2b50\u2b50 VLM-based <p>Examples:</p> <pre><code># Default detector (doclayout-yolo)\npython main.py --input doc.pdf --backend gemini\n\n# High-quality detector\npython main.py --input doc.pdf --detector paddleocr-doclayout-v2 --backend gemini\n\n# VLM-based detection\npython main.py --input doc.pdf --detector mineru-vlm --backend gemini\n</code></pre>"},{"location":"getting-started/basic-usage/#sorter-selection","title":"Sorter Selection","text":"<p>Choose the reading order algorithm:</p> Sorter Algorithm Speed Multi-Column Use Case <code>pymupdf</code> Font analysis \u26a1\u26a1\u26a1 \u2705 Multi-column docs <code>mineru-xycut</code> XY-Cut \u26a1\u26a1\u26a1 \u274c Simple layouts <code>mineru-layoutreader</code> LayoutLMv3 \u26a1\u26a1 \u2705 Complex layouts <code>mineru-vlm</code> VLM reasoning \u26a1 \u2705 Very complex <code>olmocr-vlm</code> VLM reasoning \u26a1 \u2705 Research papers <code>paddleocr-doclayout-v2</code> Pointer network \u26a1\u26a1 \u2705 With PP-DocLayoutV2 <p>Examples:</p> <pre><code># Multi-column documents\npython main.py --input doc.pdf --sorter pymupdf --backend gemini\n\n# Complex academic papers\npython main.py --input paper.pdf --sorter mineru-layoutreader --backend gemini\n\n# VLM-based ordering\npython main.py --input doc.pdf --sorter olmocr-vlm --backend gemini\n</code></pre>"},{"location":"getting-started/basic-usage/#detector-sorter-combinations","title":"Detector + Sorter Combinations","text":"<p>Not all combinations are valid. The pipeline validates compatibility:</p>"},{"location":"getting-started/basic-usage/#recommended-combinations","title":"Recommended Combinations","text":"<pre><code># Fast general-purpose\npython main.py --input doc.pdf \\\n    --detector doclayout-yolo \\\n    --sorter mineru-xycut \\\n    --backend gemini\n\n# High quality multi-column\npython main.py --input doc.pdf \\\n    --detector paddleocr-doclayout-v2 \\\n    --sorter pymupdf \\\n    --backend gemini\n\n# Maximum quality (slower)\npython main.py --input doc.pdf \\\n    --detector mineru-vlm \\\n    --sorter mineru-vlm \\\n    --backend gemini\n\n# Full PaddleOCR pipeline (local)\npython main.py --input doc.pdf \\\n    --detector paddleocr-doclayout-v2 \\\n    --recognizer paddleocr-vl\n</code></pre> <p>Invalid Combinations</p> <ul> <li><code>paddleocr-doclayout-v2</code> detector auto-selects its sorter (cannot override)</li> <li>VLM detectors (<code>mineru-vlm</code>, <code>olmocr-vlm</code>) require matching VLM sorters</li> </ul>"},{"location":"getting-started/basic-usage/#output-options","title":"Output Options","text":""},{"location":"getting-started/basic-usage/#output-directory","title":"Output Directory","text":"<pre><code># Custom output directory\npython main.py --input doc.pdf --backend gemini --output results/\n</code></pre> <p>Default: <code>output/{model}/{filename}/</code></p> <p>Example: <code>output/gemini-2.5-flash/document/page_1.json</code></p>"},{"location":"getting-started/basic-usage/#cache-control","title":"Cache Control","text":"<pre><code># Disable caching\npython main.py --input doc.pdf --backend gemini --no-cache\n\n# Custom cache directory\npython main.py --input doc.pdf --backend gemini --cache-dir .my-cache/\n</code></pre>"},{"location":"getting-started/basic-usage/#rate-limiting-gemini","title":"Rate Limiting (Gemini)","text":""},{"location":"getting-started/basic-usage/#check-status","title":"Check Status","text":"<pre><code>python main.py --rate-limit-status --backend gemini --gemini-tier free\n</code></pre> <p>Output: <pre><code>=== Gemini API Rate Limit Status ===\nTier: free\nModel: gemini-2.5-flash\n\nCurrent Limits:\n  RPM (Requests Per Minute): 2 / 15 (13.3%)\n  TPM (Tokens Per Minute): 45,234 / 1,500,000 (3.0%)\n  RPD (Requests Per Day): 156 / 1,500 (10.4%)\n</code></pre></p>"},{"location":"getting-started/basic-usage/#tier-configuration","title":"Tier Configuration","text":"<pre><code># Free tier (default)\npython main.py --input doc.pdf --backend gemini --gemini-tier free\n\n# Paid tiers (higher limits)\npython main.py --input doc.pdf --backend gemini --gemini-tier tier1\npython main.py --input doc.pdf --backend gemini --gemini-tier tier2\n</code></pre>"},{"location":"getting-started/basic-usage/#advanced-options","title":"Advanced Options","text":""},{"location":"getting-started/basic-usage/#dpi-settings","title":"DPI Settings","text":"<p>For PDF rendering quality:</p> <pre><code># Higher DPI = better quality, larger images\npython main.py --input doc.pdf --backend gemini --dpi 300  # Default: 200\n</code></pre>"},{"location":"getting-started/basic-usage/#temporary-files","title":"Temporary Files","text":"<pre><code># Custom temp directory\npython main.py --input doc.pdf --backend gemini --temp-dir /tmp/ocr/\n</code></pre>"},{"location":"getting-started/basic-usage/#logging","title":"Logging","text":"<pre><code># Verbose output\npython main.py --input doc.pdf --backend gemini -v\n\n# Very verbose (debug level)\npython main.py --input doc.pdf --backend gemini -vv\n</code></pre>"},{"location":"getting-started/basic-usage/#common-workflows","title":"Common Workflows","text":""},{"location":"getting-started/basic-usage/#academic-papers","title":"Academic Papers","text":"<pre><code># High-quality processing for research papers\npython main.py --input paper.pdf \\\n    --detector paddleocr-doclayout-v2 \\\n    --sorter mineru-layoutreader \\\n    --backend gemini \\\n    --dpi 300\n</code></pre>"},{"location":"getting-started/basic-usage/#multi-column-magazines","title":"Multi-Column Magazines","text":"<pre><code># Multi-column layout detection\npython main.py --input magazine.pdf \\\n    --detector doclayout-yolo \\\n    --sorter pymupdf \\\n    --backend gemini\n</code></pre>"},{"location":"getting-started/basic-usage/#large-batch-processing","title":"Large Batch Processing","text":"<pre><code># Process many PDFs with local model\npython main.py --input documents/ \\\n    --detector paddleocr-doclayout-v2 \\\n    --recognizer paddleocr-vl \\\n    --max-pages 10  # Limit for testing\n</code></pre>"},{"location":"getting-started/basic-usage/#cost-optimized-processing","title":"Cost-Optimized Processing","text":"<pre><code># Use Gemini free tier + caching\npython main.py --input doc.pdf \\\n    --backend gemini \\\n    --gemini-tier free \\\n    --cache-dir .cache/\n</code></pre>"},{"location":"getting-started/basic-usage/#output-structure","title":"Output Structure","text":"<p>After processing, you'll find:</p> <pre><code>output/\n\u2514\u2500\u2500 {model}/              # e.g., gemini-2.5-flash/\n    \u2514\u2500\u2500 {document}/       # e.g., research_paper/\n        \u251c\u2500\u2500 page_1.json   # Detailed page data\n        \u251c\u2500\u2500 page_1.md     # Markdown output\n        \u251c\u2500\u2500 page_2.json\n        \u251c\u2500\u2500 page_2.md\n        \u2514\u2500\u2500 {document}_summary.json  # Processing metadata\n</code></pre>"},{"location":"getting-started/basic-usage/#json-structure","title":"JSON Structure","text":"<pre><code>{\n  \"page_num\": 1,\n  \"image_size\": [1650, 2200],\n  \"text\": \"# Title\\n\\nBody text...\",\n  \"corrected_text\": \"# Title\\n\\nBody text...\",\n  \"correction_ratio\": 0.02,\n  \"processing_stopped\": false,\n  \"blocks\": [\n    {\n      \"type\": \"title\",\n      \"bbox\": [100, 50, 500, 120],\n      \"detection_confidence\": 0.95,\n      \"order\": 0,\n      \"column_index\": null,\n      \"text\": \"Title\",\n      \"corrected_text\": \"Title\",\n      \"source\": \"doclayout-yolo\"\n    }\n  ],\n  \"auxiliary_info\": {\n    \"text_spans\": [...]  # Font metadata for markdown\n  }\n}\n</code></pre>"},{"location":"getting-started/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture Overview - Understand the pipeline</li> <li>Advanced Examples - Complex use cases</li> <li>API Reference - Programmatic usage</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you set up VLM OCR Pipeline on your system.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+: Recommended for best compatibility</li> <li>uv: Fast Python package manager (optional but recommended)</li> <li>Git: For cloning the repository and managing submodules</li> </ul>"},{"location":"getting-started/installation/#quick-installation","title":"Quick Installation","text":""},{"location":"getting-started/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/NoUnique/vlm-ocr-pipeline.git\ncd vlm-ocr-pipeline\n</code></pre>"},{"location":"getting-started/installation/#2-set-up-python-environment","title":"2. Set Up Python Environment","text":"Using uv (Recommended)Using pip <pre><code># Install uv if you haven't\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create virtual environment\nuv venv --python 3.11 .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies\nuv pip install -r requirements.txt\n</code></pre> <pre><code># Create virtual environment\npython3.11 -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#3-fix-doclayout-yolo-compatibility","title":"3. Fix DocLayout-YOLO Compatibility","text":"<pre><code>python setup.py\n</code></pre> <p>What does setup.py do?</p> <p>The <code>setup.py</code> script fixes compatibility issues between DocLayout-YOLO and the ultralytics package by modifying the YOLO model files.</p>"},{"location":"getting-started/installation/#api-configuration","title":"API Configuration","text":"<p>Choose the backend you want to use and configure the corresponding API keys.</p>"},{"location":"getting-started/installation/#gemini-api-recommended-for-free-tier","title":"Gemini API (Recommended for Free Tier)","text":"<ol> <li>Visit Google AI Studio</li> <li>Sign in with your Google account</li> <li>Click \"Create API Key\"</li> <li>Copy the generated key</li> <li>Set environment variable:</li> </ol> <pre><code>export GEMINI_API_KEY=\"your_api_key_here\"\n</code></pre> <p>Free Tier Limits (as of 2024): - 15 requests per minute (RPM) - 1,500,000 tokens per minute (TPM) - 1,500 requests per day (RPD)</p>"},{"location":"getting-started/installation/#openai-api","title":"OpenAI API","text":"<ol> <li>Visit OpenAI Platform</li> <li>Create an API key</li> <li>Set environment variable:</li> </ol> <pre><code>export OPENAI_API_KEY=\"your_api_key_here\"\n</code></pre>"},{"location":"getting-started/installation/#openrouter-api-alternative","title":"OpenRouter API (Alternative)","text":"<p>OpenRouter provides access to multiple VLMs through a single API:</p> <pre><code>export OPENROUTER_API_KEY=\"your_api_key_here\"\n</code></pre>"},{"location":"getting-started/installation/#optional-components","title":"Optional Components","text":""},{"location":"getting-started/installation/#paddleocr-vl-local-recognition","title":"PaddleOCR-VL (Local Recognition)","text":"<p>For local text recognition without API calls:</p> <pre><code># PaddleX is already included as a submodule\ncd external/PaddleX\ngit checkout v3.3.1\npip install -e .\n</code></pre> <p>Requirements: - GPU with CUDA support (recommended) - ~4GB VRAM for PaddleOCR-VL-0.9B</p>"},{"location":"getting-started/installation/#external-frameworks-git-submodules","title":"External Frameworks (Git Submodules)","text":"<p>The project includes several external frameworks as submodules:</p> <pre><code># Initialize all submodules\ngit submodule update --init --recursive\n\n# Or initialize specific submodules\ngit submodule update --init external/MinerU      # MinerU detectors/sorters\ngit submodule update --init external/olmocr      # olmOCR VLM sorter\ngit submodule update --init external/PaddleOCR   # PP-DocLayoutV2 detector\ngit submodule update --init external/PaddleX     # PaddleOCR-VL recognizer\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Verify your installation:</p> <pre><code># Check Python version\npython --version  # Should be 3.11+\n\n# Check dependencies\npython -c \"import torch; print(f'PyTorch: {torch.__version__}')\"\npython -c \"import cv2; print(f'OpenCV: {cv2.__version__}')\"\n\n# Run a simple test\npython main.py --help\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#cudagpu-issues","title":"CUDA/GPU Issues","text":"<p>If you encounter CUDA-related errors:</p> <pre><code># Check CUDA availability\npython -c \"import torch; print(torch.cuda.is_available())\"\n\n# Install CPU-only PyTorch (if no GPU)\npip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n</code></pre>"},{"location":"getting-started/installation/#ultralyticsyolo-errors","title":"Ultralytics/YOLO Errors","text":"<p>If you see errors about <code>ultralytics</code> or YOLO models:</p> <pre><code># Re-run the setup script\npython setup.py\n\n# Or manually reinstall ultralytics\npip uninstall ultralytics\npip install ultralytics==8.2.0\n</code></pre>"},{"location":"getting-started/installation/#missing-dependencies","title":"Missing Dependencies","text":"<p>If you encounter import errors:</p> <pre><code># Reinstall all dependencies\npip install -r requirements.txt --force-reinstall\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that you have VLM OCR Pipeline installed:</p> <ul> <li>Quick Start Guide - Run your first OCR pipeline</li> <li>Basic Usage - Learn the core features</li> <li>Architecture Overview - Understand how it works</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with VLM OCR Pipeline in 5 minutes!</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+ installed</li> <li>Gemini API key (free tier available)</li> </ul>"},{"location":"getting-started/quickstart/#step-1-install","title":"Step 1: Install","text":"<pre><code># Clone the repository\ngit clone https://github.com/NoUnique/vlm-ocr-pipeline.git\ncd vlm-ocr-pipeline\n\n# Set up environment\nuv venv --python 3.11 .venv\nsource .venv/bin/activate\n\n# Install dependencies\nuv pip install -r requirements.txt\n\n# Fix YOLO compatibility\npython setup.py\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-configure-api","title":"Step 2: Configure API","text":"<pre><code># Set Gemini API key\nexport GEMINI_API_KEY=\"your_api_key_here\"\n</code></pre> <p>Get a Free Gemini API Key</p> <p>Visit Google AI Studio to get a free API key.</p>"},{"location":"getting-started/quickstart/#step-3-run-your-first-pipeline","title":"Step 3: Run Your First Pipeline","text":"<pre><code># Process a single PDF\npython main.py --input document.pdf --backend gemini\n</code></pre> <p>That's it! The pipeline will:</p> <ol> <li>\ud83d\udcc4 Load your PDF and render each page as an image</li> <li>\ud83d\udd0d Detect layout blocks (text, tables, figures, etc.)</li> <li>\ud83d\udcca Analyze reading order</li> <li>\ud83d\udcdd Extract text using Gemini Vision</li> <li>\ud83d\udd27 Correct and improve text quality</li> <li>\ud83d\udcbe Save results to <code>output/gemini-2.5-flash/document/</code></li> </ol>"},{"location":"getting-started/quickstart/#understanding-the-output","title":"Understanding the Output","text":"<p>After processing, you'll find:</p> <pre><code>output/\n\u2514\u2500\u2500 gemini-2.5-flash/\n    \u2514\u2500\u2500 document/\n        \u251c\u2500\u2500 page_1.json          # Detailed page data\n        \u251c\u2500\u2500 page_1.md            # Markdown output\n        \u2514\u2500\u2500 document_summary.json  # Processing metadata\n</code></pre>"},{"location":"getting-started/quickstart/#example-output","title":"Example Output","text":"<p>page_1.md: <pre><code># Introduction\n\nThis document describes...\n\n## Table of Contents\n\n1. Getting Started\n2. Advanced Features\n3. API Reference\n</code></pre></p> <p>page_1.json: <pre><code>{\n  \"page_num\": 1,\n  \"text\": \"# Introduction\\n\\nThis document describes...\",\n  \"corrected_text\": \"# Introduction\\n\\nThis document describes...\",\n  \"correction_ratio\": 0.05,\n  \"blocks\": [\n    {\n      \"type\": \"title\",\n      \"bbox\": [100, 50, 500, 120],\n      \"text\": \"Introduction\",\n      \"order\": 0\n    }\n  ]\n}\n</code></pre></p>"},{"location":"getting-started/quickstart/#common-use-cases","title":"Common Use Cases","text":""},{"location":"getting-started/quickstart/#single-image","title":"Single Image","text":"<pre><code>python main.py --input photo.jpg --backend gemini\n</code></pre>"},{"location":"getting-started/quickstart/#batch-processing","title":"Batch Processing","text":"<pre><code># Process all PDFs in a directory\npython main.py --input documents/ --backend gemini\n</code></pre>"},{"location":"getting-started/quickstart/#limit-pages-for-testing","title":"Limit Pages (for testing)","text":"<pre><code># Process only first 5 pages\npython main.py --input document.pdf --backend gemini --max-pages 5\n\n# Process specific page range\npython main.py --input document.pdf --backend gemini --page-range 10-20\n\n# Process specific pages\npython main.py --input document.pdf --backend gemini --pages 1,5,10\n</code></pre>"},{"location":"getting-started/quickstart/#use-openai-instead","title":"Use OpenAI Instead","text":"<pre><code># Set OpenAI API key\nexport OPENAI_API_KEY=\"your_api_key_here\"\n\n# Run with OpenAI backend\npython main.py --input document.pdf --backend openai --model gpt-4o\n</code></pre>"},{"location":"getting-started/quickstart/#local-processing-no-api","title":"Local Processing (No API)","text":"<pre><code># Use PaddleOCR-VL (local model, no API calls)\npython main.py --input document.pdf \\\n    --detector paddleocr-doclayout-v2 \\\n    --recognizer paddleocr-vl\n</code></pre>"},{"location":"getting-started/quickstart/#rate-limiting","title":"Rate Limiting","text":"<p>The pipeline automatically handles rate limits for Gemini API:</p> <pre><code># Check current rate limit status\npython main.py --rate-limit-status --backend gemini --gemini-tier free\n</code></pre> <p>Free Tier Limits: - 15 requests per minute - 1,500,000 tokens per minute - 1,500 requests per day</p> <p>The pipeline will automatically wait when limits are reached.</p>"},{"location":"getting-started/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quickstart/#gemini_api_key-not-set","title":"\"GEMINI_API_KEY not set\"","text":"<pre><code>export GEMINI_API_KEY=\"your_api_key_here\"\n</code></pre>"},{"location":"getting-started/quickstart/#rate-limit-exceeded","title":"\"Rate limit exceeded\"","text":"<p>The pipeline will automatically wait. Alternatively:</p> <pre><code># Use OpenAI instead\npython main.py --input doc.pdf --backend openai\n\n# Or use local model (no API)\npython main.py --input doc.pdf --recognizer paddleocr-vl\n</code></pre>"},{"location":"getting-started/quickstart/#cuda-out-of-memory","title":"\"CUDA out of memory\"","text":"<p>If using PaddleOCR-VL locally:</p> <pre><code># Reduce batch size or use CPU\nexport CUDA_VISIBLE_DEVICES=\"\"  # Force CPU mode\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you've run your first pipeline:</p> <ul> <li>Basic Usage Guide - Learn about all available options</li> <li>Architecture Overview - Understand how the pipeline works</li> <li>Advanced Examples - Complex use cases and customizations</li> </ul>"},{"location":"getting-started/quickstart/#tips-for-best-results","title":"Tips for Best Results","text":"<p>Optimize Processing</p> <ul> <li>Use <code>--max-pages</code> to test on a few pages first</li> <li>Check rate limit status regularly for Gemini</li> <li>Use caching to avoid reprocessing identical content</li> </ul> <p>API Costs</p> <ul> <li>Gemini free tier has daily limits</li> <li>OpenAI charges per token</li> <li>Consider using PaddleOCR-VL for large batch processing</li> </ul> <p>Performance</p> <ul> <li>DocLayout-YOLO is fastest for detection</li> <li>PaddleOCR-VL provides good quality without API costs</li> <li>Gemini 2.5 Flash is fast and cost-effective</li> </ul>"},{"location":"guides/advanced-examples/","title":"Advanced Examples","text":"<p>Complex use cases and advanced configurations for VLM OCR Pipeline.</p>"},{"location":"guides/advanced-examples/#multi-column-academic-papers","title":"Multi-Column Academic Papers","text":"<p>Process research papers with complex multi-column layouts:</p> <pre><code>python main.py --input paper.pdf \\\n    --detector paddleocr-doclayout-v2 \\\n    --sorter pymupdf \\\n    --backend gemini \\\n    --dpi 300 \\\n    --output results/papers/\n</code></pre> <p>Why this configuration?</p> <ul> <li>PP-DocLayoutV2: High-quality detector for academic layouts</li> <li>PyMuPDF sorter: Best for multi-column detection</li> <li>DPI 300: Higher quality for small text</li> <li>Gemini: Cost-effective with good accuracy</li> </ul>"},{"location":"guides/advanced-examples/#large-batch-processing","title":"Large Batch Processing","text":"<p>Process hundreds of PDFs efficiently:</p> <pre><code># Use local model to avoid API costs\npython main.py --input documents/ \\\n    --detector paddleocr-doclayout-v2 \\\n    --recognizer paddleocr-vl \\\n    --cache-dir .cache/ \\\n    --output batch-results/\n</code></pre> <p>Best practices:</p> <ul> <li>Use PaddleOCR-VL to avoid API costs</li> <li>Enable caching to skip duplicates</li> <li>Process in batches if memory constrained</li> <li>Monitor disk space for large outputs</li> </ul>"},{"location":"guides/advanced-examples/#mixed-content-documents","title":"Mixed Content Documents","text":"<p>Documents with tables, figures, and equations:</p> <pre><code>python main.py --input textbook.pdf \\\n    --detector mineru-vlm \\\n    --sorter mineru-vlm \\\n    --backend openai \\\n    --model gpt-4o \\\n    --output textbooks/\n</code></pre> <p>Why VLM detection?</p> <ul> <li>Better understanding of complex layouts</li> <li>Accurate table structure detection</li> <li>Equation recognition</li> <li>Figure caption association</li> </ul>"},{"location":"guides/advanced-examples/#cost-optimized-processing","title":"Cost-Optimized Processing","text":"<p>Minimize API costs while maintaining quality:</p> <pre><code># Use Gemini free tier with caching\npython main.py --input doc.pdf \\\n    --detector doclayout-yolo \\\n    --sorter mineru-xycut \\\n    --backend gemini \\\n    --gemini-tier free \\\n    --cache-dir .cache/ \\\n    --max-pages 10  # Test first\n\n# Check rate limits\npython main.py --rate-limit-status --backend gemini --gemini-tier free\n</code></pre> <p>Tips:</p> <ol> <li>Test on few pages first (<code>--max-pages</code>)</li> <li>Use caching to avoid reprocessing</li> <li>Monitor free tier limits</li> <li>Switch to local model if limits reached</li> </ol>"},{"location":"guides/advanced-examples/#programmatic-usage","title":"Programmatic Usage","text":"<p>Use the pipeline in your Python code:</p> <pre><code>from pathlib import Path\nfrom pipeline import Pipeline\n\n# Initialize pipeline\npipeline = Pipeline(\n    detector_name=\"doclayout-yolo\",\n    sorter_name=\"mineru-xycut\",\n    backend=\"gemini\",\n    model=\"gemini-2.5-flash\",\n    cache_dir=Path(\".cache\"),\n    output_dir=Path(\"output\"),\n    use_cache=True\n)\n\n# Process single PDF\nresult = pipeline.process_single_pdf(\n    pdf_path=Path(\"document.pdf\"),\n    max_pages=5\n)\n\n# Access results\nfor page in result.pages:\n    print(f\"Page {page.page_num}\")\n    print(f\"Text: {page.corrected_text}\")\n    print(f\"Blocks: {len(page.blocks)}\")\n    print(f\"Correction ratio: {page.correction_ratio}\")\n</code></pre>"},{"location":"guides/advanced-examples/#custom-detector-integration","title":"Custom Detector Integration","text":"<p>Integrate your own detector:</p> <pre><code># my_detector.py\nimport numpy as np\nfrom pipeline.types import Block, BBox\n\nclass CustomDetector:\n    \"\"\"Custom layout detector.\"\"\"\n\n    def detect(self, image: np.ndarray) -&gt; list[Block]:\n        \"\"\"Detect layout blocks.\"\"\"\n        # Your detection logic here\n        blocks = []\n\n        # Example: detect blocks\n        for detection in your_model.predict(image):\n            block = Block(\n                type=detection.label,  # \"text\", \"table\", etc.\n                bbox=BBox(\n                    x0=int(detection.box[0]),\n                    y0=int(detection.box[1]),\n                    x1=int(detection.box[2]),\n                    y1=int(detection.box[3])\n                ),\n                detection_confidence=float(detection.confidence),\n                source=\"custom-detector\"\n            )\n            blocks.append(block)\n\n        return blocks\n\n# Register in factory\n# pipeline/layout/detection/__init__.py\ndef create_detector(name: str, **kwargs) -&gt; Detector:\n    if name == \"custom\":\n        from .custom_detector import CustomDetector\n        return CustomDetector(**kwargs)\n    # ...\n\n# Use it\npython main.py --input doc.pdf --detector custom --backend gemini\n</code></pre>"},{"location":"guides/advanced-examples/#custom-prompts","title":"Custom Prompts","text":"<p>Override default prompts for specific use cases:</p> <pre><code># settings/prompts/gemini/custom_text_extraction.yaml\nsystem: |\n  You are a specialized OCR system for medical documents.\n  Pay special attention to:\n  - Drug names and dosages\n  - Medical terminology\n  - Patient information\n  - Dates and times\n\nuser: |\n  Extract text from this medical document image.\n  Preserve all formatting, especially:\n  - Tables with patient data\n  - Lists of medications\n  - Diagnostic results\n\n  Output in Markdown format.\n\nfallback: |\n  [OCR failed - manual review required]\n</code></pre> <p>Then use <code>PromptManager</code> in code:</p> <pre><code>from pipeline.prompt import PromptManager\n\npm = PromptManager(model=\"gemini-2.5-flash\")\ncustom_prompt = pm.get_prompt(\"custom_text_extraction\", \"user\")\n</code></pre>"},{"location":"guides/advanced-examples/#multi-language-documents","title":"Multi-Language Documents","text":"<p>Process documents in multiple languages:</p> <pre><code># PaddleOCR-VL supports 109 languages\npython main.py --input multilang.pdf \\\n    --detector paddleocr-doclayout-v2 \\\n    --recognizer paddleocr-vl \\\n    --output multilang-results/\n</code></pre> <p>Supported languages (PaddleOCR-VL):</p> <ul> <li>European: English, Spanish, French, German, Italian, Portuguese, etc.</li> <li>Asian: Chinese (Simplified/Traditional), Japanese, Korean, Thai, Vietnamese</li> <li>Middle Eastern: Arabic, Hebrew, Persian</li> <li>And 100+ more</li> </ul>"},{"location":"guides/advanced-examples/#rate-limit-monitoring","title":"Rate Limit Monitoring","text":"<p>Monitor and adapt to rate limits in real-time:</p> <pre><code>from pipeline.recognition.api.ratelimit import rate_limiter\n\n# Check status before processing\nstatus = rate_limiter.get_status()\nprint(f\"Current RPM: {status['current']['rpm']} / {status['limits']['rpm']}\")\nprint(f\"RPD: {status['current']['rpd']} / {status['limits']['rpd']}\")\n\n# Process with automatic throttling\nif rate_limiter.wait_if_needed(estimated_tokens=1000):\n    # Process page\n    result = pipeline.process_page(image, page_num=1)\nelse:\n    print(\"Daily limit exceeded\")\n</code></pre>"},{"location":"guides/advanced-examples/#selective-page-processing","title":"Selective Page Processing","text":"<p>Process specific pages of interest:</p> <pre><code># Process only pages with tables\npython main.py --input report.pdf \\\n    --pages 5,12,18,25 \\\n    --detector paddleocr-doclayout-v2 \\\n    --backend gemini\n\n# Process chapter intros (every 10 pages)\npython main.py --input book.pdf \\\n    --pages 1,11,21,31,41,51 \\\n    --backend gemini\n</code></pre>"},{"location":"guides/advanced-examples/#error-recovery","title":"Error Recovery","text":"<p>Handle processing errors gracefully:</p> <pre><code>from pipeline import Pipeline\nfrom pipeline.exceptions import ProcessingError, APIError\n\npipeline = Pipeline(backend=\"gemini\")\n\ntry:\n    result = pipeline.process_single_pdf(\"document.pdf\")\nexcept APIError as e:\n    print(f\"API error: {e}\")\n    # Retry with different backend\n    pipeline.backend = \"openai\"\n    result = pipeline.process_single_pdf(\"document.pdf\")\nexcept ProcessingError as e:\n    print(f\"Processing error: {e}\")\n    # Continue with next document\n    pass\n</code></pre>"},{"location":"guides/advanced-examples/#performance-benchmarking","title":"Performance Benchmarking","text":"<p>Compare different configurations:</p> <pre><code>import time\nfrom pathlib import Path\n\nconfigurations = [\n    (\"doclayout-yolo\", \"mineru-xycut\", \"gemini\"),\n    (\"paddleocr-doclayout-v2\", \"pymupdf\", \"gemini\"),\n    (\"mineru-vlm\", \"mineru-vlm\", \"openai\"),\n]\n\ntest_pdf = Path(\"test.pdf\")\nresults = {}\n\nfor detector, sorter, backend in configurations:\n    pipeline = Pipeline(\n        detector_name=detector,\n        sorter_name=sorter,\n        backend=backend\n    )\n\n    start = time.time()\n    result = pipeline.process_single_pdf(test_pdf, max_pages=3)\n    elapsed = time.time() - start\n\n    results[f\"{detector}/{sorter}/{backend}\"] = {\n        \"time\": elapsed,\n        \"pages\": len(result.pages),\n        \"avg_correction_ratio\": sum(p.correction_ratio for p in result.pages) / len(result.pages)\n    }\n\n# Print comparison\nfor config, metrics in results.items():\n    print(f\"{config}:\")\n    print(f\"  Time: {metrics['time']:.2f}s\")\n    print(f\"  Avg correction: {metrics['avg_correction_ratio']:.2%}\")\n</code></pre>"},{"location":"guides/advanced-examples/#integration-with-external-tools","title":"Integration with External Tools","text":""},{"location":"guides/advanced-examples/#export-to-different-formats","title":"Export to Different Formats","text":"<pre><code>from pipeline import Pipeline\nimport json\n\npipeline = Pipeline(backend=\"gemini\")\nresult = pipeline.process_single_pdf(\"document.pdf\")\n\n# Export to JSON\nwith open(\"output.json\", \"w\") as f:\n    json.dump([page.to_dict() for page in result.pages], f, indent=2)\n\n# Export to plain text\nwith open(\"output.txt\", \"w\") as f:\n    for page in result.pages:\n        f.write(f\"=== Page {page.page_num} ===\\n\")\n        f.write(page.corrected_text)\n        f.write(\"\\n\\n\")\n</code></pre>"},{"location":"guides/advanced-examples/#post-processing","title":"Post-Processing","text":"<pre><code>def post_process_markdown(text: str) -&gt; str:\n    \"\"\"Custom post-processing for markdown output.\"\"\"\n    # Remove excessive newlines\n    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n\n    # Fix common OCR errors\n    text = text.replace(\" ,\", \",\")\n    text = text.replace(\" .\", \".\")\n\n    # Normalize quotes\n    text = text.replace(\"\"\", '\"').replace(\"\"\", '\"')\n\n    return text\n\n# Apply to results\nfor page in result.pages:\n    page.corrected_text = post_process_markdown(page.corrected_text)\n</code></pre>"},{"location":"guides/advanced-examples/#best-practices-summary","title":"Best Practices Summary","text":"<p>Performance</p> <ul> <li>Use DocLayout-YOLO for speed</li> <li>Use XY-Cut sorter for simple layouts</li> <li>Enable caching for repeated processing</li> <li>Test on few pages before full batch</li> </ul> <p>Quality</p> <ul> <li>Use PP-DocLayoutV2 for complex layouts</li> <li>Use PyMuPDF for multi-column documents</li> <li>Use VLM detectors for mixed content</li> <li>Increase DPI for small text</li> </ul> <p>Cost Optimization</p> <ul> <li>Use Gemini free tier when possible</li> <li>Cache recognition results</li> <li>Use PaddleOCR-VL for large batches</li> <li>Monitor rate limits</li> </ul> <p>Common Issues</p> <ul> <li>Check page height for PyPDF conversions</li> <li>Validate detector/sorter combinations</li> <li>Handle rate limits gracefully</li> <li>Clean up temporary files</li> </ul>"},{"location":"guides/advanced-examples/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Detailed API documentation</li> <li>Architecture Overview - System design</li> <li>Contributing Guide - Contribute your own examples</li> </ul>"},{"location":"guides/bbox-formats/","title":"BBox Format Reference","text":""},{"location":"guides/bbox-formats/#coordinate-systems","title":"Coordinate Systems","text":"<p>All frameworks use Top-Left origin (0,0) except PyPDF.</p>"},{"location":"guides/bbox-formats/#format-comparison","title":"Format Comparison","text":"Framework Format Example Notes This Project (Internal) <code>BBox(x0, y0, x1, y1)</code> <code>BBox(100, 50, 300, 200)</code> Integer coordinates, xyxy corners This Project (JSON) <code>[x, y, w, h]</code> <code>[100, 50, 200, 150]</code> Position + Size (human-readable) YOLO <code>[x1, y1, x2, y2]</code> <code>[100, 50, 300, 200]</code> Top-Left + Bottom-Right MinerU <code>[x0, y0, x1, y1]</code> <code>[100, 50, 300, 200]</code> Top-Left + Bottom-Right PyMuPDF <code>Rect(x0, y0, x1, y1)</code> <code>Rect(100, 50, 300, 200)</code> Top-Left + Bottom-Right PyPDF \u26a0\ufe0f <code>[x0, y0, x1, y1]</code> <code>[100, 592, 300, 742]</code> Bottom-Left origin olmOCR <code>\"[x, y]text\"</code> <code>\"[100x50]Chapter 1\"</code> Text format"},{"location":"guides/bbox-formats/#visual-example","title":"Visual Example","text":"<p>Same rectangle at visual position (100, 50) with size 200\u00d7150 on page height 792:</p> <pre><code>Computer Vision (Top-Left origin):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 (0,0)           \u2502\n\u2502                 \u2502\n\u2502   [100,50]      \u2502 \u2190 Region here\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502   \u2502        \u2502    \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502          [300,200]\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nPDF (Bottom-Left origin):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          [300,742] \u2190 Y is flipped!\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502   \u2502        \u2502    \u2502\n\u2502   [100,592]     \u2502 \u2190 Same region\n\u2502                 \u2502\n\u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  (0,0)\n</code></pre>"},{"location":"guides/bbox-formats/#format-conversions","title":"Format Conversions","text":""},{"location":"guides/bbox-formats/#this-project-mineruyolo","title":"This Project \u2194 MinerU/YOLO","text":"<pre><code># [x, y, w, h] \u2192 [x0, y0, x1, y1]\nx0, y0, x1, y1 = x, y, x + w, y + h\n\n# [x0, y0, x1, y1] \u2192 [x, y, w, h]\nx, y, w, h = x0, y0, x1 - x0, y1 - y0\n</code></pre>"},{"location":"guides/bbox-formats/#pypdf-this-project-y-axis-flip-required","title":"PyPDF \u2194 This Project (Y-axis flip required!)","text":"<pre><code># PyPDF \u2192 This Project\ny_top = page_height - y1_pypdf       # 792 - 742 = 50\ny_bottom = page_height - y0_pypdf    # 792 - 592 = 200\n\n# This Project \u2192 PyPDF\ny0_pypdf = page_height - y_bottom    # 792 - 200 = 592\ny1_pypdf = page_height - y_top       # 792 - 50 = 742\n</code></pre>"},{"location":"guides/bbox-formats/#olmocr-anchor-format","title":"olmOCR Anchor Format","text":"<pre><code># Text region\nanchor = f\"[{x:.0f}x{y:.0f}]{text_content}\"\n# Example: \"[100x50]Chapter 1\"\n\n# Image/Figure region\nanchor = f\"[Image {x0:.0f}x{y0:.0f} to {x1:.0f}x{y1:.0f}]\"\n# Example: \"[Image 100x50 to 300x200]\"\n\n# Table region\nanchor = f\"[Table {x0:.0f}x{y0:.0f} to {x1:.0f}x{y1:.0f}]\"\n# Example: \"[Table 100x450 to 500x600]\"\n</code></pre>"},{"location":"guides/bbox-formats/#bbox-class-usage","title":"BBox Class Usage","text":"<p>Our <code>BBox</code> class handles all conversions automatically with integer coordinates:</p> <pre><code>from pipeline.types import BBox\n\n# Create from any format (accepts float, converts to int)\nbbox = BBox.from_xywh(100, 50, 200, 150)      # xywh format\nbbox = BBox.from_xyxy(100, 50, 300, 200)      # xyxy format (corners)\nbbox = BBox.from_cxcywh(200, 125, 200, 150)   # Center format (YOLO training)\nbbox = BBox.from_mineru_bbox([100, 50, 300, 200])\nbbox = BBox.from_pymupdf_rect(rect)\nbbox = BBox.from_pypdf_rect([100, 592, 300, 742], page_height=792)\n\n# Convert to any format\ncoords = bbox.to_xywh_list()          # [100, 50, 200, 150] (for JSON)\ncoords = bbox.to_list()                # [100, 50, 300, 200] (xyxy)\ncoords = bbox.to_dict()                # {\"x0\": 100, \"y0\": 50, \"x1\": 300, \"y1\": 200}\ncx, cy, w, h = bbox.to_cxcywh()        # Center format\ncoords = bbox.to_mineru_bbox()         # [100, 50, 300, 200]\ncoords = bbox.to_pypdf_rect(792)       # [100, 592, 300, 742]\nanchor = bbox.to_olmocr_anchor(\"image\") # \"[Image 100x50 to 300x200]\"\n\n# Geometric operations (integer results)\ncenter_x, center_y = bbox.center  # (float, float)\narea = bbox.area                  # int\nwidth = bbox.width                # int\nheight = bbox.height              # int\noverlap = bbox1.intersect(bbox2)  # int\niou = bbox1.iou(bbox2)            # float\n\n# Clear aliases\nleft = bbox.left     # = bbox.x0\ntop = bbox.top       # = bbox.y0\nright = bbox.right   # = bbox.x1\nbottom = bbox.bottom # = bbox.y1\n\n# NumPy convenience\ncropped = bbox.crop(image, padding=5)  # Direct image cropping\n</code></pre>"},{"location":"guides/bbox-formats/#region-structure","title":"Region Structure","text":"<p>The <code>Region</code> dataclass represents detected document regions:</p> <pre><code>from pipeline.types import Block, BBox\n\nblock = Block(\n    type=\"text\",\n    bbox=BBox(100, 50, 300, 200),  # Required, always present\n    detection_confidence=0.95,\n    # Optional fields\n    order=0,\n    column_index=1,\n    text=\"Extracted text...\",\n)\n\n# Serialize to JSON (bbox \u2192 xywh format for readability)\ndata = block.to_dict()\n# {\"order\": 0, \"type\": \"text\", \"xywh\": [100, 50, 200, 150], \"detection_confidence\": 0.95, ...}\n\n# Deserialize from JSON (supports xywh)\nblock = Block.from_dict(data)\n</code></pre>"},{"location":"guides/bbox-formats/#key-points","title":"Key Points","text":"<ol> <li>Internal representation: <code>BBox(x0, y0, x1, y1)</code> with integer coordinates (xyxy format)</li> <li>JSON output: <code>[x, y, w, h]</code> (xywh format) for human readability</li> <li>Most frameworks use the same internal format: <code>[x0, y0, x1, y1]</code> with Top-Left origin</li> <li>Only PyPDF is different: Bottom-Left origin requires Y-axis flip</li> <li>Region.bbox is always present: No longer optional (required field)</li> </ol>"},{"location":"guides/contributing/","title":"Contributing Guide","text":"<p>Thank you for considering contributing to VLM OCR Pipeline! This guide will help you get started.</p>"},{"location":"guides/contributing/#development-setup","title":"Development Setup","text":""},{"location":"guides/contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":"<pre><code># Fork the repository on GitHub, then clone your fork\ngit clone https://github.com/YOUR_USERNAME/vlm-ocr-pipeline.git\ncd vlm-ocr-pipeline\n</code></pre>"},{"location":"guides/contributing/#2-set-up-development-environment","title":"2. Set Up Development Environment","text":"<pre><code># Create virtual environment\nuv venv --python 3.11 .venv\nsource .venv/bin/activate\n\n# Install dependencies\nuv pip install -r requirements.txt\n\n# Install development dependencies\nuv pip install pytest pytest-cov ruff pyright\n\n# Run setup script\npython setup.py\n</code></pre>"},{"location":"guides/contributing/#3-create-a-branch","title":"3. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/your-bugfix-name\n</code></pre>"},{"location":"guides/contributing/#code-quality-standards","title":"Code Quality Standards","text":""},{"location":"guides/contributing/#type-annotations","title":"Type Annotations","text":"<p>All functions and methods must have type annotations:</p> <pre><code>def process_blocks(\n    self,\n    image: np.ndarray,\n    blocks: Sequence[Block]\n) -&gt; list[Block]:\n    \"\"\"Process blocks to extract text.\"\"\"\n    ...\n</code></pre>"},{"location":"guides/contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings for all public functions and classes:</p> <pre><code>def detect_layout(image: np.ndarray, confidence_threshold: float = 0.5) -&gt; list[Block]:\n    \"\"\"Detect layout blocks in an image.\n\n    Args:\n        image: Input image as numpy array (H, W, C)\n        confidence_threshold: Minimum confidence score for detection\n\n    Returns:\n        List of detected blocks with bounding boxes\n\n    Raises:\n        DetectionError: If detection fails\n\n    Example:\n        &gt;&gt;&gt; detector = DocLayoutYOLO()\n        &gt;&gt;&gt; blocks = detector.detect(image, confidence_threshold=0.7)\n        &gt;&gt;&gt; len(blocks)\n        15\n    \"\"\"\n</code></pre>"},{"location":"guides/contributing/#code-style","title":"Code Style","text":"<p>We use ruff for linting and formatting:</p> <pre><code># Format code\nuv run ruff format .\n\n# Check linting\nuv run ruff check .\n\n# Auto-fix linting issues\nuv run ruff check . --fix\n</code></pre> <p>Configuration (ruff.toml): - Line length: 120 characters - Import order: isort compatible - First-party modules: <code>[\"pipeline\", \"models\"]</code></p>"},{"location":"guides/contributing/#type-checking","title":"Type Checking","text":"<p>We use pyright for static type checking:</p> <pre><code># Type check entire project\nnpx pyright\n\n# Type check specific files\nnpx pyright pipeline/__init__.py\n</code></pre> <p>Note: Use <code>npx pyright</code>, not global <code>pyright</code></p>"},{"location":"guides/contributing/#testing","title":"Testing","text":""},{"location":"guides/contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run specific test file\nuv run pytest tests/test_types.py\n\n# Run with coverage\nuv run pytest --cov=pipeline --cov-report=term-missing\n\n# Run verbose\nuv run pytest -v\n</code></pre>"},{"location":"guides/contributing/#writing-tests","title":"Writing Tests","text":"<p>Place tests in <code>tests/</code> directory with naming convention <code>test_*.py</code>:</p> <pre><code>def test_bbox_from_yolo():\n    \"\"\"Test BBox conversion from YOLO format.\"\"\"\n    bbox = BBox.from_yolo([0.5, 0.5, 0.3, 0.4], 1000, 800)\n    assert bbox.x0 == 350\n    assert bbox.y0 == 240\n    assert bbox.x1 == 650\n    assert bbox.y1 == 640\n</code></pre> <p>Coverage Goal: 90%+ for new code</p>"},{"location":"guides/contributing/#test-fixtures","title":"Test Fixtures","text":"<p>Use fixtures in <code>tests/fixtures/</code>: - Sample images - Sample PDFs - Expected outputs</p>"},{"location":"guides/contributing/#bbox-handling-rules","title":"BBox Handling Rules","text":"<p>Critical: BBox Standards</p> <ol> <li>Always use BBox class - Never use raw lists/tuples</li> <li>Internal operations use xyxy - Access via <code>bbox.x0, bbox.y0, bbox.x1, bbox.y1</code></li> <li>JSON serialization uses xywh - Call <code>bbox.to_xywh_list()</code></li> <li>Accept floats, output integers - All BBox methods round to nearest integer</li> <li>PyPDF requires page height - Use <code>BBox.from_pypdf_rect(rect, page_height)</code></li> </ol>"},{"location":"guides/contributing/#adding-new-components","title":"Adding New Components","text":""},{"location":"guides/contributing/#adding-a-detector","title":"Adding a Detector","text":"<ol> <li>Create detector file in <code>pipeline/layout/detection/</code></li> <li>Implement <code>Detector</code> protocol from <code>pipeline/types.py</code></li> <li>Register in <code>create_detector()</code> factory</li> <li>Add validation rule in <code>validate_combination()</code> if needed</li> <li>Write tests in <code>tests/test_detectors.py</code></li> </ol> <p>Example:</p> <pre><code># pipeline/layout/detection/my_detector.py\nfrom pipeline.types import Block, Detector\n\nclass MyDetector:\n    \"\"\"My custom detector implementation.\"\"\"\n\n    def detect(self, image: np.ndarray) -&gt; list[Block]:\n        \"\"\"Detect layout blocks.\n\n        Args:\n            image: Input image (H, W, C)\n\n        Returns:\n            List of detected blocks\n        \"\"\"\n        # Your detection logic\n        return blocks\n\n# pipeline/layout/detection/__init__.py\ndef create_detector(name: str, **kwargs) -&gt; Detector:\n    if name == \"my-detector\":\n        from .my_detector import MyDetector  # noqa: PLC0415\n        return MyDetector(**kwargs)\n    ...\n</code></pre>"},{"location":"guides/contributing/#adding-a-sorter","title":"Adding a Sorter","text":"<p>Similar process in <code>pipeline/layout/ordering/</code>:</p> <pre><code>from pipeline.types import Block, Sorter\n\nclass MySorter:\n    \"\"\"My custom sorter implementation.\"\"\"\n\n    def sort(self, blocks: list[Block], image: np.ndarray, **kwargs) -&gt; list[Block]:\n        \"\"\"Sort blocks in reading order.\n\n        Args:\n            blocks: Detected blocks\n            image: Original image for context\n\n        Returns:\n            Sorted blocks with order field\n        \"\"\"\n        # Your sorting logic\n        return sorted_blocks\n</code></pre>"},{"location":"guides/contributing/#adding-prompts","title":"Adding Prompts","text":"<p>Place YAML prompts in <code>settings/prompts/{model}/</code>:</p> <pre><code># settings/prompts/my-model/text_extraction.yaml\nsystem: |\n  You are an expert OCR system.\n\nuser: |\n  Extract text from this image.\n  Preserve formatting and structure.\n\nfallback: |\n  [OCR failed]\n</code></pre>"},{"location":"guides/contributing/#error-handling","title":"Error Handling","text":"<p>Follow the error handling policy (see Error Handling Guide):</p>"},{"location":"guides/contributing/#custom-exceptions","title":"Custom Exceptions","text":"<p>Use specific exception types from <code>pipeline/exceptions.py</code>:</p> <pre><code>from pipeline.exceptions import DetectionError, InvalidConfigError\n\nif confidence &lt; 0 or confidence &gt; 1:\n    raise InvalidConfigError(f\"Confidence must be between 0 and 1, got {confidence}\")\n\ntry:\n    blocks = self.detector.detect(image)\nexcept Exception as e:\n    raise DetectionError(f\"Detection failed: {e}\") from e\n</code></pre>"},{"location":"guides/contributing/#error-logging","title":"Error Logging","text":"<p>Use proper logging with <code>%s</code> formatting (not f-strings):</p> <pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\n# \u2705 Good\nlogger.error(\"Failed to load file %s: %s\", file_path, error)\n\n# \u274c Bad\nlogger.error(f\"Failed to load file {file_path}: {error}\")\n</code></pre> <p>Add <code>exc_info=True</code> for unexpected errors:</p> <pre><code>except Exception as e:\n    logger.error(\"Unexpected error: %s\", e, exc_info=True)\n</code></pre>"},{"location":"guides/contributing/#commit-guidelines","title":"Commit Guidelines","text":""},{"location":"guides/contributing/#commit-messages","title":"Commit Messages","text":"<p>Follow conventional commits:</p> <pre><code>feat: add new detector for layout analysis\nfix: resolve type error in BBox conversion\ndocs: update installation guide\ntest: add tests for multi-column detection\nrefactor: simplify block sorting logic\nperf: optimize image preprocessing\n</code></pre>"},{"location":"guides/contributing/#before-committing","title":"Before Committing","text":"<pre><code># Format code\nuv run ruff format .\n\n# Check linting\nuv run ruff check .\n\n# Run tests\nuv run pytest\n\n# Type check\nnpx pyright\n</code></pre>"},{"location":"guides/contributing/#creating-a-pull-request","title":"Creating a Pull Request","text":"<ol> <li>Ensure all tests pass</li> <li>Update documentation if needed</li> <li>Add entry to CHANGELOG (if exists)</li> <li>Create PR with clear description:</li> </ol> <pre><code>## Summary\n\nBrief description of changes\n\n## Changes\n\n- Added feature X\n- Fixed bug Y\n- Updated documentation Z\n\n## Testing\n\n- Tested on Python 3.11\n- All existing tests pass\n- Added new tests for feature X\n\n## Breaking Changes\n\nNone (or list if applicable)\n</code></pre>"},{"location":"guides/contributing/#common-pitfalls","title":"Common Pitfalls","text":"<p>Avoid These Mistakes</p> <ul> <li>Don't use bare except: Catch specific exceptions</li> <li>Don't create empty <code>__init__.py</code>: Use PEP 420 namespace packages</li> <li>Don't install with <code>-e</code>: Never use editable mode from external directories</li> <li>Don't mix xywh/xyxy: Always convert via BBox methods</li> <li>Don't forget page_height for PyPDF: Y-axis flip required</li> </ul>"},{"location":"guides/contributing/#documentation","title":"Documentation","text":""},{"location":"guides/contributing/#building-docs-locally","title":"Building Docs Locally","text":"<pre><code># Install MkDocs\nuv pip install mkdocs mkdocs-material mkdocstrings[python]\n\n# Serve docs locally\nmkdocs serve\n\n# Build docs\nmkdocs build\n</code></pre>"},{"location":"guides/contributing/#writing-docs","title":"Writing Docs","text":"<ul> <li>Use Markdown with admonitions</li> <li>Include code examples</li> <li>Add mermaid diagrams where helpful</li> <li>Cross-reference related pages</li> </ul>"},{"location":"guides/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> <li>Documentation: This site</li> </ul>"},{"location":"guides/contributing/#code-review-process","title":"Code Review Process","text":"<ol> <li>All PRs require review</li> <li>Address review comments</li> <li>Keep PRs focused (one feature/fix per PR)</li> <li>Maintain backward compatibility when possible</li> <li>Update tests and docs</li> </ol>"},{"location":"guides/contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the same license as the project.</p>"},{"location":"guides/detector-block-types/","title":"Block Type System Documentation","text":"<p>This document describes the standardized block type system used throughout the VLM OCR Pipeline.</p>"},{"location":"guides/detector-block-types/#overview","title":"Overview","text":"<p>All detectors in this pipeline use a unified type system that supports 25+ block types. The type system is based on comprehensive document element categories, with detector-specific types automatically mapped to standardized types for consistent processing throughout the pipeline.</p>"},{"location":"guides/detector-block-types/#implementation","title":"Implementation","text":"<ul> <li>Type definitions: <code>pipeline/types.py</code> - <code>Block</code> dataclass with <code>type: str</code> field</li> <li>Detector integration: Applied in all detectors during block creation</li> <li>Output conversion: <code>pipeline/conversion/output/markdown/__init__.py</code></li> </ul>"},{"location":"guides/detector-block-types/#type-mapping-process","title":"Type Mapping Process","text":"<ol> <li>Detector returns blocks with native type names (e.g., <code>\"plain text\"</code>, <code>\"abandon\"</code>, <code>\"figure_caption\"</code>)</li> <li>Type normalization converts to standardized type:</li> <li><code>\"plain text\"</code> \u2192 <code>\"text\"</code></li> <li><code>\"abandon\"</code> \u2192 <code>\"discarded\"</code></li> <li><code>\"figure_caption\"</code> \u2192 <code>\"image_caption\"</code></li> <li>All downstream pipeline stages use standardized types</li> <li>Markdown conversion handles all 25+ standardized types</li> </ol>"},{"location":"guides/detector-block-types/#standardized-block-types","title":"Standardized Block Types","text":"<p>Based on MinerU 2.5 VLM (<code>external/MinerU/mineru/utils/enum_class.py</code>):</p>"},{"location":"guides/detector-block-types/#content-types","title":"Content Types","text":"<ul> <li><code>text</code> - Body text paragraphs</li> <li><code>title</code> - Document/section titles</li> </ul>"},{"location":"guides/detector-block-types/#figure-types","title":"Figure Types","text":"<ul> <li><code>image</code> - Image regions</li> <li><code>image_body</code> - Image content area</li> <li><code>image_caption</code> - Image captions</li> <li><code>image_footnote</code> - Image footnotes</li> </ul>"},{"location":"guides/detector-block-types/#table-types","title":"Table Types","text":"<ul> <li><code>table</code> - Table regions</li> <li><code>table_body</code> - Table content area</li> <li><code>table_caption</code> - Table captions</li> <li><code>table_footnote</code> - Table footnotes</li> </ul>"},{"location":"guides/detector-block-types/#equation-types","title":"Equation Types","text":"<ul> <li><code>interline_equation</code> - Display equations (block-level)</li> <li><code>inline_equation</code> - Inline equations</li> </ul>"},{"location":"guides/detector-block-types/#code-types","title":"Code Types","text":"<ul> <li><code>code</code> - Code blocks</li> <li><code>code_body</code> - Code content area</li> <li><code>code_caption</code> - Code captions</li> <li><code>algorithm</code> - Algorithm pseudocode</li> </ul>"},{"location":"guides/detector-block-types/#list-types","title":"List Types","text":"<ul> <li><code>list</code> - List items</li> </ul>"},{"location":"guides/detector-block-types/#page-elements","title":"Page Elements","text":"<ul> <li><code>header</code> - Page headers (not content headings)</li> <li><code>footer</code> - Page footers</li> <li><code>page_number</code> - Page numbers</li> <li><code>page_footnote</code> - Page-level footnotes</li> </ul>"},{"location":"guides/detector-block-types/#reference-types","title":"Reference Types","text":"<ul> <li><code>ref_text</code> - Reference text</li> <li><code>phonetic</code> - Phonetic annotations</li> <li><code>aside_text</code> - Aside/sidebar text</li> <li><code>index</code> - Index entries</li> </ul>"},{"location":"guides/detector-block-types/#special-types","title":"Special Types","text":"<ul> <li><code>discarded</code> - Discarded/invalid content</li> </ul>"},{"location":"guides/detector-block-types/#detector-support-matrix","title":"Detector Support Matrix","text":"Block Type DocLayoutYOLO MinerU DocLayoutYOLO MinerU VLM 2.5 PaddleOCR PP-DocLayoutV2 Content Types <code>text</code> <code>plain text</code> <code>plain text</code> <code>text</code> <code>text</code><code>vertical_text</code><code>abstract</code><code>contents</code> <code>title</code> <code>title</code> <code>title</code> <code>title</code> <code>doc_title</code><code>paragraph_title</code> Figure Types <code>image</code> <code>figure</code> <code>figure</code> <code>image</code> <code>image</code><code>chart</code><code>seal</code> <code>image_body</code> - - <code>image_body</code> - <code>image_caption</code> - <code>figure_caption</code><code>formula_caption</code> <code>image_caption</code> <code>figure_title</code> <code>image_footnote</code> - - <code>image_footnote</code> - Table Types <code>table</code> <code>table</code> <code>table</code> <code>table</code> <code>table</code> <code>table_body</code> - - <code>table_body</code> - <code>table_caption</code> - <code>table_caption</code> <code>table_caption</code> <code>figure_title</code> <code>table_footnote</code> - <code>table_footnote</code> <code>table_footnote</code> - Equation Types <code>interline_equation</code> <code>equation</code> <code>isolate_formula</code> <code>interline_equation</code> <code>display_formula</code><code>formula_number</code> <code>inline_equation</code> - - <code>inline_equation</code> <code>inline_formula</code> Code Types <code>code</code> - - <code>code</code> - <code>code_body</code> - - <code>code_body</code> - <code>code_caption</code> - - <code>code_caption</code> - <code>algorithm</code> - - <code>algorithm</code> <code>algorithm</code> List Types <code>list</code> <code>list</code><code>list_item</code> - <code>list</code> - Page Elements <code>header</code> - - <code>header</code> <code>header</code><code>header_image</code> <code>footer</code> - - <code>footer</code> <code>footer</code><code>footer_image</code> <code>page_number</code> - - <code>page_number</code> <code>page_number</code> <code>page_footnote</code> - - <code>page_footnote</code> <code>footnote</code> Reference Types <code>ref_text</code> - - <code>ref_text</code> <code>reference</code><code>reference_content</code> <code>phonetic</code> - - <code>phonetic</code> - <code>aside_text</code> - - <code>aside_text</code> <code>aside_text</code> <code>index</code> - - <code>index</code> - Special Types <code>discarded</code> - <code>abandon</code> <code>discarded</code> - <p>Legend: - Detector-specific type names shown in backticks - <code>-</code> = Not supported by detector - Multiple types per cell separated by line breaks</p>"},{"location":"guides/detector-block-types/#detector-specifications","title":"Detector Specifications","text":""},{"location":"guides/detector-block-types/#doclayout-yolo-project","title":"DocLayout-YOLO (Project)","text":"<p>Implementation: <code>pipeline/layout/detection/doclayout_yolo.py</code></p> <p>Type System: Model-dependent (loaded from YOLO model weights)</p> <p>Common Types: - <code>title</code>, <code>plain text</code>, <code>figure</code>, <code>table</code>, <code>equation</code>, <code>list</code>, <code>list_item</code></p> <p>Notes: - Actual types depend on model's <code>class_names</code> (determined at runtime) - DocStructBench model typically includes 6-8 types</p>"},{"location":"guides/detector-block-types/#paddleocr-pp-doclayoutv2","title":"PaddleOCR PP-DocLayoutV2","text":"<p>Implementation: <code>pipeline/layout/detection/paddleocr/doclayout_v2.py</code></p> <p>Type System: Fixed 25-type system with integrated reading order</p> <p>Supported Types: <pre><code>{\n    # Titles and text\n    \"doc_title\": \"title\",\n    \"paragraph_title\": \"title\",\n    \"text\": \"text\",\n    \"vertical_text\": \"text\",\n    \"aside_text\": \"aside_text\",\n\n    # Page elements\n    \"page_number\": \"page_number\",\n    \"header\": \"header\",\n    \"footer\": \"footer\",\n    \"header_image\": \"header\",\n    \"footer_image\": \"footer\",\n\n    # Structural elements\n    \"abstract\": \"text\",\n    \"contents\": \"text\",\n    \"reference\": \"ref_text\",\n    \"reference_content\": \"ref_text\",\n    \"footnote\": \"page_footnote\",\n\n    # Math and formulas\n    \"inline_formula\": \"inline_equation\",\n    \"display_formula\": \"interline_equation\",\n    \"formula_number\": \"interline_equation\",\n    \"algorithm\": \"algorithm\",\n\n    # Visual elements\n    \"image\": \"image\",\n    \"table\": \"table\",\n    \"chart\": \"image\",\n    \"seal\": \"image\",\n\n    # Captions (unified as figure_title)\n    \"figure_title\": \"image_caption\",  # Includes figure/table/chart captions\n}\n</code></pre></p> <p>Features: - Unified model: RT-DETR-L based PP-DocLayout_plus-L (81.4 mAP) - Reading order: Built-in pointer network (6 Transformer layers) - Output is pre-sorted: Blocks already have <code>order</code> field set - 25 categories: Most comprehensive category coverage - Model size: 203.8 MB - Supports: Chinese, English, Japanese, and vertical text documents - Trained on: Papers, magazines, PPTs, contracts, and diverse document types</p> <p>Notes: - No additional sorter needed - reading order is built-in - <code>figure_title</code> applies to all captions (images, tables, charts) - Distinguishes between doc-level and paragraph-level titles</p>"},{"location":"guides/detector-block-types/#mineru-doclayout-yolo","title":"MinerU DocLayout-YOLO","text":"<p>Implementation: <code>pipeline/layout/detection/mineru/doclayout_yolo.py</code></p> <p>Type System: Fixed 10-type system</p> <p>Supported Types: <pre><code>{\n    0: \"title\",\n    1: \"plain text\",\n    2: \"abandon\",           # \u2192 discarded\n    3: \"figure\",            # \u2192 image\n    4: \"figure_caption\",    # \u2192 image_caption\n    5: \"table\",\n    6: \"table_caption\",\n    7: \"table_footnote\",\n    8: \"isolate_formula\",   # \u2192 interline_equation\n    9: \"formula_caption\",   # \u2192 image_caption\n}\n</code></pre></p>"},{"location":"guides/detector-block-types/#mineru-vlm-25","title":"MinerU VLM 2.5","text":"<p>Implementation: <code>pipeline/layout/detection/mineru/vlm.py</code></p> <p>Type System: 25 standardized types (canonical reference)</p> <p>Features: - Most comprehensive type coverage - Includes code, algorithm, and specialized academic types - Distinguishes between body/caption/footnote for images, tables, and code - Separate page elements (header, footer, page_number) - Already uses standardized types (identity mapping)</p>"},{"location":"guides/detector-block-types/#olmocr-vlm","title":"olmOCR VLM","text":"<p>Implementation: Used via sorters in <code>pipeline/layout/ordering/olmocr/</code></p> <p>Type System: Single type (<code>text</code>)</p> <p>Special Behavior: - VLM generates complete Markdown directly - Output includes YAML front matter with metadata - No block-level type distinction - All content marked as <code>text</code> type</p> <p>Example Output: <pre><code>---\nprimary_language: en\nis_rotation_valid: true\nrotation_correction: 0\n---\n\n# Chapter 1\n\nContent with **formatting**, $$equations$$, and tables.\n</code></pre></p>"},{"location":"guides/detector-block-types/#markdown-conversion-rules","title":"Markdown Conversion Rules","text":"<p>Defined in <code>pipeline/conversion/output/markdown/__init__.py</code>:</p> Type Markdown Format Example <code>title</code> <code># {text}</code> <code># Introduction</code> <code>text</code> <code>{text}</code> Plain text <code>image</code>, <code>image_body</code> <code>**Figure:** {text}</code> <code>**Figure:** Chart showing...</code> <code>image_caption</code> <code>**Figure:** {text}</code> <code>**Figure:** Monthly sales</code> <code>image_footnote</code> <code>*{text}*</code> <code>*Source: 2024 report*</code> <code>table</code>, <code>table_body</code> <code>{text}</code> or <code>**Table:**\\n\\n{text}</code> Markdown table or formatted <code>table_caption</code> <code>**Table:** {text}</code> <code>**Table:** Q1 Results</code> <code>table_footnote</code> <code>*{text}*</code> <code>*n=100*</code> <code>interline_equation</code> <code>$${text}$$</code> <code>$$E = mc^2$$</code> <code>inline_equation</code> <code>${text}$</code> <code>$x^2$</code> <code>code</code>, <code>code_body</code>, <code>algorithm</code> <code>```\\n{text}\\n```</code> Code block <code>code_caption</code> <code>**Code:** {text}</code> <code>**Code:** Algorithm 1</code> <code>list</code> <code>- {text}</code> <code>- Item</code> <code>header</code>, <code>footer</code>, <code>page_number</code> (skipped) - <code>ref_text</code> <code>{text}</code> Plain text <code>phonetic</code>, <code>aside_text</code> <code>*{text}*</code> <code>*pronunciation*</code> <code>page_footnote</code> <code>*{text}*</code> <code>*footnote*</code> <code>index</code> <code>{text}</code> Plain text <code>discarded</code> (skipped) -"},{"location":"guides/detector-block-types/#adding-new-detectors","title":"Adding New Detectors","text":"<p>To integrate a new detector with the type system:</p> <ol> <li> <p>Implement the Detector protocol (see <code>pipeline/types.py</code>):    <pre><code>class MyDetector:\n    def detect(self, image: np.ndarray) -&gt; list[Block]:\n        # Return blocks with native type names\n        pass\n</code></pre></p> </li> <li> <p>Create type mapping dictionary (if detector uses non-standard types):    <pre><code>_TYPE_MAP = {\n    \"native_type_1\": \"text\",\n    \"native_type_2\": \"image\",\n    \"native_caption\": \"image_caption\",\n    # Map all detector-specific types to standardized types\n}\n</code></pre></p> </li> <li> <p>Apply mapping in detector's block creation:    <pre><code>def detect(self, image: np.ndarray) -&gt; list[Block]:\n    # Get detections from model\n    raw_detections = self.model.predict(image)\n\n    # Convert to Block objects with standardized types\n    blocks = []\n    for det in raw_detections:\n        # Map type using _TYPE_MAP\n        standardized_type = self._TYPE_MAP.get(det.type, det.type)\n\n        block = Block(\n            type=standardized_type,\n            bbox=BBox.from_xyxy(det.x0, det.y0, det.x1, det.y1),\n            detection_confidence=det.confidence,\n            order=None,  # Set by sorter\n            column_index=None,  # Set by sorter if multi-column\n            text=None,  # Set by recognizer\n            corrected_text=None,\n            correction_ratio=None,\n            source=\"my-detector\",\n        )\n        blocks.append(block)\n\n    return blocks\n</code></pre></p> </li> <li> <p>Register in factory (see <code>pipeline/layout/detection/__init__.py</code>):    <pre><code>def create_detector(name: str, **kwargs) -&gt; Detector:\n    if name == \"my-detector\":\n        from .my_detector import MyDetector  # noqa: PLC0415\n        return MyDetector(**kwargs)\n    # ...\n</code></pre></p> </li> </ol> <p>That's it! The rest of the pipeline will automatically handle your detector's output using standardized types.</p>"},{"location":"guides/error-handling/","title":"Error Handling Guidelines","text":"<p>This document defines the error handling policy for the VLM OCR Pipeline project.</p>"},{"location":"guides/error-handling/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Error Handling Guidelines</li> <li>Table of Contents</li> <li>1. Custom Exception Hierarchy</li> <li>2. When to Use Each Exception<ul> <li>ConfigurationError</li> <li>APIError</li> <li>ProcessingError</li> <li>FileError</li> <li>DependencyError</li> </ul> </li> <li>3. Exception Handling Best Practices<ul> <li>3.1. Catch Specific Exceptions</li> <li>3.2. Exception Chaining</li> <li>3.3. When to Use Broad Exception Handlers</li> <li>3.4. Re-raising Exceptions</li> </ul> </li> <li>4. Error Logging Standards<ul> <li>4.1. Logging Levels</li> <li>4.2. Log Message Format</li> <li>4.3. Exception Stack Traces</li> </ul> </li> <li>5. Error Recovery Strategies<ul> <li>5.1. Graceful Degradation</li> <li>5.2. Retry Logic</li> <li>5.3. Fallback Values</li> </ul> </li> <li>6. Testing Error Handling</li> <li>7. Migration Guide<ul> <li>Step 1: Identify the error type</li> <li>Step 2: Choose the appropriate custom exception</li> <li>Step 3: Add proper error context</li> <li>Step 4: Update logging</li> </ul> </li> </ul>"},{"location":"guides/error-handling/#1-custom-exception-hierarchy","title":"1. Custom Exception Hierarchy","text":"<p>All custom exceptions inherit from <code>PipelineError</code> and are defined in <code>pipeline/exceptions.py</code>:</p> <pre><code>PipelineError (base)\n\u251c\u2500\u2500 ConfigurationError\n\u2502   \u251c\u2500\u2500 InvalidConfigError\n\u2502   \u2514\u2500\u2500 MissingConfigError\n\u251c\u2500\u2500 APIError\n\u2502   \u251c\u2500\u2500 APIClientError\n\u2502   \u251c\u2500\u2500 APIAuthenticationError\n\u2502   \u251c\u2500\u2500 APIRateLimitError\n\u2502   \u2514\u2500\u2500 APITimeoutError\n\u251c\u2500\u2500 ProcessingError\n\u2502   \u251c\u2500\u2500 PageProcessingError\n\u2502   \u251c\u2500\u2500 DetectionError\n\u2502   \u251c\u2500\u2500 RecognitionError\n\u2502   \u2514\u2500\u2500 RenderingError\n\u251c\u2500\u2500 FileError\n\u2502   \u251c\u2500\u2500 FileLoadError\n\u2502   \u251c\u2500\u2500 FileSaveError\n\u2502   \u2514\u2500\u2500 FileFormatError\n\u2514\u2500\u2500 DependencyError\n</code></pre> <p>Import all exceptions from: <pre><code>from pipeline.exceptions import (\n    PipelineError,\n    ConfigurationError,\n    InvalidConfigError,\n    MissingConfigError,\n    APIError,\n    APIClientError,\n    APIAuthenticationError,\n    APIRateLimitError,\n    APITimeoutError,\n    ProcessingError,\n    PageProcessingError,\n    DetectionError,\n    RecognitionError,\n    RenderingError,\n    FileError,\n    FileLoadError,\n    FileSaveError,\n    FileFormatError,\n    DependencyError,\n)\n</code></pre></p>"},{"location":"guides/error-handling/#2-when-to-use-each-exception","title":"2. When to Use Each Exception","text":""},{"location":"guides/error-handling/#configurationerror","title":"ConfigurationError","text":"<p>Use when dealing with configuration files, settings, or initialization parameters.</p> <p>InvalidConfigError: <pre><code># Example: Invalid tier name\nif tier not in [\"free\", \"tier1\", \"tier2\", \"tier3\"]:\n    raise InvalidConfigError(f\"Invalid tier: {tier}. Must be one of: free, tier1, tier2, tier3\")\n\n# Example: Malformed YAML\ntry:\n    config = yaml.safe_load(f)\nexcept yaml.YAMLError as e:\n    raise InvalidConfigError(f\"Malformed YAML in {config_file}: {e}\") from e\n</code></pre></p> <p>MissingConfigError: <pre><code># Example: Missing API key\nif not api_key:\n    raise MissingConfigError(\"OpenAI API key not found. Set OPENAI_API_KEY environment variable.\")\n\n# Example: Required config file not found\nif not config_file.exists():\n    raise MissingConfigError(f\"Configuration file not found: {config_file}\")\n</code></pre></p>"},{"location":"guides/error-handling/#apierror","title":"APIError","text":"<p>Use when interacting with external APIs (OpenAI, Gemini, PaddleOCR-VL, etc.).</p> <p>APIClientError: <pre><code># Example: Client initialization failure\ntry:\n    self.client = OpenAI(api_key=api_key, base_url=base_url)\nexcept TypeError as e:\n    raise APIClientError(f\"Failed to initialize OpenAI client: {e}\") from e\n</code></pre></p> <p>APIAuthenticationError: <pre><code># Example: Invalid API key (from external library)\ntry:\n    response = self.client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    raise APIAuthenticationError(f\"OpenAI authentication failed: {e}\") from e\n</code></pre></p> <p>APIRateLimitError: <pre><code># Example: Rate limit exceeded\ntry:\n    response = self.client.chat.completions.create(...)\nexcept RateLimitError as e:\n    raise APIRateLimitError(f\"OpenAI rate limit exceeded: {e}\") from e\n</code></pre></p> <p>APITimeoutError: <pre><code># Example: Request timeout\ntry:\n    response = requests.get(url, timeout=30)\nexcept requests.Timeout as e:\n    raise APITimeoutError(f\"API request timed out: {e}\") from e\n</code></pre></p>"},{"location":"guides/error-handling/#processingerror","title":"ProcessingError","text":"<p>Use when processing documents through the pipeline stages.</p> <p>PageProcessingError: <pre><code># Example: Page rendering failure\ntry:\n    page_image = render_pdf_page(pdf_path, page_num)\nexcept Exception as e:\n    raise PageProcessingError(f\"Failed to process page {page_num}: {e}\") from e\n</code></pre></p> <p>DetectionError: <pre><code># Example: Layout detection failure\ntry:\n    blocks = self.detector.detect(page_image)\nexcept Exception as e:\n    raise DetectionError(f\"Layout detection failed: {e}\") from e\n</code></pre></p> <p>RecognitionError: <pre><code># Example: Text recognition failure\ntry:\n    text = self.recognizer.extract_text(block_image)\nexcept Exception as e:\n    raise RecognitionError(f\"Text recognition failed for block: {e}\") from e\n</code></pre></p> <p>RenderingError: <pre><code># Example: Markdown conversion error\ntry:\n    markdown = block_to_markdown(block)\nexcept Exception as e:\n    raise RenderingError(f\"Failed to render block to markdown: {e}\") from e\n</code></pre></p>"},{"location":"guides/error-handling/#fileerror","title":"FileError","text":"<p>Use for file I/O operations.</p> <p>FileLoadError: <pre><code># Example: File not found\nif not file_path.exists():\n    raise FileLoadError(f\"File not found: {file_path}\")\n\n# Example: PDF loading failure\ntry:\n    images = convert_from_path(str(pdf_path))\nexcept Exception as e:\n    raise FileLoadError(f\"Failed to load PDF: {e}\") from e\n</code></pre></p> <p>FileSaveError: <pre><code># Example: Permission denied\ntry:\n    with open(output_file, \"w\") as f:\n        json.dump(data, f)\nexcept PermissionError as e:\n    raise FileSaveError(f\"Permission denied writing to {output_file}: {e}\") from e\n\n# Example: Disk full\nexcept OSError as e:\n    raise FileSaveError(f\"Failed to save file {output_file}: {e}\") from e\n</code></pre></p> <p>FileFormatError: <pre><code># Example: Invalid PDF\nif not is_valid_pdf(file_path):\n    raise FileFormatError(f\"Invalid PDF file: {file_path}\")\n\n# Example: Unsupported image format\nif file_path.suffix not in [\".png\", \".jpg\", \".jpeg\"]:\n    raise FileFormatError(f\"Unsupported image format: {file_path.suffix}\")\n</code></pre></p>"},{"location":"guides/error-handling/#dependencyerror","title":"DependencyError","text":"<p>Use when optional dependencies are missing or incompatible.</p> <pre><code># Example: Missing optional dependency\ntry:\n    import fitz  # PyMuPDF\nexcept ImportError as e:\n    raise DependencyError(\"PyMuPDF is required for multi-column detection. Install with: uv pip install pymupdf\") from e\n\n# Example: Incompatible version\nif version.parse(mineru.__version__) &lt; version.parse(\"0.8.0\"):\n    raise DependencyError(f\"MinerU version {mineru.__version__} is not supported. Requires &gt;= 0.8.0\")\n</code></pre>"},{"location":"guides/error-handling/#3-exception-handling-best-practices","title":"3. Exception Handling Best Practices","text":""},{"location":"guides/error-handling/#31-catch-specific-exceptions","title":"3.1. Catch Specific Exceptions","text":"<p>\u2705 Good: <pre><code>try:\n    config = yaml.safe_load(f)\nexcept yaml.YAMLError as e:\n    raise InvalidConfigError(f\"Malformed YAML: {e}\") from e\nexcept OSError as e:\n    raise FileLoadError(f\"Failed to read config file: {e}\") from e\n</code></pre></p> <p>\u274c Bad: <pre><code>try:\n    config = yaml.safe_load(f)\nexcept Exception as e:  # Too broad!\n    logger.error(\"Error: %s\", e)\n</code></pre></p>"},{"location":"guides/error-handling/#32-exception-chaining","title":"3.2. Exception Chaining","text":"<p>Always use <code>from e</code> to preserve the original exception context:</p> <p>\u2705 Good: <pre><code>try:\n    response = self.client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    raise APIAuthenticationError(f\"Authentication failed: {e}\") from e\n</code></pre></p> <p>\u274c Bad: <pre><code>try:\n    response = self.client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    raise APIAuthenticationError(f\"Authentication failed: {e}\")  # Lost context!\n</code></pre></p>"},{"location":"guides/error-handling/#33-when-to-use-broad-exception-handlers","title":"3.3. When to Use Broad Exception Handlers","text":"<p>Broad exception handlers (<code>except Exception</code>) are only allowed in these cases:</p> <ol> <li> <p>Top-level CLI entry points (with <code># noqa: BLE001</code> comment):    <pre><code># main.py\ntry:\n    result = pipeline.process(pdf_path)\nexcept Exception as exc:  # noqa: BLE001 - retain broad logging for CLI\n    logger.error(\"Unexpected error: %s\", exc, exc_info=True)\n    return 1\n</code></pre></p> </li> <li> <p>Optional dependency guards (with <code># pragma: no cover</code> comment):    <pre><code>try:\n    import fitz\nexcept Exception:  # pragma: no cover - optional dependency guard\n    fitz = None\n</code></pre></p> </li> <li> <p>Fallback for unexpected errors (after catching specific errors):    <pre><code>try:\n    response = self.client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    raise APIAuthenticationError(f\"Authentication failed: {e}\") from e\nexcept RateLimitError as e:\n    raise APIRateLimitError(f\"Rate limit exceeded: {e}\") from e\nexcept Exception as e:\n    # Fallback for unexpected errors (document why!)\n    logger.error(\"Unexpected API error: %s\", e)\n    return {\"error\": \"api_error\", \"message\": str(e)}\n</code></pre></p> </li> </ol>"},{"location":"guides/error-handling/#34-re-raising-exceptions","title":"3.4. Re-raising Exceptions","text":"<p>When you want to log an error but still propagate it:</p> <pre><code>try:\n    page_result = self._process_pdf_page(pdf_path, page_num)\nexcept PageProcessingError as e:\n    logger.error(\"Page %d processing failed: %s\", page_num, e)\n    raise  # Re-raise the same exception\n</code></pre>"},{"location":"guides/error-handling/#4-error-logging-standards","title":"4. Error Logging Standards","text":""},{"location":"guides/error-handling/#41-logging-levels","title":"4.1. Logging Levels","text":"<p>Use appropriate logging levels:</p> Level When to Use Example <code>DEBUG</code> Detailed diagnostic information <code>logger.debug(\"Processing block %d of %d\", i, total)</code> <code>INFO</code> General informational messages <code>logger.info(\"Loaded %d pages from %s\", len(pages), pdf_path)</code> <code>WARNING</code> Recoverable errors, fallback used <code>logger.warning(\"PyMuPDF not available, using basic sorter\")</code> <code>ERROR</code> Serious errors, operation failed <code>logger.error(\"Failed to process page %d: %s\", page_num, e)</code> <code>CRITICAL</code> Critical errors, system cannot continue <code>logger.critical(\"API key not found, cannot proceed\")</code>"},{"location":"guides/error-handling/#42-log-message-format","title":"4.2. Log Message Format","text":"<p>Format: <code>\"Action failed: %s\", error_details</code></p> <p>\u2705 Good: <pre><code>logger.error(\"Failed to load config file %s: %s\", config_path, e)\nlogger.warning(\"Rate limit reached. Waiting %.2f seconds...\", wait_time)\nlogger.info(\"Processed %d pages in %.2f seconds\", page_count, elapsed)\n</code></pre></p> <p>\u274c Bad: <pre><code>logger.error(f\"Failed to load config file {config_path}: {e}\")  # Don't use f-strings!\nlogger.error(\"Error!\")  # Not descriptive!\nlogger.error(str(e))  # Missing context!\n</code></pre></p> <p>Why avoid f-strings in logging? - f-strings are evaluated before the log level check (performance overhead) - <code>%s</code> formatting is only evaluated if the log level is enabled - Better for structured logging</p>"},{"location":"guides/error-handling/#43-exception-stack-traces","title":"4.3. Exception Stack Traces","text":"<p>Use <code>exc_info=True</code> to include full stack trace:</p> <p>\u2705 Good: <pre><code>try:\n    result = process_page(page_num)\nexcept PageProcessingError as e:\n    logger.error(\"Error processing page %d: %s\", page_num, e, exc_info=True)\n</code></pre></p> <p>When to use <code>exc_info=True</code>? - For unexpected errors at top-level handlers - When debugging complex issues - For errors that should never happen</p> <p>When NOT to use <code>exc_info=True</code>? - For expected errors (rate limits, missing files) - For informational warnings - When stack trace would be too verbose</p>"},{"location":"guides/error-handling/#5-error-recovery-strategies","title":"5. Error Recovery Strategies","text":""},{"location":"guides/error-handling/#51-graceful-degradation","title":"5.1. Graceful Degradation","text":"<p>Continue processing with reduced functionality:</p> <pre><code>try:\n    import fitz  # PyMuPDF\nexcept ImportError:\n    logger.warning(\"PyMuPDF not available. Multi-column detection disabled.\")\n    fitz = None\n\n# Later in code\nif fitz is not None:\n    # Use advanced multi-column detection\n    layout = detect_multi_column_layout(page)\nelse:\n    # Fallback to basic processing\n    layout = None\n</code></pre>"},{"location":"guides/error-handling/#52-retry-logic","title":"5.2. Retry Logic","text":"<p>Retry on transient failures:</p> <pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n    reraise=True\n)\ndef api_call_with_retry():\n    try:\n        return self.client.chat.completions.create(...)\n    except APITimeoutError:\n        logger.warning(\"API timeout, retrying...\")\n        raise\n</code></pre>"},{"location":"guides/error-handling/#53-fallback-values","title":"5.3. Fallback Values","text":"<p>Return safe defaults on error:</p> <pre><code>def load_config(config_path: Path) -&gt; dict[str, Any]:\n    \"\"\"Load configuration file with fallback to empty dict.\"\"\"\n    try:\n        with open(config_path) as f:\n            return yaml.safe_load(f)\n    except FileNotFoundError:\n        logger.debug(\"Config file not found: %s\", config_path)\n        return {}\n    except yaml.YAMLError as e:\n        logger.warning(\"Failed to parse config file %s: %s\", config_path, e)\n        return {}\n</code></pre>"},{"location":"guides/error-handling/#6-testing-error-handling","title":"6. Testing Error Handling","text":"<p>Always test error paths:</p> <pre><code>def test_invalid_tier_raises_error():\n    \"\"\"Test that InvalidConfigError is raised for invalid tier.\"\"\"\n    with pytest.raises(InvalidConfigError, match=\"Invalid tier\"):\n        rate_limiter.set_tier_and_model(\"invalid_tier\", \"gemini-2.5-flash\")\n\ndef test_missing_api_key_raises_error():\n    \"\"\"Test that MissingConfigError is raised when API key is missing.\"\"\"\n    with pytest.raises(MissingConfigError, match=\"API key not found\"):\n        OpenAIClient(api_key=None)\n\ndef test_file_not_found_raises_error():\n    \"\"\"Test that FileLoadError is raised for non-existent file.\"\"\"\n    with pytest.raises(FileLoadError, match=\"File not found\"):\n        load_pdf(\"/nonexistent/file.pdf\")\n</code></pre>"},{"location":"guides/error-handling/#7-migration-guide","title":"7. Migration Guide","text":"<p>When migrating from <code>except Exception</code> to specific exceptions:</p>"},{"location":"guides/error-handling/#step-1-identify-the-error-type","title":"Step 1: Identify the error type","text":"<p>Look at what can go wrong in the try block: <pre><code># Before\ntry:\n    config = yaml.safe_load(f)\nexcept Exception as e:\n    logger.error(\"Error: %s\", e)\n    return {}\n</code></pre></p>"},{"location":"guides/error-handling/#step-2-choose-the-appropriate-custom-exception","title":"Step 2: Choose the appropriate custom exception","text":"<ul> <li>YAML parsing error \u2192 <code>InvalidConfigError</code></li> <li>File I/O error \u2192 <code>FileLoadError</code></li> </ul>"},{"location":"guides/error-handling/#step-3-add-proper-error-context","title":"Step 3: Add proper error context","text":"<pre><code># After\ntry:\n    config = yaml.safe_load(f)\nexcept yaml.YAMLError as e:\n    raise InvalidConfigError(f\"Malformed YAML in {config_file}: {e}\") from e\nexcept OSError as e:\n    raise FileLoadError(f\"Failed to read config file {config_file}: {e}\") from e\n</code></pre>"},{"location":"guides/error-handling/#step-4-update-logging","title":"Step 4: Update logging","text":"<pre><code># Caller code\ntry:\n    config = load_config(config_path)\nexcept InvalidConfigError as e:\n    logger.error(\"Configuration error: %s\", e)\n    return {}\nexcept FileLoadError as e:\n    logger.warning(\"Config file not found: %s\", e)\n    return {}\n</code></pre> <p>Last Updated: 2025-01-26</p> <p>See Also: - <code>pipeline/exceptions.py</code> - Custom exception definitions - <code>CLAUDE.md</code> - Project coding standards - <code>.cursorrules</code> - Detailed coding guidelines</p>"},{"location":"reference/cli/","title":"CLI Reference","text":"<p>Complete command-line interface documentation.</p>"},{"location":"reference/cli/#main-command","title":"Main Command","text":"<pre><code>python main.py [OPTIONS]\n</code></pre>"},{"location":"reference/cli/#options","title":"Options","text":"<p>For complete CLI documentation, see Basic Usage.</p>"},{"location":"reference/cli/#input","title":"Input","text":"<ul> <li><code>--input PATH</code>: Input file or directory</li> <li><code>--max-pages N</code>: Process first N pages</li> <li><code>--page-range START-END</code>: Process page range</li> <li><code>--pages LIST</code>: Process specific pages (comma-separated)</li> </ul>"},{"location":"reference/cli/#backend","title":"Backend","text":"<ul> <li><code>--backend {openai,gemini}</code>: VLM backend</li> <li><code>--model MODEL</code>: Model name</li> <li><code>--gemini-tier {free,tier1,tier2,tier3}</code>: Gemini API tier</li> </ul>"},{"location":"reference/cli/#components","title":"Components","text":"<ul> <li><code>--detector NAME</code>: Layout detector</li> <li><code>--sorter NAME</code>: Reading order sorter</li> <li><code>--recognizer NAME</code>: Text recognizer</li> </ul>"},{"location":"reference/cli/#output","title":"Output","text":"<ul> <li><code>--output DIR</code>: Output directory</li> <li><code>--cache-dir DIR</code>: Cache directory</li> <li><code>--temp-dir DIR</code>: Temporary files directory</li> <li><code>--no-cache</code>: Disable caching</li> </ul>"},{"location":"reference/cli/#other","title":"Other","text":"<ul> <li><code>--dpi N</code>: PDF rendering DPI (default: 200)</li> <li><code>-v, --verbose</code>: Verbose output</li> <li><code>-vv</code>: Very verbose (debug)</li> <li><code>--rate-limit-status</code>: Check Gemini rate limits</li> </ul> <p>Note</p> <p>Run <code>python main.py --help</code> for complete option list.</p>"}]}