{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"VLM OCR Pipeline","text":"<p>A modular document processing system that combines layout detection (DocLayout-YOLO, MinerU, olmOCR, PaddleOCR) with Vision Language Models (OpenAI, Gemini, PaddleOCR-VL) for intelligent text extraction and correction.</p> <p>Based On</p> <p>This project is based on and modified from Versatile-OCR-Program</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#modular-architecture","title":"\ud83c\udfaf Modular Architecture","text":"<ul> <li>Flexible Detector/Sorter Combinations: Mix and match detectors (DocLayout-YOLO, PaddleOCR PP-DocLayoutV2, MinerU, olmOCR) with sorters (multi-column, LayoutReader, XY-Cut, VLM)</li> <li>8-Stage Pipeline: Document loading \u2192 Detection \u2192 Ordering \u2192 Recognition \u2192 Block correction \u2192 Rendering \u2192 Page correction \u2192 Output</li> <li>Unified BBox System: Integer-based bounding boxes (internal: xyxy, JSON: xywh) with automatic conversion between 6+ formats</li> </ul>"},{"location":"#multiple-recognition-backends","title":"\ud83e\udd16 Multiple Recognition Backends","text":"<ul> <li>Cloud VLM APIs: OpenAI GPT-4 Vision, Gemini 2.5 Flash</li> <li>Local VLM: PaddleOCR-VL-0.9B (0.9B parameters, NaViT + ERNIE-4.5-0.3B)</li> <li>109+ Languages: Extensive multilingual support</li> </ul>"},{"location":"#advanced-document-understanding","title":"\ud83d\udcc4 Advanced Document Understanding","text":"<ul> <li>Layout Detection: Automatically detects text, tables, figures, equations, lists</li> <li>Reading Order Analysis: Multi-column detection, LayoutLMv3, XY-Cut, VLM-based ordering</li> <li>Special Content Processing: Enhanced analysis of tables and figures with structured output</li> <li>AI-Powered Correction: Intelligent text correction using VLMs</li> </ul>"},{"location":"#performance-efficiency","title":"\u26a1 Performance &amp; Efficiency","text":"<ul> <li>Intelligent Caching: Content-based hashing to avoid reprocessing</li> <li>Rate Limiting: Automatic throttling for Gemini API (free/tier1/tier2/tier3)</li> <li>Batch Processing: Process entire directories of PDFs</li> <li>Model-Specific Prompts: YAML-based prompt templates for optimal results</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># Basic usage with Gemini\npython main.py --input document.pdf --backend gemini\n\n# Advanced: Custom detector + sorter + recognizer\npython main.py --input doc.pdf \\\n    --detector paddleocr-doclayout-v2 \\\n    --sorter mineru-layoutreader \\\n    --recognizer paddleocr-vl\n\n# Batch processing with page limits\npython main.py --input pdfs/ --backend openai --max-pages 5\n</code></pre>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph LR\n    A[Input PDF/Image] --&gt; B[Stage 1: Load Document]\n    B --&gt; C[Stage 2: Layout Detection]\n    C --&gt; D[Stage 3: Reading Order]\n    D --&gt; E[Stage 4: Text Recognition]\n    E --&gt; F[Stage 5: Block Correction]\n    F --&gt; G[Stage 6: Markdown Rendering]\n    G --&gt; H[Stage 7: Page Correction]\n    H --&gt; I[Stage 8: Save Output]\n</code></pre> <p>Each stage is modular and can be configured independently:</p> <ul> <li>Detection: <code>doclayout-yolo</code>, <code>paddleocr-doclayout-v2</code>, <code>mineru-vlm</code>, <code>olmocr-vlm</code></li> <li>Ordering: <code>pymupdf</code>, <code>mineru-layoutreader</code>, <code>mineru-xycut</code>, <code>mineru-vlm</code>, <code>olmocr-vlm</code></li> <li>Recognition: <code>openai</code>, <code>gemini</code>, <code>paddleocr-vl</code></li> </ul>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li> <p>:material-download:{ .lg .middle } Installation</p> <p>Install VLM OCR Pipeline in minutes</p> <p>:octicons-arrow-right-24: Getting started</p> </li> <li> <p>:material-book-open-variant:{ .lg .middle } User Guides</p> <p>Learn about BBox formats, detectors, and advanced usage</p> <p>:octicons-arrow-right-24: User Guides</p> </li> <li> <p>:material-api:{ .lg .middle } API Reference</p> <p>Explore the complete API documentation</p> <p>:octicons-arrow-right-24: API Reference</p> </li> <li> <p>:material-file-code:{ .lg .middle } Architecture</p> <p>Deep dive into pipeline stages and design patterns</p> <p>:octicons-arrow-right-24: Architecture</p> </li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub: NoUnique/vlm-ocr-pipeline</li> <li>Issues: Report bugs or request features</li> <li>Contributing: See our Contributing Guide</li> </ul>"},{"location":"AUTO_OPTIMIZATION_DESIGN/","title":"GPU Auto-Optimization Design","text":"<p>Core Principle: Zero-configuration optimization - the system automatically applies optimal settings based on detected GPU environment without any user intervention.</p>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#1-problem-statement","title":"1. Problem Statement","text":""},{"location":"AUTO_OPTIMIZATION_DESIGN/#manual-configuration-required-before","title":"Manual Configuration Required (Before)","text":"<pre><code># Too many manual settings required\npython main.py --input doc.pdf \\\n    --recognizer deepseek-ocr \\\n    --recognizer-backend vllm \\           # Manual 1\n    --tensor-parallel-size 4 \\            # Manual 2\n    --data-parallel-workers 2 \\           # Manual 3\n    --gpu-memory-utilization 0.90 \\       # Manual 4\n    --batch-size 8                        # Manual 5\n</code></pre> <p>Problem: Users must understand GPU environment and know optimal settings.</p>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#2-goal-complete-automation","title":"2. Goal: Complete Automation","text":""},{"location":"AUTO_OPTIMIZATION_DESIGN/#ideal-user-experience","title":"Ideal User Experience","text":"<pre><code># Optimal performance without any configuration\npython main.py --input doc.pdf --recognizer deepseek-ocr\n\n# Output:\n# \ud83d\udd0d Detecting GPU environment...\n# \u2705 Found 8x NVIDIA A100-SXM4-80GB (640GB total)\n# \ud83d\ude80 Auto-optimizing for maximum performance...\n#    Backend: vLLM (tensor parallel)\n#    Tensor Parallel Size: 4\n#    Data Parallel Workers: 2\n#    GPU Memory Utilization: 90%\n#    Flash Attention: Enabled\n# \u26a1 Expected speedup: 12x (vs single GPU)\n</code></pre> <p>User specifies nothing - system automatically optimizes</p>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#3-implementation-design","title":"3. Implementation Design","text":""},{"location":"AUTO_OPTIMIZATION_DESIGN/#31-architecture","title":"3.1 Architecture","text":"<pre><code>[User Input] \u2192 [Auto-Optimizer] \u2192 [Optimized Execution]\n                      \u2193\n          [GPU Environment Detector]\n</code></pre>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#32-core-components","title":"3.2 Core Components","text":""},{"location":"AUTO_OPTIMIZATION_DESIGN/#component-1-gpu-environment-detector-singleton","title":"Component 1: GPU Environment Detector (Singleton)","text":"<p>File: <code>pipeline/gpu_environment.py</code></p> <pre><code>@dataclass\nclass GPUConfig:\n    \"\"\"Auto-generated optimal GPU configuration.\"\"\"\n\n    # Detected environment\n    has_cuda: bool\n    gpu_count: int\n    total_memory_gb: float\n    compute_capability: tuple[int, int]\n\n    # Auto-optimized settings\n    recommended_backend: str  # \"vllm\", \"hf\", \"pytorch\"\n    tensor_parallel_size: int\n    data_parallel_workers: int\n    gpu_memory_utilization: float\n    use_flash_attention: bool\n    use_bf16: bool\n\n    optimization_strategy: str  # \"hybrid_tp4_dp2\", etc.\n    expected_speedup: float  # vs single GPU sequential\n\n\n@lru_cache(maxsize=1)\ndef get_gpu_config() -&gt; GPUConfig:\n    \"\"\"Get cached GPU configuration (singleton).\n\n    Called once at startup and cached.\n    All components reference this single source of truth.\n    \"\"\"\n    # Detect GPU environment\n    # Calculate optimal settings\n    # Return configuration\n</code></pre> <p>Detection Rules: - 8x A100 80GB \u2192 vLLM TP=4, DP=2 (hybrid, 12x speedup) - 4x A100 80GB \u2192 vLLM TP=2, DP=2 (hybrid, 6x speedup) - 2x A100 80GB \u2192 hf-ray DP=2 (data parallel, 2.5x speedup) - 1x A100 80GB \u2192 vLLM TP=1 (single GPU, 1.5x speedup) - No CUDA \u2192 pytorch CPU fallback (0.1x speedup)</p>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#component-2-recognizer-auto-optimization","title":"Component 2: Recognizer Auto-Optimization","text":"<p>File: <code>pipeline/recognition/__init__.py</code></p> <pre><code>def create_recognizer(\n    name: str,\n    backend: str | None = None,  # None = auto-select\n    use_auto_optimization: bool = True,\n    **kwargs: Any,\n) -&gt; Recognizer:\n    \"\"\"Create recognizer with auto-optimization.\n\n    Auto-optimization (when enabled):\n    - Auto-select backend based on GPU environment\n    - Auto-configure tensor_parallel_size\n    - Auto-configure gpu_memory_utilization\n    - Auto-select device placement\n    \"\"\"\n    if use_auto_optimization:\n        gpu_config = get_gpu_config()\n\n        # Auto-select backend if not specified\n        if backend is None:\n            backend = gpu_config.recommended_backend\n\n        # Inject auto-optimized settings (if not manually overridden)\n        if \"tensor_parallel_size\" not in kwargs:\n            kwargs[\"tensor_parallel_size\"] = gpu_config.tensor_parallel_size\n\n        if \"gpu_memory_utilization\" not in kwargs:\n            kwargs[\"gpu_memory_utilization\"] = gpu_config.gpu_memory_utilization\n\n    return _RECOGNIZER_REGISTRY[name](**kwargs)\n</code></pre>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#4-user-scenarios","title":"4. User Scenarios","text":""},{"location":"AUTO_OPTIMIZATION_DESIGN/#scenario-1-fully-automatic-recommended","title":"Scenario 1: Fully Automatic (Recommended)","text":"<pre><code>python main.py --input doc.pdf --recognizer deepseek-ocr\n</code></pre> <p>Internal Flow: 1. Detect GPU environment: 8x A100 80GB 2. Auto-select backend: vLLM 3. Auto-calculate settings: TP=4, DP=2 4. Expected speedup: 12x</p>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#scenario-2-partial-override","title":"Scenario 2: Partial Override","text":"<pre><code># Specify backend only, rest auto-optimized\npython main.py --input doc.pdf \\\n    --recognizer deepseek-ocr \\\n    --recognizer-backend hf\n</code></pre> <p>Internal Flow: 1. Detect GPU environment: 8x A100 80GB 2. Backend: hf (user-specified) 3. Auto-calculate settings: device_map=\"auto\", TP=4</p>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#scenario-3-advanced-users-fully-manual","title":"Scenario 3: Advanced Users (Fully Manual)","text":"<pre><code># Disable auto-optimization\npython main.py --input doc.pdf \\\n    --recognizer deepseek-ocr \\\n    --recognizer-backend vllm \\\n    --tensor-parallel-size 2 \\\n    --use-auto-optimization=False\n</code></pre>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#scenario-4-cpu-environment-auto-fallback","title":"Scenario 4: CPU Environment (Auto Fallback)","text":"<pre><code>python main.py --input doc.pdf --recognizer deepseek-ocr\n</code></pre> <p>Internal Flow: 1. Detect GPU environment: No CUDA 2. Auto-select backend: pytorch (CPU) 3. Log warning: Performance will be 10x slower</p>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#5-override-priority","title":"5. Override Priority","text":"<pre><code>1. Explicit CLI arguments (highest priority)\n   \u2514\u2500&gt; --tensor-parallel-size 8\n\n2. Environment variables\n   \u2514\u2500&gt; VLLM_TENSOR_PARALLEL_SIZE=4\n\n3. Configuration files\n   \u2514\u2500&gt; config/gpu_optimization.yaml\n\n4. Auto-detection (default)\n   \u2514\u2500&gt; gpu_config.recommended_tensor_parallel_size\n</code></pre>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#6-key-differences","title":"6. Key Differences","text":""},{"location":"AUTO_OPTIMIZATION_DESIGN/#before-problem","title":"Before (Problem)","text":"<pre><code># User must know everything\nllm = LLM(\n    model=model_name,\n    tensor_parallel_size=???,  # User must calculate?\n    gpu_memory_utilization=???,  # How much is safe?\n)\n</code></pre>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#after-automatic","title":"After (Automatic)","text":"<pre><code># Auto-optimized based on GPU environment\ngpu_config = get_gpu_config()  # Automatic detection\n\nllm = LLM(\n    model=model_name,\n    tensor_parallel_size=gpu_config.tensor_parallel_size,  # Automatic\n    gpu_memory_utilization=gpu_config.gpu_memory_utilization,  # Automatic\n)\n</code></pre>"},{"location":"AUTO_OPTIMIZATION_DESIGN/#7-expected-performance-8x-a100-80gb","title":"7. Expected Performance (8x A100 80GB)","text":"User Input Auto Settings Processing Time Speedup <code>python main.py --input doc.pdf</code> vLLM TP=4, DP=2 4.2 min 12x (no manual configuration) Auto Ray parallelization"},{"location":"AUTO_OPTIMIZATION_DESIGN/#8-design-philosophy","title":"8. Design Philosophy","text":"<ol> <li>Zero-configuration by default - Users should not need to understand GPU architecture</li> <li>Expert override available - Advanced users can still manually configure</li> <li>Fail-safe fallbacks - Always degrade gracefully (GPU \u2192 CPU)</li> <li>Single source of truth - One singleton <code>GPUConfig</code> referenced everywhere</li> <li>Logged transparency - Users see what the system auto-selected</li> </ol> <p>Core Concept: Users type <code>python main.py --input doc.pdf</code> and the system automatically finds and applies optimal settings.</p>"},{"location":"CI_CD/","title":"CI/CD Pipeline","text":"<p>This document describes the Continuous Integration and Continuous Deployment (CI/CD) workflows for the VLM OCR Pipeline project.</p>"},{"location":"CI_CD/#overview","title":"Overview","text":"<p>The project uses GitHub Actions for automated testing, linting, type checking, and documentation deployment. All workflows are defined in <code>.github/workflows/</code>.</p>"},{"location":"CI_CD/#workflows","title":"Workflows","text":""},{"location":"CI_CD/#1-main-ci-workflow-githubworkflowsciyml","title":"1. Main CI Workflow (<code>.github/workflows/ci.yml</code>)","text":"<p>Runs on every push to <code>main</code>/<code>develop</code> branches and on pull requests.</p>"},{"location":"CI_CD/#jobs","title":"Jobs","text":"<p>Lint and Format Check - Checks code formatting with <code>ruff format --check</code> - Runs linting with <code>ruff check</code> - Ensures code style consistency</p> <p>Type Check - Runs <code>pyright</code> for static type checking - Validates type annotations across the codebase - Uses Node.js 20 for <code>npx pyright</code></p> <p>Test - Runs pytest test suite - Tests on Python 3.11 and 3.12 - Uploads coverage reports to Codecov (optional) - Uses matrix strategy for multi-version testing</p> <p>Build Documentation - Builds MkDocs documentation with <code>--strict</code> flag - Catches documentation errors before deployment - Validates all documentation links and references</p> <p>All Checks Passed - Final gate that requires all previous jobs to succeed - Provides clear status for PR mergeability</p>"},{"location":"CI_CD/#trigger-events","title":"Trigger Events","text":"<pre><code>on:\n  push:\n    branches: [main, develop]\n  pull_request:\n    branches: [main, develop]\n  workflow_dispatch:  # Manual trigger\n</code></pre>"},{"location":"CI_CD/#2-pr-checks-workflow-githubworkflowspr-checksyml","title":"2. PR Checks Workflow (<code>.github/workflows/pr-checks.yml</code>)","text":"<p>Provides automated analysis and labeling for pull requests.</p>"},{"location":"CI_CD/#jobs_1","title":"Jobs","text":"<p>PR Information - Analyzes changed files (Python, tests, docs, workflows) - Counts lines added/deleted - Posts summary comment on PR - Updates existing comment on new commits</p> <p>Size Label - Automatically labels PRs by size:   - <code>size/XS</code>: &lt; 50 lines   - <code>size/S</code>: 50-200 lines   - <code>size/M</code>: 200-500 lines   - <code>size/L</code>: 500-1000 lines   - <code>size/XL</code>: &gt; 1000 lines</p>"},{"location":"CI_CD/#permissions","title":"Permissions","text":"<pre><code>permissions:\n  pull-requests: write\n  contents: read\n  checks: write\n</code></pre>"},{"location":"CI_CD/#3-documentation-deployment-githubworkflowsdocsyml","title":"3. Documentation Deployment (<code>.github/workflows/docs.yml</code>)","text":"<p>Automatically deploys documentation to GitHub Pages.</p>"},{"location":"CI_CD/#trigger-conditions","title":"Trigger Conditions","text":"<ul> <li>Changes to <code>docs/**</code></li> <li>Changes to <code>mkdocs.yml</code></li> <li>Changes to workflow file itself</li> <li>Manual dispatch</li> </ul>"},{"location":"CI_CD/#deployment","title":"Deployment","text":"<ul> <li>Builds documentation with MkDocs</li> <li>Deploys to <code>gh-pages</code> branch</li> <li>Serves at: <code>https://&lt;username&gt;.github.io/&lt;repo&gt;/</code></li> </ul>"},{"location":"CI_CD/#setup-instructions","title":"Setup Instructions","text":""},{"location":"CI_CD/#1-enable-github-actions","title":"1. Enable GitHub Actions","text":"<p>GitHub Actions should be enabled by default. Verify in repository Settings \u2192 Actions.</p>"},{"location":"CI_CD/#2-configure-github-pages","title":"2. Configure GitHub Pages","text":"<ol> <li>Go to repository Settings \u2192 Pages</li> <li>Source: Deploy from a branch</li> <li>Branch: <code>gh-pages</code> / <code>root</code></li> <li>Save</li> </ol>"},{"location":"CI_CD/#3-add-secrets-optional","title":"3. Add Secrets (Optional)","text":"<p>For coverage reports: <pre><code>Settings \u2192 Secrets \u2192 Actions \u2192 New repository secret\nName: CODECOV_TOKEN\nValue: &lt;your-codecov-token&gt;\n</code></pre></p>"},{"location":"CI_CD/#4-branch-protection-recommended","title":"4. Branch Protection (Recommended)","text":"<p>Protect <code>main</code> branch with required status checks:</p> <ol> <li>Settings \u2192 Branches \u2192 Add rule</li> <li>Branch name pattern: <code>main</code></li> <li>Require status checks before merging:</li> <li>\u2705 Lint and Format Check</li> <li>\u2705 Type Check</li> <li>\u2705 Test (3.11)</li> <li>\u2705 Test (3.12)</li> <li>\u2705 Build Documentation</li> <li>\u2705 All Checks Passed</li> <li>Require pull request reviews</li> <li>Save changes</li> </ol>"},{"location":"CI_CD/#local-development-workflow","title":"Local Development Workflow","text":""},{"location":"CI_CD/#pre-commit-checks","title":"Pre-commit Checks","text":"<p>Before committing, run the pre-commit script:</p> <pre><code>./scripts/pre-commit-check.sh\n</code></pre> <p>This runs the same checks as CI: 1. Code formatting check 2. Linting 3. Type checking 4. Documentation build</p>"},{"location":"CI_CD/#manual-ci-checks","title":"Manual CI Checks","text":"<p>Run individual checks locally:</p> <pre><code># Format check\nuv run ruff format --check .\n\n# Lint\nuv run ruff check .\n\n# Type check\nnpx pyright\n\n# Test\nuv run pytest tests/ -v\n\n# Build docs\nuv run mkdocs build --strict\n</code></pre>"},{"location":"CI_CD/#fix-issues","title":"Fix Issues","text":"<pre><code># Auto-fix formatting\nuv run ruff format .\n\n# Auto-fix linting issues\nuv run ruff check . --fix\n\n# View type errors (no auto-fix)\nnpx pyright\n</code></pre>"},{"location":"CI_CD/#cicd-best-practices","title":"CI/CD Best Practices","text":""},{"location":"CI_CD/#commit-messages","title":"Commit Messages","text":"<p>Use conventional commits format: <pre><code>feat: add new feature\nfix: resolve bug\ndocs: update documentation\ntest: add test coverage\nrefactor: improve code structure\nperf: performance improvement\nci: update CI/CD configuration\n</code></pre></p>"},{"location":"CI_CD/#pull-requests","title":"Pull Requests","text":"<ol> <li>Keep PRs focused and small (prefer size/S or size/M)</li> <li>Ensure all CI checks pass before requesting review</li> <li>Add tests for new features</li> <li>Update documentation for user-facing changes</li> <li>Respond to automated PR comments</li> </ol>"},{"location":"CI_CD/#testing","title":"Testing","text":"<ul> <li>Write tests for all new features</li> <li>Maintain &gt;80% test coverage</li> <li>Include both unit and integration tests</li> <li>Use fixtures for test data</li> </ul>"},{"location":"CI_CD/#type-safety","title":"Type Safety","text":"<ul> <li>Add type annotations to all functions</li> <li>Use <code>from __future__ import annotations</code> for forward references</li> <li>Run <code>npx pyright</code> before committing</li> <li>Fix type errors, don't suppress them unnecessarily</li> </ul>"},{"location":"CI_CD/#troubleshooting","title":"Troubleshooting","text":""},{"location":"CI_CD/#ci-fails-but-local-checks-pass","title":"CI Fails but Local Checks Pass","text":"<ol> <li>Ensure you're using the same Python version (3.11)</li> <li>Check that all dependencies are in <code>requirements.txt</code></li> <li>Clear local cache: <code>rm -rf .venv .ruff_cache .pytest_cache</code></li> <li>Reinstall: <code>uv venv --python 3.11 &amp;&amp; uv pip install -r requirements.txt</code></li> </ol>"},{"location":"CI_CD/#type-check-fails-on-ci","title":"Type Check Fails on CI","text":"<ul> <li>CI uses <code>npx pyright</code> (not global install)</li> <li>Ensure Node.js types are consistent</li> <li>Check for platform-specific type issues</li> </ul>"},{"location":"CI_CD/#tests-timeout","title":"Tests Timeout","text":"<ul> <li>CI has 6-hour timeout per workflow</li> <li>Individual jobs have 360-minute timeout</li> <li>Long-running tests should use appropriate fixtures</li> <li>Consider mocking external API calls</li> </ul>"},{"location":"CI_CD/#documentation-build-fails","title":"Documentation Build Fails","text":"<ul> <li>Run <code>uv run mkdocs build --strict</code> locally</li> <li>Check for broken links in markdown files</li> <li>Verify all referenced files exist</li> <li>Ensure proper YAML frontmatter in docs</li> </ul>"},{"location":"CI_CD/#monitoring","title":"Monitoring","text":""},{"location":"CI_CD/#view-workflow-status","title":"View Workflow Status","text":"<ul> <li>Repository \u2192 Actions tab</li> <li>Click on workflow run to see details</li> <li>Each job shows detailed logs</li> </ul>"},{"location":"CI_CD/#status-badges","title":"Status Badges","text":"<p>Add to README.md:</p> <pre><code>[![CI](https://github.com/username/repo/actions/workflows/ci.yml/badge.svg)](https://github.com/username/repo/actions/workflows/ci.yml)\n</code></pre>"},{"location":"CI_CD/#notifications","title":"Notifications","text":"<p>Configure in GitHub Settings \u2192 Notifications: - Email notifications for failed workflows - Slack/Discord integration (via webhooks)</p>"},{"location":"CI_CD/#future-improvements","title":"Future Improvements","text":""},{"location":"CI_CD/#planned-enhancements","title":"Planned Enhancements","text":"<ol> <li>Code Coverage Enforcement</li> <li>Fail CI if coverage drops below threshold</li> <li> <p>Per-file coverage reports</p> </li> <li> <p>Security Scanning</p> </li> <li>Dependency vulnerability scanning (Dependabot)</li> <li>Secret scanning</li> <li> <p>SAST (Static Application Security Testing)</p> </li> <li> <p>Performance Regression Testing</p> </li> <li>Benchmark tests in CI</li> <li> <p>Compare performance against main branch</p> </li> <li> <p>Automatic Dependency Updates</p> </li> <li>Renovate or Dependabot</li> <li> <p>Auto-merge minor updates if tests pass</p> </li> <li> <p>Release Automation</p> </li> <li>Automatic changelog generation</li> <li>Semantic versioning</li> <li>PyPI package publishing</li> </ol>"},{"location":"CI_CD/#see-also","title":"See Also","text":"<ul> <li>Pre-commit Script</li> <li>CLAUDE.md - Development guidelines</li> <li>GitHub Actions Documentation</li> </ul>"},{"location":"PERFORMANCE_ANALYSIS/","title":"Performance Analysis and Optimization","text":"<p>This document describes the performance analysis of the VLM OCR Pipeline and identifies optimization opportunities.</p>"},{"location":"PERFORMANCE_ANALYSIS/#performance-profiling-tools","title":"Performance Profiling Tools","text":"<p>The project includes two profiling tools:</p> <ol> <li>scripts/benchmark.py: Simple timing benchmark for overall pipeline performance</li> <li>scripts/profile_pipeline.py: Detailed cProfile-based profiling for function-level analysis</li> <li>pipeline/profiling.py: Performance metrics collection utilities with decorators</li> </ol>"},{"location":"PERFORMANCE_ANALYSIS/#usage","title":"Usage","text":"<pre><code># Simple benchmark (overall timing)\npython scripts/benchmark.py --input document.pdf --max-pages 1 --detector doclayout-yolo\n\n# Detailed profiling\npython scripts/profile_pipeline.py --input document.pdf --max-pages 1 --detailed\n\n# Save results to JSON\npython scripts/benchmark.py --input doc.pdf --max-pages 5 --output results.json\n</code></pre>"},{"location":"PERFORMANCE_ANALYSIS/#identified-performance-bottlenecks","title":"Identified Performance Bottlenecks","text":""},{"location":"PERFORMANCE_ANALYSIS/#1-pdf-rendering-high-impact","title":"1. PDF Rendering (High Impact)","text":"<p>Issue: <code>pdf2image.convert_from_path()</code> renders PDF pages to images, which is CPU and memory intensive.</p> <p>Location: <code>pipeline/conversion/input/pdf.py:render_pdf_page()</code></p> <p>Current Behavior: <pre><code>images = convert_from_path(pdf_path, first_page=page_num, last_page=page_num, dpi=200)\n</code></pre></p> <p>Optimization Opportunities: - Use PyMuPDF's <code>get_pixmap()</code> for faster rendering (2-3x speedup) - Reduce DPI for non-critical pages (trade quality for speed) - Implement parallel rendering for multi-page PDFs - Cache rendered images when processing the same PDF multiple times</p> <p>Estimated Impact: 30-50% reduction in Stage 1 (Input) time</p>"},{"location":"PERFORMANCE_ANALYSIS/#2-pymupdf-document-reopening-medium-impact","title":"2. PyMuPDF Document Reopening (Medium Impact)","text":"<p>Issue: PDF files may be opened multiple times during processing: - Once for page rendering - Again for text span extraction (if using font-based headers) - Potentially again for PyMuPDF sorter</p> <p>Location: <code>pipeline/conversion/input/pdf.py:extract_text_spans_from_pdf()</code></p> <p>Current Behavior: <pre><code>with open_pdf_document(pdf_path) as doc:\n    # Extract text spans for a single page\n</code></pre></p> <p>Optimization Opportunities: - Open PDF once and pass the document object to all functions - Cache document handle at Pipeline level - Batch extract text spans for all pages at once</p> <p>Estimated Impact: 10-20% reduction in Stage 1 time for multi-page PDFs</p>"},{"location":"PERFORMANCE_ANALYSIS/#3-list-operations-and-block-processing-low-medium-impact","title":"3. List Operations and Block Processing (Low-Medium Impact)","text":"<p>Issue: Multiple passes over block lists for different operations: - Detection - Sorting - Column layout extraction - Text extraction - Correction</p> <p>Location: Multiple locations in <code>pipeline/__init__.py</code></p> <p>Current Behavior: <pre><code># Multiple separate iterations\nsorted_blocks = self.ordering_stage.sort(blocks, ...)\ncolumn_layout = self.detection_stage.extract_column_layout(sorted_blocks)\nprocessed_blocks = self.recognition_stage.recognize_blocks(sorted_blocks, ...)\n</code></pre></p> <p>Optimization Opportunities: - Combine operations where possible - Use generator expressions for lazy evaluation - Pre-allocate result lists with known sizes - Use numpy arrays for block coordinates (vectorized operations)</p> <p>Estimated Impact: 5-10% reduction in overall processing time</p>"},{"location":"PERFORMANCE_ANALYSIS/#4-image-memory-management-medium-impact","title":"4. Image Memory Management (Medium Impact)","text":"<p>Issue: Large numpy arrays (images) are passed between functions, potentially causing unnecessary copies.</p> <p>Location: Throughout recognition and detection stages</p> <p>Current Behavior: <pre><code>block_image = block.bbox.crop(image, padding=5)\n# block_image passed to multiple functions\n</code></pre></p> <p>Optimization Opportunities: - \u2705 IMPLEMENTED: Context managers for automatic cleanup (pipeline/resources.py) - Use array views instead of copies where possible - Implement image pyramid for multi-scale processing - Compress images before API calls (if backend supports)</p> <p>Estimated Impact: 10-15% reduction in memory usage, 5-10% speed improvement</p> <p>Status: Context managers implemented in commit bd0167c</p>"},{"location":"PERFORMANCE_ANALYSIS/#5-vlm-api-calls-high-impact-limited-optimization","title":"5. VLM API Calls (High Impact, Limited Optimization)","text":"<p>Issue: Network latency and API processing time dominate Stage 4 (Recognition).</p> <p>Location: <code>pipeline/recognition/api/</code> modules</p> <p>Current Behavior: - Sequential API calls for each block - Rate limiting may add delays</p> <p>Optimization Opportunities: - \u2705 IMPLEMENTED: Caching system to avoid redundant API calls - Batch processing where API supports it (Gemini batch API) - Parallel processing with async/await (within rate limits) - Request compression for faster uploads - Use lower-tier models for simple text blocks</p> <p>Estimated Impact: 20-40% reduction in Stage 4 time (with caching and batching)</p> <p>Status: Caching implemented, batching opportunity remains</p>"},{"location":"PERFORMANCE_ANALYSIS/#6-unnecessary-gccollect-calls-low-impact","title":"6. Unnecessary gc.collect() Calls (Low Impact)","text":"<p>Issue: Manual <code>gc.collect()</code> calls may slow down execution unnecessarily.</p> <p>Location: Various locations (now centralized in context managers)</p> <p>Current Behavior: <pre><code>del large_object\ngc.collect()  # May pause execution\n</code></pre></p> <p>Optimization Opportunities: - \u2705 IMPLEMENTED: Remove manual gc.collect(), rely on automatic GC - Only force GC after processing each page (not each block)</p> <p>Estimated Impact: 2-5% speed improvement</p> <p>Status: Improved with context managers in commit bd0167c</p>"},{"location":"PERFORMANCE_ANALYSIS/#performance-optimization-roadmap","title":"Performance Optimization Roadmap","text":""},{"location":"PERFORMANCE_ANALYSIS/#phase-1-quick-wins-1-2-weeks","title":"Phase 1: Quick Wins (1-2 weeks)","text":"<ul> <li>[x] Implement context managers for resource cleanup</li> <li>[ ] Replace pdf2image with PyMuPDF for rendering</li> <li>[ ] Implement PDF document handle caching</li> <li>[ ] Remove unnecessary gc.collect() calls</li> </ul>"},{"location":"PERFORMANCE_ANALYSIS/#phase-2-architectural-improvements-2-4-weeks","title":"Phase 2: Architectural Improvements (2-4 weeks)","text":"<ul> <li>[ ] Implement async/parallel API calls with rate limiting</li> <li>[ ] Add batch processing support for VLM APIs</li> <li>[ ] Optimize block list operations (vectorization)</li> <li>[ ] Implement image compression for API calls</li> </ul>"},{"location":"PERFORMANCE_ANALYSIS/#phase-3-advanced-optimizations-1-2-months","title":"Phase 3: Advanced Optimizations (1-2 months)","text":"<ul> <li>[ ] Multi-page parallel processing</li> <li>[ ] GPU acceleration for image operations</li> <li>[ ] Custom CUDA kernels for block cropping</li> <li>[ ] Model quantization for faster inference</li> </ul>"},{"location":"PERFORMANCE_ANALYSIS/#benchmarking-results","title":"Benchmarking Results","text":""},{"location":"PERFORMANCE_ANALYSIS/#baseline-before-optimizations","title":"Baseline (Before Optimizations)","text":"Stage Time (s) % Total 1. Input (PDF\u2192Image) 2.50 25% 2. Detection 1.20 12% 3. Ordering 0.30 3% 4. Recognition (VLM) 5.00 50% 5. Block Correction 0.10 1% 6. Rendering 0.20 2% 7. Page Correction 0.50 5% 8. Output 0.20 2% Total 10.00 100% <p>Note: This is a hypothetical baseline. Run actual benchmarks with your data.</p>"},{"location":"PERFORMANCE_ANALYSIS/#after-memory-management-optimizations-bd0167c","title":"After Memory Management Optimizations (bd0167c)","text":"<ul> <li>Memory leaks eliminated</li> <li>5-10% speed improvement from reduced GC pressure</li> <li>More stable memory usage across long-running processes</li> </ul>"},{"location":"PERFORMANCE_ANALYSIS/#target-after-all-optimizations","title":"Target After All Optimizations","text":"Stage Target (s) Improvement 1. Input 1.25 50% 2. Detection 1.00 17% 3. Ordering 0.25 17% 4. Recognition 3.00 40% 5-8. Other 0.80 20% Total 6.30 37%"},{"location":"PERFORMANCE_ANALYSIS/#monitoring-performance","title":"Monitoring Performance","text":""},{"location":"PERFORMANCE_ANALYSIS/#enable-profiling-in-code","title":"Enable Profiling in Code","text":"<pre><code>from pipeline.profiling import get_metrics, measure_time\n\n# Enable metrics collection\nmetrics = get_metrics()\nmetrics.enable()\n\n# Use context manager for timing\nwith measure_time(\"my_operation\"):\n    # Your code here\n    pass\n\n# Print report\nmetrics.print_report()\n</code></pre>"},{"location":"PERFORMANCE_ANALYSIS/#decorator-based-profiling","title":"Decorator-Based Profiling","text":"<pre><code>from pipeline.profiling import timed\n\n@timed(\"expensive_function\")\ndef process_blocks(blocks):\n    # Function implementation\n    pass\n</code></pre>"},{"location":"PERFORMANCE_ANALYSIS/#see-also","title":"See Also","text":"<ul> <li>pipeline/profiling.py - Profiling utilities</li> <li>scripts/benchmark.py - Simple benchmark tool</li> <li>scripts/profile_pipeline.py - Detailed profiler</li> </ul>"},{"location":"api/conversion/","title":"Conversion API","text":"<p>PDF and image conversion utilities.</p> <p>API Reference</p> <p>Detailed API reference for conversion utilities will be added soon.</p> <p>For now, see source code: - <code>pipeline/conversion/input/pdf.py</code> - PDF conversion - <code>pipeline/conversion/output/markdown/</code> - Markdown generation</p>"},{"location":"api/detection/","title":"Detection API","text":"<p>Factory function and interface for layout detectors.</p>"},{"location":"api/detection/#factory-function","title":"Factory Function","text":"<pre><code>from pipeline.layout.detection import create_detector\n\ndetector = create_detector(\"doclayout-yolo\")\n</code></pre>"},{"location":"api/detection/#detector-protocol","title":"Detector Protocol","text":"<pre><code>from typing import Protocol\nimport numpy as np\nfrom pipeline.types import Block\n\nclass Detector(Protocol):\n    def detect(self, image: np.ndarray) -&gt; list[Block]:\n        \"\"\"Detect layout blocks in image.\"\"\"\n        ...\n</code></pre> <p>Full API Reference</p> <p>Detailed API reference coming soon. See Detectors for available detectors.</p>"},{"location":"api/ordering/","title":"Ordering API","text":"<p>Factory function and interface for reading order sorters.</p>"},{"location":"api/ordering/#factory-function","title":"Factory Function","text":"<pre><code>from pipeline.layout.ordering import create_sorter, validate_combination\n\nsorter = create_sorter(\"mineru-xycut\")\nvalidate_combination(\"doclayout-yolo\", \"mineru-xycut\")  # Check compatibility\n</code></pre>"},{"location":"api/ordering/#sorter-protocol","title":"Sorter Protocol","text":"<pre><code>from typing import Protocol\nimport numpy as np\nfrom pipeline.types import Block\n\nclass Sorter(Protocol):\n    def sort(self, blocks: list[Block], image: np.ndarray) -&gt; list[Block]:\n        \"\"\"Sort blocks in reading order.\"\"\"\n        ...\n</code></pre> <p>Full API Reference</p> <p>Detailed API reference coming soon. See Sorters for available sorters.</p>"},{"location":"api/pipeline/","title":"Pipeline API","text":"<p>Main pipeline class for document processing.</p>"},{"location":"api/pipeline/#overview","title":"Overview","text":"<p>The <code>Pipeline</code> class orchestrates the entire 8-stage document processing workflow.</p> <pre><code>from pathlib import Path\nfrom pipeline import Pipeline\n\n# Create pipeline\npipeline = Pipeline(\n    detector_name=\"doclayout-yolo\",\n    sorter_name=\"mineru-xycut\",\n    backend=\"gemini\",\n    model=\"gemini-2.5-flash\"\n)\n\n# Process PDF\nresult = pipeline.process_single_pdf(Path(\"document.pdf\"))\n</code></pre>"},{"location":"api/pipeline/#key-methods","title":"Key Methods","text":""},{"location":"api/pipeline/#__init__","title":"<code>__init__</code>","text":"<p>Initialize the pipeline with detector, sorter, and recognizer configurations.</p>"},{"location":"api/pipeline/#process_single_pdf","title":"<code>process_single_pdf</code>","text":"<p>Process a single PDF file and return results.</p>"},{"location":"api/pipeline/#process_directory","title":"<code>process_directory</code>","text":"<p>Batch process all PDFs in a directory.</p>"},{"location":"api/pipeline/#process_page","title":"<code>process_page</code>","text":"<p>Process a single page (used internally).</p> <p>Full API Reference</p> <p>Detailed API reference coming soon. See Advanced Examples for usage patterns.</p>"},{"location":"api/recognition/","title":"Recognition API","text":"<p>Text recognition and correction using VLMs.</p>"},{"location":"api/recognition/#textrecognizer","title":"TextRecognizer","text":"<pre><code>from pipeline.recognition import TextRecognizer\n\nrecognizer = TextRecognizer(\n    backend=\"gemini\",\n    model=\"gemini-2.5-flash\",\n    use_cache=True\n)\n\n# Process blocks\nprocessed_blocks = recognizer.process_blocks(image, blocks)\n\n# Correct text\ncorrected = recognizer.correct_text(raw_text)\n</code></pre> <p>Full API Reference</p> <p>Detailed API reference coming soon. See Recognizers for available recognizers.</p>"},{"location":"api/types/","title":"Types API","text":"<p>Core type definitions and data structures.</p>"},{"location":"api/types/#bbox","title":"BBox","text":"<p>Unified bounding box representation with automatic format conversion.</p> <pre><code>from pipeline.types import BBox\n\n# Create from xyxy coordinates\nbbox = BBox(x0=100, y0=50, x1=500, y1=200)\n\n# Convert from different formats\nbbox = BBox.from_yolo([0.5, 0.3, 0.4, 0.2], width=1000, height=800)\nbbox = BBox.from_mineru([100, 50, 400, 150])  # xywh\nbbox = BBox.from_pypdf_rect([100, 550, 500, 600], page_height=792)\n\n# Export\nxywh = bbox.to_xywh_list()  # [100, 50, 400, 150]\n</code></pre>"},{"location":"api/types/#block","title":"Block","text":"<p>Represents a detected layout block with metadata.</p> <pre><code>from pipeline.types import Block, BBox\n\nblock = Block(\n    type=\"text\",\n    bbox=BBox(100, 50, 500, 200),\n    detection_confidence=0.95,\n    order=0,\n    text=\"Extracted text\",\n    source=\"doclayout-yolo\"\n)\n</code></pre>"},{"location":"api/types/#protocols","title":"Protocols","text":""},{"location":"api/types/#detector","title":"Detector","text":"<pre><code>class Detector(Protocol):\n    def detect(self, image: np.ndarray) -&gt; list[Block]:\n        ...\n</code></pre>"},{"location":"api/types/#sorter","title":"Sorter","text":"<pre><code>class Sorter(Protocol):\n    def sort(self, blocks: list[Block], image: np.ndarray) -&gt; list[Block]:\n        ...\n</code></pre>"},{"location":"api/types/#recognizer","title":"Recognizer","text":"<pre><code>class Recognizer(Protocol):\n    def process_blocks(self, image: np.ndarray, blocks: Sequence[Block]) -&gt; list[Block]:\n        ...\n\n    def correct_text(self, text: str) -&gt; str | dict[str, Any]:\n        ...\n</code></pre> <p>Full API Reference</p> <p>Detailed API reference coming soon. See BBox Formats for conversion examples.</p>"},{"location":"architecture/detectors/","title":"Detectors","text":"<p>Available layout detection models and their characteristics.</p>"},{"location":"architecture/detectors/#supported-detectors","title":"Supported Detectors","text":"Detector Source Technology Speed Quality <code>doclayout-yolo</code> This project YOLO v8 \u26a1\u26a1\u26a1 \u2b50\u2b50\u2b50 <code>mineru-doclayout-yolo</code> MinerU YOLO v8 \u26a1\u26a1 \u2b50\u2b50\u2b50 <code>paddleocr-doclayout-v2</code> PaddleOCR PP-DocLayoutV2 \u26a1\u26a1 \u2b50\u2b50\u2b50\u2b50 <code>mineru-vlm</code> MinerU VLM-based \u26a1 \u2b50\u2b50\u2b50\u2b50 <code>olmocr-vlm</code> olmOCR VLM-based \u26a1 \u2b50\u2b50\u2b50\u2b50 <p>Note</p> <p>Detailed detector documentation coming soon. See Detector Block Types for block type mappings.</p>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>VLM OCR Pipeline is built on a modular, 8-stage architecture that separates concerns and allows flexible configuration of each processing step.</p>"},{"location":"architecture/overview/#core-design-principles","title":"Core Design Principles","text":""},{"location":"architecture/overview/#1-modular-stages","title":"1. Modular Stages","text":"<p>Each stage has a single responsibility and can be tested independently:</p> <pre><code># Each stage is a self-contained module\ninput_stage = InputStage(...)\ndetection_stage = DetectionStage(detector)\nordering_stage = OrderingStage(sorter)\nrecognition_stage = RecognitionStage(recognizer)\n</code></pre>"},{"location":"architecture/overview/#2-factory-pattern","title":"2. Factory Pattern","text":"<p>Detectors and sorters are created through factory functions:</p> <pre><code># Centralized creation with validation\ndetector = create_detector(\"doclayout-yolo\")\nsorter = create_sorter(\"mineru-xycut\")\nvalidate_combination(detector_name, sorter_name)  # Ensures compatibility\n</code></pre>"},{"location":"architecture/overview/#3-protocol-based-interfaces","title":"3. Protocol-Based Interfaces","text":"<p>Type-safe plugin system using Python protocols:</p> <pre><code>class Detector(Protocol):\n    \"\"\"All detectors must implement this interface.\"\"\"\n    def detect(self, image: np.ndarray) -&gt; list[Block]:\n        ...\n\nclass Sorter(Protocol):\n    \"\"\"All sorters must implement this interface.\"\"\"\n    def sort(self, blocks: list[Block], image: np.ndarray) -&gt; list[Block]:\n        ...\n</code></pre>"},{"location":"architecture/overview/#4-unified-bbox-system","title":"4. Unified BBox System","text":"<p>All bounding boxes use the <code>BBox</code> dataclass for automatic format conversion:</p> <pre><code>@dataclass\nclass BBox:\n    x0: int  # Left\n    y0: int  # Top\n    x1: int  # Right\n    y1: int  # Bottom\n\n    # Automatic conversion from 6+ formats\n    @classmethod\n    def from_yolo(cls, bbox, image_width, image_height) -&gt; BBox:\n        ...\n\n    @classmethod\n    def from_mineru(cls, bbox) -&gt; BBox:\n        ...\n</code></pre> <p>Internal: xyxy (corners) JSON Output: xywh (x, y, width, height)</p>"},{"location":"architecture/overview/#8-stage-pipeline","title":"8-Stage Pipeline","text":"<pre><code>graph TD\n    A[\ud83d\udcc4 Stage 1: Input] --&gt; B[\ud83d\udd0d Stage 2: Detection]\n    B --&gt; C[\ud83d\udcca Stage 3: Ordering]\n    C --&gt; D[\ud83d\udcdd Stage 4: Recognition]\n    D --&gt; E[\u270f\ufe0f Stage 5: Block Correction]\n    E --&gt; F[\ud83d\udccb Stage 6: Rendering]\n    F --&gt; G[\ud83d\udd27 Stage 7: Page Correction]\n    G --&gt; H[\ud83d\udcbe Stage 8: Output]\n\n    style A fill:#e1f5ff\n    style B fill:#fff3e1\n    style C fill:#e8f5e9\n    style D fill:#f3e5f5\n    style E fill:#fce4ec\n    style F fill:#fff9e1\n    style G fill:#e0f2f1\n    style H fill:#f1f8e9\n</code></pre>"},{"location":"architecture/overview/#stage-1-input","title":"Stage 1: Input","text":"<p>Responsibility: Load documents and extract auxiliary information</p> <ul> <li>Renders PDF pages to images (pdf2image)</li> <li>Loads image files directly (OpenCV)</li> <li>Extracts text spans with font metadata (PyMuPDF)</li> </ul> <p>Output: <code>(image: np.ndarray, auxiliary_info: dict)</code></p>"},{"location":"architecture/overview/#stage-2-detection","title":"Stage 2: Detection","text":"<p>Responsibility: Detect layout blocks</p> <ul> <li>Runs selected detector (DocLayout-YOLO, PaddleOCR, MinerU, olmOCR)</li> <li>Returns blocks with bounding boxes, types, and confidence scores</li> <li>Detects: text, title, table, figure, equation, list</li> </ul> <p>Output: <code>list[Block]</code> with <code>bbox</code>, <code>type</code>, <code>confidence</code></p>"},{"location":"architecture/overview/#stage-3-ordering","title":"Stage 3: Ordering","text":"<p>Responsibility: Analyze reading order</p> <ul> <li>Runs selected sorter (PyMuPDF, LayoutReader, XY-Cut, VLM)</li> <li>Adds <code>order</code> field to blocks</li> <li>Optionally adds <code>column_index</code> for multi-column documents</li> </ul> <p>Output: <code>list[Block]</code> sorted with <code>order</code> field</p>"},{"location":"architecture/overview/#stage-4-recognition","title":"Stage 4: Recognition","text":"<p>Responsibility: Extract text from blocks</p> <ul> <li>Crops block images from full page</li> <li>Sends to VLM API or local model</li> <li>Uses block-type-specific prompts</li> <li>Handles special content (tables, figures)</li> </ul> <p>Output: <code>list[Block]</code> with <code>text</code> field populated</p>"},{"location":"architecture/overview/#stage-5-block-correction","title":"Stage 5: Block Correction","text":"<p>Responsibility: Block-level text correction (placeholder)</p> <ul> <li>Currently disabled by default</li> <li>Future: VLM-based correction at block level</li> <li>Currently just copies <code>text</code> to <code>corrected_text</code></li> </ul> <p>Output: <code>list[Block]</code> with <code>corrected_text</code></p>"},{"location":"architecture/overview/#stage-6-rendering","title":"Stage 6: Rendering","text":"<p>Responsibility: Convert to output format</p> <ul> <li>Assembles blocks in reading order</li> <li>Generates Markdown or plaintext</li> <li>Uses auxiliary info for enhanced formatting</li> <li>Supports multiple rendering strategies</li> </ul> <p>Output: <code>str</code> (Markdown/plaintext)</p>"},{"location":"architecture/overview/#stage-7-page-correction","title":"Stage 7: Page Correction","text":"<p>Responsibility: Page-level VLM correction</p> <ul> <li>Sends entire page text to VLM</li> <li>Corrects OCR errors and formatting</li> <li>Calculates correction ratio</li> <li>Handles rate limits</li> <li>Skipped for local models</li> </ul> <p>Output: <code>(corrected_text: str, correction_ratio: float, should_stop: bool)</code></p>"},{"location":"architecture/overview/#stage-8-output","title":"Stage 8: Output","text":"<p>Responsibility: Save results</p> <ul> <li>Builds <code>Page</code> objects with metadata</li> <li>Saves JSON and Markdown files</li> <li>Generates document summaries</li> <li>Creates output directory structure</li> </ul> <p>Output: Saved files in <code>output/{model}/{document}/</code></p>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#block-evolution-through-stages","title":"Block Evolution Through Stages","text":"<p>A block's data evolves as it passes through stages:</p> <pre><code># After Detection (Stage 2)\nBlock(\n    type=\"text\",\n    bbox=BBox(100, 50, 500, 120),\n    detection_confidence=0.95,\n    source=\"doclayout-yolo\"\n)\n\n# After Ordering (Stage 3)\nBlock(\n    type=\"text\",\n    bbox=BBox(100, 50, 500, 120),\n    detection_confidence=0.95,\n    order=0,  # Added\n    column_index=0,  # Added (optional)\n    source=\"doclayout-yolo\"\n)\n\n# After Recognition (Stage 4)\nBlock(\n    type=\"text\",\n    bbox=BBox(100, 50, 500, 120),\n    detection_confidence=0.95,\n    order=0,\n    text=\"Chapter 1: Introduction\",  # Added\n    source=\"doclayout-yolo\"\n)\n\n# After Block Correction (Stage 5)\nBlock(\n    type=\"text\",\n    bbox=BBox(100, 50, 500, 120),\n    detection_confidence=0.95,\n    order=0,\n    text=\"Chapter 1: Introduction\",\n    corrected_text=\"Chapter 1: Introduction\",  # Added\n    source=\"doclayout-yolo\"\n)\n</code></pre>"},{"location":"architecture/overview/#extensibility","title":"Extensibility","text":""},{"location":"architecture/overview/#adding-a-new-detector","title":"Adding a New Detector","text":"<ol> <li>Implement the <code>Detector</code> protocol</li> <li>Register in <code>create_detector()</code> factory</li> <li>Add to <code>validate_combination()</code> if needed</li> </ol> <pre><code># pipeline/layout/detection/my_detector.py\nclass MyDetector:\n    def detect(self, image: np.ndarray) -&gt; list[Block]:\n        # Your detection logic\n        return blocks\n\n# pipeline/layout/detection/__init__.py\ndef create_detector(name: str, **kwargs) -&gt; Detector:\n    if name == \"my-detector\":\n        return MyDetector(**kwargs)\n</code></pre>"},{"location":"architecture/overview/#adding-a-new-sorter","title":"Adding a New Sorter","text":"<ol> <li>Implement the <code>Sorter</code> protocol</li> <li>Register in <code>create_sorter()</code> factory</li> <li>Add combination validation</li> </ol> <pre><code># pipeline/layout/ordering/my_sorter.py\nclass MySorter:\n    def sort(self, blocks: list[Block], image: np.ndarray) -&gt; list[Block]:\n        # Your sorting logic\n        return sorted_blocks\n\n# pipeline/layout/ordering/__init__.py\ndef create_sorter(name: str, **kwargs) -&gt; Sorter:\n    if name == \"my-sorter\":\n        return MySorter(**kwargs)\n</code></pre>"},{"location":"architecture/overview/#adding-a-new-recognizer","title":"Adding a New Recognizer","text":"<ol> <li>Implement the <code>Recognizer</code> protocol</li> <li>Add to TextRecognizer backend selection</li> </ol> <pre><code>class Recognizer(Protocol):\n    def process_blocks(self, image: np.ndarray, blocks: Sequence[Block]) -&gt; list[Block]:\n        ...\n\n    def correct_text(self, text: str) -&gt; str | dict[str, Any]:\n        ...\n</code></pre>"},{"location":"architecture/overview/#error-handling","title":"Error Handling","text":"<p>The pipeline uses a comprehensive error handling system:</p> <ul> <li>Custom exceptions: Specific exception types for different errors</li> <li>Graceful degradation: Continue processing on non-critical failures</li> <li>Error logging: Detailed logs with stack traces</li> <li>Rate limit handling: Automatic retry and backoff</li> </ul> <p>See Error Handling Guide for details.</p>"},{"location":"architecture/overview/#testing-strategy","title":"Testing Strategy","text":"<p>Each component can be tested independently:</p> <pre><code># Test detector\ndetector = create_detector(\"doclayout-yolo\")\nblocks = detector.detect(test_image)\nassert len(blocks) &gt; 0\n\n# Test sorter\nsorter = create_sorter(\"mineru-xycut\")\nsorted_blocks = sorter.sort(blocks, test_image)\nassert sorted_blocks[0].order is not None\n\n# Test full pipeline\npipeline = Pipeline(detector=\"doclayout-yolo\", sorter=\"mineru-xycut\")\nresult = pipeline.process_single_pdf(test_pdf)\n</code></pre>"},{"location":"architecture/overview/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/overview/#caching","title":"Caching","text":"<p>The recognition stage uses content-based caching:</p> <pre><code># Cache key = hash(block_image + block_type + prompt)\ncache_key = hashlib.sha256(\n    block_image.tobytes() +\n    block_type.encode() +\n    prompt.encode()\n).hexdigest()\n</code></pre>"},{"location":"architecture/overview/#rate-limiting","title":"Rate Limiting","text":"<p>Gemini API rate limiting is handled globally:</p> <pre><code>rate_limiter.wait_if_needed(estimated_tokens=1000)\n# Automatic throttling based on tier limits\n</code></pre>"},{"location":"architecture/overview/#memory-management","title":"Memory Management","text":"<ul> <li>Block images are deleted after recognition</li> <li>Garbage collection is triggered after each block</li> <li>Temporary files are cleaned up automatically</li> </ul>"},{"location":"architecture/overview/#configuration","title":"Configuration","text":"<p>Pipeline behavior is controlled through:</p> <ol> <li>CLI Arguments: Runtime configuration</li> <li>Environment Variables: API keys, paths</li> <li>YAML Files: Prompts, rate limits</li> <li>Factory Functions: Component selection</li> </ol>"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Pipeline Stages - Detailed stage documentation</li> <li>Detectors - Available detection models</li> <li>Sorters - Reading order algorithms</li> <li>Recognizers - Text extraction backends</li> </ul>"},{"location":"architecture/pipeline-stages/","title":"Pipeline Stages","text":"<p>Detailed documentation for each of the 8 pipeline stages.</p> <p>For a high-level overview, see Architecture Overview.</p>"},{"location":"architecture/pipeline-stages/#stage-documentation","title":"Stage Documentation","text":"<ul> <li>Stage 1: Input - Document loading</li> <li>Stage 2: Detection - Layout detection</li> <li>Stage 3: Ordering - Reading order</li> <li>Stage 4: Recognition - Text extraction</li> <li>Stage 5: Block Correction - Block-level correction</li> <li>Stage 6: Rendering - Format conversion</li> <li>Stage 7: Page Correction - Page-level correction</li> <li>Stage 8: Output - Result saving</li> </ul> <p>Note</p> <p>Detailed implementation documentation coming soon. See Architecture Overview for current documentation.</p>"},{"location":"architecture/recognizers/","title":"Recognizers","text":"<p>Text recognition backends for extracting text from detected blocks.</p>"},{"location":"architecture/recognizers/#supported-recognizers","title":"Supported Recognizers","text":""},{"location":"architecture/recognizers/#cloud-vlm-apis","title":"Cloud VLM APIs","text":"<ul> <li>OpenAI: GPT-4 Vision, GPT-4o</li> <li>Gemini: Gemini 2.5 Flash, Gemini 2.0 Flash</li> <li>OpenRouter: Access to multiple VLMs</li> </ul>"},{"location":"architecture/recognizers/#local-models","title":"Local Models","text":"<ul> <li>PaddleOCR-VL: PaddleOCR-VL-0.9B (NaViT + ERNIE-4.5-0.3B)</li> <li>0.9B parameters</li> <li>109 languages support</li> <li>No API costs</li> <li>Requires GPU (recommended)</li> </ul> <p>Note</p> <p>Detailed recognizer API documentation coming soon. See Basic Usage for usage examples.</p>"},{"location":"architecture/sorters/","title":"Sorters","text":"<p>Reading order analysis algorithms for document layout.</p>"},{"location":"architecture/sorters/#supported-sorters","title":"Supported Sorters","text":"Sorter Algorithm Multi-Column Speed Quality <code>pymupdf</code> Font analysis \u2705 \u26a1\u26a1\u26a1 \u2b50\u2b50\u2b50 <code>mineru-xycut</code> XY-Cut \u274c \u26a1\u26a1\u26a1 \u2b50\u2b50 <code>mineru-layoutreader</code> LayoutLMv3 \u2705 \u26a1\u26a1 \u2b50\u2b50\u2b50\u2b50 <code>mineru-vlm</code> VLM reasoning \u2705 \u26a1 \u2b50\u2b50\u2b50\u2b50 <code>olmocr-vlm</code> VLM reasoning \u2705 \u26a1 \u2b50\u2b50\u2b50\u2b50 <code>paddleocr-doclayout-v2</code> Pointer network \u2705 \u26a1\u26a1 \u2b50\u2b50\u2b50\u2b50 <p>Note</p> <p>Detailed sorter documentation coming soon. See Architecture Overview for more information.</p>"},{"location":"getting-started/basic-usage/","title":"Basic Usage","text":"<p>Learn the core features and command-line options of VLM OCR Pipeline.</p>"},{"location":"getting-started/basic-usage/#command-line-interface","title":"Command-Line Interface","text":"<p>The main entry point is <code>main.py</code>, which provides a comprehensive CLI.</p>"},{"location":"getting-started/basic-usage/#basic-syntax","title":"Basic Syntax","text":"<pre><code>python main.py [OPTIONS]\n</code></pre>"},{"location":"getting-started/basic-usage/#input-options","title":"Input Options","text":""},{"location":"getting-started/basic-usage/#single-file","title":"Single File","text":"<pre><code># Process a PDF\npython main.py --input document.pdf --backend gemini\n\n# Process an image\npython main.py --input photo.jpg --backend gemini\n</code></pre>"},{"location":"getting-started/basic-usage/#batch-processing","title":"Batch Processing","text":"<pre><code># Process all PDFs in a directory\npython main.py --input documents/ --backend gemini\n</code></pre>"},{"location":"getting-started/basic-usage/#page-limiting","title":"Page Limiting","text":"<p>Control which pages to process:</p> Max PagesPage RangeSpecific Pages <p>Process first N pages only: <pre><code>python main.py --input doc.pdf --backend gemini --max-pages 5\n</code></pre></p> <p>Process a specific range (inclusive): <pre><code>python main.py --input doc.pdf --backend gemini --page-range 10-20\n</code></pre></p> <p>Process selected pages: <pre><code>python main.py --input doc.pdf --backend gemini --pages 1,5,10,15\n</code></pre></p>"},{"location":"getting-started/basic-usage/#backend-selection","title":"Backend Selection","text":""},{"location":"getting-started/basic-usage/#cloud-vlm-apis","title":"Cloud VLM APIs","text":"GeminiOpenAIOpenRouter <p>Google's Gemini API (free tier available): <pre><code>export GEMINI_API_KEY=\"your_key\"\npython main.py --input doc.pdf --backend gemini --model gemini-2.5-flash\n</code></pre></p> <p>Tier Options: <code>free</code>, <code>tier1</code>, <code>tier2</code>, <code>tier3</code> <pre><code>python main.py --input doc.pdf --backend gemini --gemini-tier free\n</code></pre></p> <p>OpenAI's GPT-4 Vision: <pre><code>export OPENAI_API_KEY=\"your_key\"\npython main.py --input doc.pdf --backend openai --model gpt-4o\n</code></pre></p> <p>Access multiple VLMs through OpenRouter: <pre><code>export OPENROUTER_API_KEY=\"your_key\"\npython main.py --input doc.pdf --backend openai --model google/gemini-2.5-flash\n</code></pre></p>"},{"location":"getting-started/basic-usage/#local-recognition","title":"Local Recognition","text":"<p>PaddleOCR-VL (no API required): <pre><code>python main.py --input doc.pdf --recognizer paddleocr-vl\n</code></pre></p>"},{"location":"getting-started/basic-usage/#detector-selection","title":"Detector Selection","text":"<p>Choose the layout detection model:</p> Detector Source Speed Quality Use Case <code>doclayout-yolo</code> This project \u26a1\u26a1\u26a1 \u2b50\u2b50\u2b50 Default, fast <code>mineru-doclayout-yolo</code> MinerU \u26a1\u26a1 \u2b50\u2b50\u2b50 MinerU pipeline <code>paddleocr-doclayout-v2</code> PaddleOCR \u26a1\u26a1 \u2b50\u2b50\u2b50\u2b50 High quality <code>mineru-vlm</code> MinerU \u26a1 \u2b50\u2b50\u2b50\u2b50 VLM-based <code>olmocr-vlm</code> olmOCR \u26a1 \u2b50\u2b50\u2b50\u2b50 VLM-based <p>Examples:</p> <pre><code># Default detector (doclayout-yolo)\npython main.py --input doc.pdf --backend gemini\n\n# High-quality detector\npython main.py --input doc.pdf --detector paddleocr-doclayout-v2 --backend gemini\n\n# VLM-based detection\npython main.py --input doc.pdf --detector mineru-vlm --backend gemini\n</code></pre>"},{"location":"getting-started/basic-usage/#sorter-selection","title":"Sorter Selection","text":"<p>Choose the reading order algorithm:</p> Sorter Algorithm Speed Multi-Column Use Case <code>pymupdf</code> Font analysis \u26a1\u26a1\u26a1 \u2705 Multi-column docs <code>mineru-xycut</code> XY-Cut \u26a1\u26a1\u26a1 \u274c Simple layouts <code>mineru-layoutreader</code> LayoutLMv3 \u26a1\u26a1 \u2705 Complex layouts <code>mineru-vlm</code> VLM reasoning \u26a1 \u2705 Very complex <code>olmocr-vlm</code> VLM reasoning \u26a1 \u2705 Research papers <code>paddleocr-doclayout-v2</code> Pointer network \u26a1\u26a1 \u2705 With PP-DocLayoutV2 <p>Examples:</p> <pre><code># Multi-column documents\npython main.py --input doc.pdf --sorter pymupdf --backend gemini\n\n# Complex academic papers\npython main.py --input paper.pdf --sorter mineru-layoutreader --backend gemini\n\n# VLM-based ordering\npython main.py --input doc.pdf --sorter olmocr-vlm --backend gemini\n</code></pre>"},{"location":"getting-started/basic-usage/#detector-sorter-combinations","title":"Detector + Sorter Combinations","text":"<p>Not all combinations are valid. The pipeline validates compatibility:</p>"},{"location":"getting-started/basic-usage/#recommended-combinations","title":"Recommended Combinations","text":"<pre><code># Fast general-purpose\npython main.py --input doc.pdf \\\n    --detector doclayout-yolo \\\n    --sorter mineru-xycut \\\n    --backend gemini\n\n# High quality multi-column\npython main.py --input doc.pdf \\\n    --detector paddleocr-doclayout-v2 \\\n    --sorter pymupdf \\\n    --backend gemini\n\n# Maximum quality (slower)\npython main.py --input doc.pdf \\\n    --detector mineru-vlm \\\n    --sorter mineru-vlm \\\n    --backend gemini\n\n# Full PaddleOCR pipeline (local)\npython main.py --input doc.pdf \\\n    --detector paddleocr-doclayout-v2 \\\n    --recognizer paddleocr-vl\n</code></pre> <p>Invalid Combinations</p> <ul> <li><code>paddleocr-doclayout-v2</code> detector auto-selects its sorter (cannot override)</li> <li>VLM detectors (<code>mineru-vlm</code>, <code>olmocr-vlm</code>) require matching VLM sorters</li> </ul>"},{"location":"getting-started/basic-usage/#output-options","title":"Output Options","text":""},{"location":"getting-started/basic-usage/#output-directory","title":"Output Directory","text":"<pre><code># Custom output directory\npython main.py --input doc.pdf --backend gemini --output results/\n</code></pre> <p>Default: <code>output/{model}/{filename}/</code></p> <p>Example: <code>output/gemini-2.5-flash/document/page_1.json</code></p>"},{"location":"getting-started/basic-usage/#cache-control","title":"Cache Control","text":"<pre><code># Disable caching\npython main.py --input doc.pdf --backend gemini --no-cache\n\n# Custom cache directory\npython main.py --input doc.pdf --backend gemini --cache-dir .my-cache/\n</code></pre>"},{"location":"getting-started/basic-usage/#rate-limiting-gemini","title":"Rate Limiting (Gemini)","text":""},{"location":"getting-started/basic-usage/#check-status","title":"Check Status","text":"<pre><code>python main.py --rate-limit-status --backend gemini --gemini-tier free\n</code></pre> <p>Output: <pre><code>=== Gemini API Rate Limit Status ===\nTier: free\nModel: gemini-2.5-flash\n\nCurrent Limits:\n  RPM (Requests Per Minute): 2 / 15 (13.3%)\n  TPM (Tokens Per Minute): 45,234 / 1,500,000 (3.0%)\n  RPD (Requests Per Day): 156 / 1,500 (10.4%)\n</code></pre></p>"},{"location":"getting-started/basic-usage/#tier-configuration","title":"Tier Configuration","text":"<pre><code># Free tier (default)\npython main.py --input doc.pdf --backend gemini --gemini-tier free\n\n# Paid tiers (higher limits)\npython main.py --input doc.pdf --backend gemini --gemini-tier tier1\npython main.py --input doc.pdf --backend gemini --gemini-tier tier2\n</code></pre>"},{"location":"getting-started/basic-usage/#advanced-options","title":"Advanced Options","text":""},{"location":"getting-started/basic-usage/#dpi-settings","title":"DPI Settings","text":"<p>For PDF rendering quality:</p> <pre><code># Higher DPI = better quality, larger images\npython main.py --input doc.pdf --backend gemini --dpi 300  # Default: 200\n</code></pre>"},{"location":"getting-started/basic-usage/#temporary-files","title":"Temporary Files","text":"<pre><code># Custom temp directory\npython main.py --input doc.pdf --backend gemini --temp-dir /tmp/ocr/\n</code></pre>"},{"location":"getting-started/basic-usage/#logging","title":"Logging","text":"<pre><code># Verbose output\npython main.py --input doc.pdf --backend gemini -v\n\n# Very verbose (debug level)\npython main.py --input doc.pdf --backend gemini -vv\n</code></pre>"},{"location":"getting-started/basic-usage/#common-workflows","title":"Common Workflows","text":""},{"location":"getting-started/basic-usage/#academic-papers","title":"Academic Papers","text":"<pre><code># High-quality processing for research papers\npython main.py --input paper.pdf \\\n    --detector paddleocr-doclayout-v2 \\\n    --sorter mineru-layoutreader \\\n    --backend gemini \\\n    --dpi 300\n</code></pre>"},{"location":"getting-started/basic-usage/#multi-column-magazines","title":"Multi-Column Magazines","text":"<pre><code># Multi-column layout detection\npython main.py --input magazine.pdf \\\n    --detector doclayout-yolo \\\n    --sorter pymupdf \\\n    --backend gemini\n</code></pre>"},{"location":"getting-started/basic-usage/#large-batch-processing","title":"Large Batch Processing","text":"<pre><code># Process many PDFs with local model\npython main.py --input documents/ \\\n    --detector paddleocr-doclayout-v2 \\\n    --recognizer paddleocr-vl \\\n    --max-pages 10  # Limit for testing\n</code></pre>"},{"location":"getting-started/basic-usage/#cost-optimized-processing","title":"Cost-Optimized Processing","text":"<pre><code># Use Gemini free tier + caching\npython main.py --input doc.pdf \\\n    --backend gemini \\\n    --gemini-tier free \\\n    --cache-dir .cache/\n</code></pre>"},{"location":"getting-started/basic-usage/#output-structure","title":"Output Structure","text":"<p>After processing, you'll find:</p> <pre><code>output/\n\u2514\u2500\u2500 {model}/              # e.g., gemini-2.5-flash/\n    \u2514\u2500\u2500 {document}/       # e.g., research_paper/\n        \u251c\u2500\u2500 page_1.json   # Detailed page data\n        \u251c\u2500\u2500 page_1.md     # Markdown output\n        \u251c\u2500\u2500 page_2.json\n        \u251c\u2500\u2500 page_2.md\n        \u2514\u2500\u2500 {document}_summary.json  # Processing metadata\n</code></pre>"},{"location":"getting-started/basic-usage/#json-structure","title":"JSON Structure","text":"<pre><code>{\n  \"page_num\": 1,\n  \"image_size\": [1650, 2200],\n  \"text\": \"# Title\\n\\nBody text...\",\n  \"corrected_text\": \"# Title\\n\\nBody text...\",\n  \"correction_ratio\": 0.02,\n  \"processing_stopped\": false,\n  \"blocks\": [\n    {\n      \"type\": \"title\",\n      \"bbox\": [100, 50, 500, 120],\n      \"detection_confidence\": 0.95,\n      \"order\": 0,\n      \"column_index\": null,\n      \"text\": \"Title\",\n      \"corrected_text\": \"Title\",\n      \"source\": \"doclayout-yolo\"\n    }\n  ],\n  \"auxiliary_info\": {\n    \"text_spans\": [...]  # Font metadata for markdown\n  }\n}\n</code></pre>"},{"location":"getting-started/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture Overview - Understand the pipeline</li> <li>Advanced Examples - Complex use cases</li> <li>API Reference - Programmatic usage</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you set up VLM OCR Pipeline on your system.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+: Recommended for best compatibility</li> <li>uv: Fast Python package manager (optional but recommended)</li> <li>Git: For cloning the repository and managing submodules</li> </ul>"},{"location":"getting-started/installation/#quick-installation","title":"Quick Installation","text":""},{"location":"getting-started/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/NoUnique/vlm-ocr-pipeline.git\ncd vlm-ocr-pipeline\n</code></pre>"},{"location":"getting-started/installation/#2-set-up-python-environment","title":"2. Set Up Python Environment","text":"Using uv (Recommended)Using pip <pre><code># Install uv if you haven't\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create virtual environment\nuv venv --python 3.11 .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies\nuv pip install -r requirements.txt\n</code></pre> <pre><code># Create virtual environment\npython3.11 -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#3-fix-doclayout-yolo-compatibility","title":"3. Fix DocLayout-YOLO Compatibility","text":"<pre><code>python setup.py\n</code></pre> <p>What does setup.py do?</p> <p>The <code>setup.py</code> script fixes compatibility issues between DocLayout-YOLO and the ultralytics package by modifying the YOLO model files.</p>"},{"location":"getting-started/installation/#api-configuration","title":"API Configuration","text":"<p>Choose the backend you want to use and configure the corresponding API keys.</p>"},{"location":"getting-started/installation/#gemini-api-recommended-for-free-tier","title":"Gemini API (Recommended for Free Tier)","text":"<ol> <li>Visit Google AI Studio</li> <li>Sign in with your Google account</li> <li>Click \"Create API Key\"</li> <li>Copy the generated key</li> <li>Set environment variable:</li> </ol> <pre><code>export GEMINI_API_KEY=\"your_api_key_here\"\n</code></pre> <p>Free Tier Limits (as of 2024): - 15 requests per minute (RPM) - 1,500,000 tokens per minute (TPM) - 1,500 requests per day (RPD)</p>"},{"location":"getting-started/installation/#openai-api","title":"OpenAI API","text":"<ol> <li>Visit OpenAI Platform</li> <li>Create an API key</li> <li>Set environment variable:</li> </ol> <pre><code>export OPENAI_API_KEY=\"your_api_key_here\"\n</code></pre>"},{"location":"getting-started/installation/#openrouter-api-alternative","title":"OpenRouter API (Alternative)","text":"<p>OpenRouter provides access to multiple VLMs through a single API:</p> <pre><code>export OPENROUTER_API_KEY=\"your_api_key_here\"\n</code></pre>"},{"location":"getting-started/installation/#optional-components","title":"Optional Components","text":""},{"location":"getting-started/installation/#paddleocr-vl-local-recognition","title":"PaddleOCR-VL (Local Recognition)","text":"<p>For local text recognition without API calls:</p> <pre><code># PaddleX is already included as a submodule\ncd external/PaddleX\ngit checkout v3.3.1\npip install -e .\n</code></pre> <p>Requirements: - GPU with CUDA support (recommended) - ~4GB VRAM for PaddleOCR-VL-0.9B</p>"},{"location":"getting-started/installation/#external-frameworks-git-submodules","title":"External Frameworks (Git Submodules)","text":"<p>The project includes several external frameworks as submodules:</p> <pre><code># Initialize all submodules\ngit submodule update --init --recursive\n\n# Or initialize specific submodules\ngit submodule update --init external/MinerU      # MinerU detectors/sorters\ngit submodule update --init external/olmocr      # olmOCR VLM sorter\ngit submodule update --init external/PaddleOCR   # PP-DocLayoutV2 detector\ngit submodule update --init external/PaddleX     # PaddleOCR-VL recognizer\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Verify your installation:</p> <pre><code># Check Python version\npython --version  # Should be 3.11+\n\n# Check dependencies\npython -c \"import torch; print(f'PyTorch: {torch.__version__}')\"\npython -c \"import cv2; print(f'OpenCV: {cv2.__version__}')\"\n\n# Run a simple test\npython main.py --help\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#cudagpu-issues","title":"CUDA/GPU Issues","text":"<p>If you encounter CUDA-related errors:</p> <pre><code># Check CUDA availability\npython -c \"import torch; print(torch.cuda.is_available())\"\n\n# Install CPU-only PyTorch (if no GPU)\npip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n</code></pre>"},{"location":"getting-started/installation/#ultralyticsyolo-errors","title":"Ultralytics/YOLO Errors","text":"<p>If you see errors about <code>ultralytics</code> or YOLO models:</p> <pre><code># Re-run the setup script\npython setup.py\n\n# Or manually reinstall ultralytics\npip uninstall ultralytics\npip install ultralytics==8.2.0\n</code></pre>"},{"location":"getting-started/installation/#missing-dependencies","title":"Missing Dependencies","text":"<p>If you encounter import errors:</p> <pre><code># Reinstall all dependencies\npip install -r requirements.txt --force-reinstall\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that you have VLM OCR Pipeline installed:</p> <ul> <li>Quick Start Guide - Run your first OCR pipeline</li> <li>Basic Usage - Learn the core features</li> <li>Architecture Overview - Understand how it works</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with VLM OCR Pipeline in 5 minutes!</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+ installed</li> <li>Gemini API key (free tier available)</li> </ul>"},{"location":"getting-started/quickstart/#step-1-install","title":"Step 1: Install","text":"<pre><code># Clone the repository\ngit clone https://github.com/NoUnique/vlm-ocr-pipeline.git\ncd vlm-ocr-pipeline\n\n# Set up environment\nuv venv --python 3.11 .venv\nsource .venv/bin/activate\n\n# Install dependencies\nuv pip install -r requirements.txt\n\n# Fix YOLO compatibility\npython setup.py\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-configure-api","title":"Step 2: Configure API","text":"<pre><code># Set Gemini API key\nexport GEMINI_API_KEY=\"your_api_key_here\"\n</code></pre> <p>Get a Free Gemini API Key</p> <p>Visit Google AI Studio to get a free API key.</p>"},{"location":"getting-started/quickstart/#step-3-run-your-first-pipeline","title":"Step 3: Run Your First Pipeline","text":"<pre><code># Process a single PDF\npython main.py --input document.pdf --backend gemini\n</code></pre> <p>That's it! The pipeline will:</p> <ol> <li>\ud83d\udcc4 Load your PDF and render each page as an image</li> <li>\ud83d\udd0d Detect layout blocks (text, tables, figures, etc.)</li> <li>\ud83d\udcca Analyze reading order</li> <li>\ud83d\udcdd Extract text using Gemini Vision</li> <li>\ud83d\udd27 Correct and improve text quality</li> <li>\ud83d\udcbe Save results to <code>output/gemini-2.5-flash/document/</code></li> </ol>"},{"location":"getting-started/quickstart/#understanding-the-output","title":"Understanding the Output","text":"<p>After processing, you'll find:</p> <pre><code>output/\n\u2514\u2500\u2500 gemini-2.5-flash/\n    \u2514\u2500\u2500 document/\n        \u251c\u2500\u2500 page_1.json          # Detailed page data\n        \u251c\u2500\u2500 page_1.md            # Markdown output\n        \u2514\u2500\u2500 document_summary.json  # Processing metadata\n</code></pre>"},{"location":"getting-started/quickstart/#example-output","title":"Example Output","text":"<p>page_1.md: <pre><code># Introduction\n\nThis document describes...\n\n## Table of Contents\n\n1. Getting Started\n2. Advanced Features\n3. API Reference\n</code></pre></p> <p>page_1.json: <pre><code>{\n  \"page_num\": 1,\n  \"text\": \"# Introduction\\n\\nThis document describes...\",\n  \"corrected_text\": \"# Introduction\\n\\nThis document describes...\",\n  \"correction_ratio\": 0.05,\n  \"blocks\": [\n    {\n      \"type\": \"title\",\n      \"bbox\": [100, 50, 500, 120],\n      \"text\": \"Introduction\",\n      \"order\": 0\n    }\n  ]\n}\n</code></pre></p>"},{"location":"getting-started/quickstart/#common-use-cases","title":"Common Use Cases","text":""},{"location":"getting-started/quickstart/#single-image","title":"Single Image","text":"<pre><code>python main.py --input photo.jpg --backend gemini\n</code></pre>"},{"location":"getting-started/quickstart/#batch-processing","title":"Batch Processing","text":"<pre><code># Process all PDFs in a directory\npython main.py --input documents/ --backend gemini\n</code></pre>"},{"location":"getting-started/quickstart/#limit-pages-for-testing","title":"Limit Pages (for testing)","text":"<pre><code># Process only first 5 pages\npython main.py --input document.pdf --backend gemini --max-pages 5\n\n# Process specific page range\npython main.py --input document.pdf --backend gemini --page-range 10-20\n\n# Process specific pages\npython main.py --input document.pdf --backend gemini --pages 1,5,10\n</code></pre>"},{"location":"getting-started/quickstart/#use-openai-instead","title":"Use OpenAI Instead","text":"<pre><code># Set OpenAI API key\nexport OPENAI_API_KEY=\"your_api_key_here\"\n\n# Run with OpenAI backend\npython main.py --input document.pdf --backend openai --model gpt-4o\n</code></pre>"},{"location":"getting-started/quickstart/#local-processing-no-api","title":"Local Processing (No API)","text":"<pre><code># Use PaddleOCR-VL (local model, no API calls)\npython main.py --input document.pdf \\\n    --detector paddleocr-doclayout-v2 \\\n    --recognizer paddleocr-vl\n</code></pre>"},{"location":"getting-started/quickstart/#rate-limiting","title":"Rate Limiting","text":"<p>The pipeline automatically handles rate limits for Gemini API:</p> <pre><code># Check current rate limit status\npython main.py --rate-limit-status --backend gemini --gemini-tier free\n</code></pre> <p>Free Tier Limits: - 15 requests per minute - 1,500,000 tokens per minute - 1,500 requests per day</p> <p>The pipeline will automatically wait when limits are reached.</p>"},{"location":"getting-started/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quickstart/#gemini_api_key-not-set","title":"\"GEMINI_API_KEY not set\"","text":"<pre><code>export GEMINI_API_KEY=\"your_api_key_here\"\n</code></pre>"},{"location":"getting-started/quickstart/#rate-limit-exceeded","title":"\"Rate limit exceeded\"","text":"<p>The pipeline will automatically wait. Alternatively:</p> <pre><code># Use OpenAI instead\npython main.py --input doc.pdf --backend openai\n\n# Or use local model (no API)\npython main.py --input doc.pdf --recognizer paddleocr-vl\n</code></pre>"},{"location":"getting-started/quickstart/#cuda-out-of-memory","title":"\"CUDA out of memory\"","text":"<p>If using PaddleOCR-VL locally:</p> <pre><code># Reduce batch size or use CPU\nexport CUDA_VISIBLE_DEVICES=\"\"  # Force CPU mode\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you've run your first pipeline:</p> <ul> <li>Basic Usage Guide - Learn about all available options</li> <li>Architecture Overview - Understand how the pipeline works</li> <li>Advanced Examples - Complex use cases and customizations</li> </ul>"},{"location":"getting-started/quickstart/#tips-for-best-results","title":"Tips for Best Results","text":"<p>Optimize Processing</p> <ul> <li>Use <code>--max-pages</code> to test on a few pages first</li> <li>Check rate limit status regularly for Gemini</li> <li>Use caching to avoid reprocessing identical content</li> </ul> <p>API Costs</p> <ul> <li>Gemini free tier has daily limits</li> <li>OpenAI charges per token</li> <li>Consider using PaddleOCR-VL for large batch processing</li> </ul> <p>Performance</p> <ul> <li>DocLayout-YOLO is fastest for detection</li> <li>PaddleOCR-VL provides good quality without API costs</li> <li>Gemini 2.5 Flash is fast and cost-effective</li> </ul>"},{"location":"guides/advanced-examples/","title":"Advanced Examples","text":"<p>Complex use cases and advanced configurations for VLM OCR Pipeline.</p>"},{"location":"guides/advanced-examples/#multi-column-academic-papers","title":"Multi-Column Academic Papers","text":"<p>Process research papers with complex multi-column layouts:</p> <pre><code>python main.py --input paper.pdf \\\n    --detector paddleocr-doclayout-v2 \\\n    --sorter pymupdf \\\n    --backend gemini \\\n    --dpi 300 \\\n    --output results/papers/\n</code></pre> <p>Why this configuration?</p> <ul> <li>PP-DocLayoutV2: High-quality detector for academic layouts</li> <li>PyMuPDF sorter: Best for multi-column detection</li> <li>DPI 300: Higher quality for small text</li> <li>Gemini: Cost-effective with good accuracy</li> </ul>"},{"location":"guides/advanced-examples/#large-batch-processing","title":"Large Batch Processing","text":"<p>Process hundreds of PDFs efficiently:</p> <pre><code># Use local model to avoid API costs\npython main.py --input documents/ \\\n    --detector paddleocr-doclayout-v2 \\\n    --recognizer paddleocr-vl \\\n    --cache-dir .cache/ \\\n    --output batch-results/\n</code></pre> <p>Best practices:</p> <ul> <li>Use PaddleOCR-VL to avoid API costs</li> <li>Enable caching to skip duplicates</li> <li>Process in batches if memory constrained</li> <li>Monitor disk space for large outputs</li> </ul>"},{"location":"guides/advanced-examples/#mixed-content-documents","title":"Mixed Content Documents","text":"<p>Documents with tables, figures, and equations:</p> <pre><code>python main.py --input textbook.pdf \\\n    --detector mineru-vlm \\\n    --sorter mineru-vlm \\\n    --backend openai \\\n    --model gpt-4o \\\n    --output textbooks/\n</code></pre> <p>Why VLM detection?</p> <ul> <li>Better understanding of complex layouts</li> <li>Accurate table structure detection</li> <li>Equation recognition</li> <li>Figure caption association</li> </ul>"},{"location":"guides/advanced-examples/#cost-optimized-processing","title":"Cost-Optimized Processing","text":"<p>Minimize API costs while maintaining quality:</p> <pre><code># Use Gemini free tier with caching\npython main.py --input doc.pdf \\\n    --detector doclayout-yolo \\\n    --sorter mineru-xycut \\\n    --backend gemini \\\n    --gemini-tier free \\\n    --cache-dir .cache/ \\\n    --max-pages 10  # Test first\n\n# Check rate limits\npython main.py --rate-limit-status --backend gemini --gemini-tier free\n</code></pre> <p>Tips:</p> <ol> <li>Test on few pages first (<code>--max-pages</code>)</li> <li>Use caching to avoid reprocessing</li> <li>Monitor free tier limits</li> <li>Switch to local model if limits reached</li> </ol>"},{"location":"guides/advanced-examples/#programmatic-usage","title":"Programmatic Usage","text":"<p>Use the pipeline in your Python code:</p> <pre><code>from pathlib import Path\nfrom pipeline import Pipeline\n\n# Initialize pipeline\npipeline = Pipeline(\n    detector_name=\"doclayout-yolo\",\n    sorter_name=\"mineru-xycut\",\n    backend=\"gemini\",\n    model=\"gemini-2.5-flash\",\n    cache_dir=Path(\".cache\"),\n    output_dir=Path(\"output\"),\n    use_cache=True\n)\n\n# Process single PDF\nresult = pipeline.process_single_pdf(\n    pdf_path=Path(\"document.pdf\"),\n    max_pages=5\n)\n\n# Access results\nfor page in result.pages:\n    print(f\"Page {page.page_num}\")\n    print(f\"Text: {page.corrected_text}\")\n    print(f\"Blocks: {len(page.blocks)}\")\n    print(f\"Correction ratio: {page.correction_ratio}\")\n</code></pre>"},{"location":"guides/advanced-examples/#custom-detector-integration","title":"Custom Detector Integration","text":"<p>Integrate your own detector:</p> <pre><code># my_detector.py\nimport numpy as np\nfrom pipeline.types import Block, BBox\n\nclass CustomDetector:\n    \"\"\"Custom layout detector.\"\"\"\n\n    def detect(self, image: np.ndarray) -&gt; list[Block]:\n        \"\"\"Detect layout blocks.\"\"\"\n        # Your detection logic here\n        blocks = []\n\n        # Example: detect blocks\n        for detection in your_model.predict(image):\n            block = Block(\n                type=detection.label,  # \"text\", \"table\", etc.\n                bbox=BBox(\n                    x0=int(detection.box[0]),\n                    y0=int(detection.box[1]),\n                    x1=int(detection.box[2]),\n                    y1=int(detection.box[3])\n                ),\n                detection_confidence=float(detection.confidence),\n                source=\"custom-detector\"\n            )\n            blocks.append(block)\n\n        return blocks\n\n# Register in factory\n# pipeline/layout/detection/__init__.py\ndef create_detector(name: str, **kwargs) -&gt; Detector:\n    if name == \"custom\":\n        from .custom_detector import CustomDetector\n        return CustomDetector(**kwargs)\n    # ...\n\n# Use it\npython main.py --input doc.pdf --detector custom --backend gemini\n</code></pre>"},{"location":"guides/advanced-examples/#custom-prompts","title":"Custom Prompts","text":"<p>Override default prompts for specific use cases:</p> <pre><code># settings/prompts/gemini/custom_text_extraction.yaml\nsystem: |\n  You are a specialized OCR system for medical documents.\n  Pay special attention to:\n  - Drug names and dosages\n  - Medical terminology\n  - Patient information\n  - Dates and times\n\nuser: |\n  Extract text from this medical document image.\n  Preserve all formatting, especially:\n  - Tables with patient data\n  - Lists of medications\n  - Diagnostic results\n\n  Output in Markdown format.\n\nfallback: |\n  [OCR failed - manual review required]\n</code></pre> <p>Then use <code>PromptManager</code> in code:</p> <pre><code>from pipeline.prompt import PromptManager\n\npm = PromptManager(model=\"gemini-2.5-flash\")\ncustom_prompt = pm.get_prompt(\"custom_text_extraction\", \"user\")\n</code></pre>"},{"location":"guides/advanced-examples/#multi-language-documents","title":"Multi-Language Documents","text":"<p>Process documents in multiple languages:</p> <pre><code># PaddleOCR-VL supports 109 languages\npython main.py --input multilang.pdf \\\n    --detector paddleocr-doclayout-v2 \\\n    --recognizer paddleocr-vl \\\n    --output multilang-results/\n</code></pre> <p>Supported languages (PaddleOCR-VL):</p> <ul> <li>European: English, Spanish, French, German, Italian, Portuguese, etc.</li> <li>Asian: Chinese (Simplified/Traditional), Japanese, Korean, Thai, Vietnamese</li> <li>Middle Eastern: Arabic, Hebrew, Persian</li> <li>And 100+ more</li> </ul>"},{"location":"guides/advanced-examples/#rate-limit-monitoring","title":"Rate Limit Monitoring","text":"<p>Monitor and adapt to rate limits in real-time:</p> <pre><code>from pipeline.recognition.api.ratelimit import rate_limiter\n\n# Check status before processing\nstatus = rate_limiter.get_status()\nprint(f\"Current RPM: {status['current']['rpm']} / {status['limits']['rpm']}\")\nprint(f\"RPD: {status['current']['rpd']} / {status['limits']['rpd']}\")\n\n# Process with automatic throttling\nif rate_limiter.wait_if_needed(estimated_tokens=1000):\n    # Process page\n    result = pipeline.process_page(image, page_num=1)\nelse:\n    print(\"Daily limit exceeded\")\n</code></pre>"},{"location":"guides/advanced-examples/#selective-page-processing","title":"Selective Page Processing","text":"<p>Process specific pages of interest:</p> <pre><code># Process only pages with tables\npython main.py --input report.pdf \\\n    --pages 5,12,18,25 \\\n    --detector paddleocr-doclayout-v2 \\\n    --backend gemini\n\n# Process chapter intros (every 10 pages)\npython main.py --input book.pdf \\\n    --pages 1,11,21,31,41,51 \\\n    --backend gemini\n</code></pre>"},{"location":"guides/advanced-examples/#error-recovery","title":"Error Recovery","text":"<p>Handle processing errors gracefully:</p> <pre><code>from pipeline import Pipeline\nfrom pipeline.exceptions import ProcessingError, APIError\n\npipeline = Pipeline(backend=\"gemini\")\n\ntry:\n    result = pipeline.process_single_pdf(\"document.pdf\")\nexcept APIError as e:\n    print(f\"API error: {e}\")\n    # Retry with different backend\n    pipeline.backend = \"openai\"\n    result = pipeline.process_single_pdf(\"document.pdf\")\nexcept ProcessingError as e:\n    print(f\"Processing error: {e}\")\n    # Continue with next document\n    pass\n</code></pre>"},{"location":"guides/advanced-examples/#performance-benchmarking","title":"Performance Benchmarking","text":"<p>Compare different configurations:</p> <pre><code>import time\nfrom pathlib import Path\n\nconfigurations = [\n    (\"doclayout-yolo\", \"mineru-xycut\", \"gemini\"),\n    (\"paddleocr-doclayout-v2\", \"pymupdf\", \"gemini\"),\n    (\"mineru-vlm\", \"mineru-vlm\", \"openai\"),\n]\n\ntest_pdf = Path(\"test.pdf\")\nresults = {}\n\nfor detector, sorter, backend in configurations:\n    pipeline = Pipeline(\n        detector_name=detector,\n        sorter_name=sorter,\n        backend=backend\n    )\n\n    start = time.time()\n    result = pipeline.process_single_pdf(test_pdf, max_pages=3)\n    elapsed = time.time() - start\n\n    results[f\"{detector}/{sorter}/{backend}\"] = {\n        \"time\": elapsed,\n        \"pages\": len(result.pages),\n        \"avg_correction_ratio\": sum(p.correction_ratio for p in result.pages) / len(result.pages)\n    }\n\n# Print comparison\nfor config, metrics in results.items():\n    print(f\"{config}:\")\n    print(f\"  Time: {metrics['time']:.2f}s\")\n    print(f\"  Avg correction: {metrics['avg_correction_ratio']:.2%}\")\n</code></pre>"},{"location":"guides/advanced-examples/#integration-with-external-tools","title":"Integration with External Tools","text":""},{"location":"guides/advanced-examples/#export-to-different-formats","title":"Export to Different Formats","text":"<pre><code>from pipeline import Pipeline\nimport json\n\npipeline = Pipeline(backend=\"gemini\")\nresult = pipeline.process_single_pdf(\"document.pdf\")\n\n# Export to JSON\nwith open(\"output.json\", \"w\") as f:\n    json.dump([page.to_dict() for page in result.pages], f, indent=2)\n\n# Export to plain text\nwith open(\"output.txt\", \"w\") as f:\n    for page in result.pages:\n        f.write(f\"=== Page {page.page_num} ===\\n\")\n        f.write(page.corrected_text)\n        f.write(\"\\n\\n\")\n</code></pre>"},{"location":"guides/advanced-examples/#post-processing","title":"Post-Processing","text":"<pre><code>def post_process_markdown(text: str) -&gt; str:\n    \"\"\"Custom post-processing for markdown output.\"\"\"\n    # Remove excessive newlines\n    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n\n    # Fix common OCR errors\n    text = text.replace(\" ,\", \",\")\n    text = text.replace(\" .\", \".\")\n\n    # Normalize quotes\n    text = text.replace(\"\"\", '\"').replace(\"\"\", '\"')\n\n    return text\n\n# Apply to results\nfor page in result.pages:\n    page.corrected_text = post_process_markdown(page.corrected_text)\n</code></pre>"},{"location":"guides/advanced-examples/#best-practices-summary","title":"Best Practices Summary","text":"<p>Performance</p> <ul> <li>Use DocLayout-YOLO for speed</li> <li>Use XY-Cut sorter for simple layouts</li> <li>Enable caching for repeated processing</li> <li>Test on few pages before full batch</li> </ul> <p>Quality</p> <ul> <li>Use PP-DocLayoutV2 for complex layouts</li> <li>Use PyMuPDF for multi-column documents</li> <li>Use VLM detectors for mixed content</li> <li>Increase DPI for small text</li> </ul> <p>Cost Optimization</p> <ul> <li>Use Gemini free tier when possible</li> <li>Cache recognition results</li> <li>Use PaddleOCR-VL for large batches</li> <li>Monitor rate limits</li> </ul> <p>Common Issues</p> <ul> <li>Check page height for PyPDF conversions</li> <li>Validate detector/sorter combinations</li> <li>Handle rate limits gracefully</li> <li>Clean up temporary files</li> </ul>"},{"location":"guides/advanced-examples/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Detailed API documentation</li> <li>Architecture Overview - System design</li> <li>Contributing Guide - Contribute your own examples</li> </ul>"},{"location":"guides/bbox-formats/","title":"BBox Format Reference","text":""},{"location":"guides/bbox-formats/#coordinate-systems","title":"Coordinate Systems","text":"<p>All frameworks use Top-Left origin (0,0) except PyPDF.</p>"},{"location":"guides/bbox-formats/#format-comparison","title":"Format Comparison","text":"Framework Format Example Notes This Project (Internal) <code>BBox(x0, y0, x1, y1)</code> <code>BBox(100, 50, 300, 200)</code> Integer coordinates, xyxy corners This Project (JSON) <code>[x, y, w, h]</code> <code>[100, 50, 200, 150]</code> Position + Size (human-readable) YOLO <code>[x1, y1, x2, y2]</code> <code>[100, 50, 300, 200]</code> Top-Left + Bottom-Right MinerU <code>[x0, y0, x1, y1]</code> <code>[100, 50, 300, 200]</code> Top-Left + Bottom-Right PyMuPDF <code>Rect(x0, y0, x1, y1)</code> <code>Rect(100, 50, 300, 200)</code> Top-Left + Bottom-Right PyPDF \u26a0\ufe0f <code>[x0, y0, x1, y1]</code> <code>[100, 592, 300, 742]</code> Bottom-Left origin olmOCR <code>\"[x, y]text\"</code> <code>\"[100x50]Chapter 1\"</code> Text format"},{"location":"guides/bbox-formats/#visual-example","title":"Visual Example","text":"<p>Same rectangle at visual position (100, 50) with size 200\u00d7150 on page height 792:</p> <pre><code>Computer Vision (Top-Left origin):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 (0,0)           \u2502\n\u2502                 \u2502\n\u2502   [100,50]      \u2502 \u2190 Region here\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502   \u2502        \u2502    \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502          [300,200]\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nPDF (Bottom-Left origin):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          [300,742] \u2190 Y is flipped!\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502   \u2502        \u2502    \u2502\n\u2502   [100,592]     \u2502 \u2190 Same region\n\u2502                 \u2502\n\u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  (0,0)\n</code></pre>"},{"location":"guides/bbox-formats/#format-conversions","title":"Format Conversions","text":""},{"location":"guides/bbox-formats/#this-project-mineruyolo","title":"This Project \u2194 MinerU/YOLO","text":"<pre><code># [x, y, w, h] \u2192 [x0, y0, x1, y1]\nx0, y0, x1, y1 = x, y, x + w, y + h\n\n# [x0, y0, x1, y1] \u2192 [x, y, w, h]\nx, y, w, h = x0, y0, x1 - x0, y1 - y0\n</code></pre>"},{"location":"guides/bbox-formats/#pypdf-this-project-y-axis-flip-required","title":"PyPDF \u2194 This Project (Y-axis flip required!)","text":"<pre><code># PyPDF \u2192 This Project\ny_top = page_height - y1_pypdf       # 792 - 742 = 50\ny_bottom = page_height - y0_pypdf    # 792 - 592 = 200\n\n# This Project \u2192 PyPDF\ny0_pypdf = page_height - y_bottom    # 792 - 200 = 592\ny1_pypdf = page_height - y_top       # 792 - 50 = 742\n</code></pre>"},{"location":"guides/bbox-formats/#olmocr-anchor-format","title":"olmOCR Anchor Format","text":"<pre><code># Text region\nanchor = f\"[{x:.0f}x{y:.0f}]{text_content}\"\n# Example: \"[100x50]Chapter 1\"\n\n# Image/Figure region\nanchor = f\"[Image {x0:.0f}x{y0:.0f} to {x1:.0f}x{y1:.0f}]\"\n# Example: \"[Image 100x50 to 300x200]\"\n\n# Table region\nanchor = f\"[Table {x0:.0f}x{y0:.0f} to {x1:.0f}x{y1:.0f}]\"\n# Example: \"[Table 100x450 to 500x600]\"\n</code></pre>"},{"location":"guides/bbox-formats/#bbox-class-usage","title":"BBox Class Usage","text":"<p>Our <code>BBox</code> class handles all conversions automatically with integer coordinates:</p> <pre><code>from pipeline.types import BBox\n\n# Create from any format (accepts float, converts to int)\nbbox = BBox.from_xywh(100, 50, 200, 150)      # xywh format\nbbox = BBox.from_xyxy(100, 50, 300, 200)      # xyxy format (corners)\nbbox = BBox.from_cxcywh(200, 125, 200, 150)   # Center format (YOLO training)\nbbox = BBox.from_mineru_bbox([100, 50, 300, 200])\nbbox = BBox.from_pymupdf_rect(rect)\nbbox = BBox.from_pypdf_rect([100, 592, 300, 742], page_height=792)\n\n# Convert to any format\ncoords = bbox.to_xywh_list()          # [100, 50, 200, 150] (for JSON)\ncoords = bbox.to_list()                # [100, 50, 300, 200] (xyxy)\ncoords = bbox.to_dict()                # {\"x0\": 100, \"y0\": 50, \"x1\": 300, \"y1\": 200}\ncx, cy, w, h = bbox.to_cxcywh()        # Center format\ncoords = bbox.to_mineru_bbox()         # [100, 50, 300, 200]\ncoords = bbox.to_pypdf_rect(792)       # [100, 592, 300, 742]\nanchor = bbox.to_olmocr_anchor(\"image\") # \"[Image 100x50 to 300x200]\"\n\n# Geometric operations (integer results)\ncenter_x, center_y = bbox.center  # (float, float)\narea = bbox.area                  # int\nwidth = bbox.width                # int\nheight = bbox.height              # int\noverlap = bbox1.intersect(bbox2)  # int\niou = bbox1.iou(bbox2)            # float\n\n# Clear aliases\nleft = bbox.left     # = bbox.x0\ntop = bbox.top       # = bbox.y0\nright = bbox.right   # = bbox.x1\nbottom = bbox.bottom # = bbox.y1\n\n# NumPy convenience\ncropped = bbox.crop(image, padding=5)  # Direct image cropping\n</code></pre>"},{"location":"guides/bbox-formats/#region-structure","title":"Region Structure","text":"<p>The <code>Region</code> dataclass represents detected document regions:</p> <pre><code>from pipeline.types import Block, BBox\n\nblock = Block(\n    type=\"text\",\n    bbox=BBox(100, 50, 300, 200),  # Required, always present\n    detection_confidence=0.95,\n    # Optional fields\n    order=0,\n    column_index=1,\n    text=\"Extracted text...\",\n)\n\n# Serialize to JSON (bbox \u2192 xywh format for readability)\ndata = block.to_dict()\n# {\"order\": 0, \"type\": \"text\", \"xywh\": [100, 50, 200, 150], \"detection_confidence\": 0.95, ...}\n\n# Deserialize from JSON (supports xywh)\nblock = Block.from_dict(data)\n</code></pre>"},{"location":"guides/bbox-formats/#key-points","title":"Key Points","text":"<ol> <li>Internal representation: <code>BBox(x0, y0, x1, y1)</code> with integer coordinates (xyxy format)</li> <li>JSON output: <code>[x, y, w, h]</code> (xywh format) for human readability</li> <li>Most frameworks use the same internal format: <code>[x0, y0, x1, y1]</code> with Top-Left origin</li> <li>Only PyPDF is different: Bottom-Left origin requires Y-axis flip</li> <li>Region.bbox is always present: No longer optional (required field)</li> </ol>"},{"location":"guides/contributing/","title":"Contributing Guide","text":"<p>Thank you for considering contributing to VLM OCR Pipeline! This guide will help you get started.</p>"},{"location":"guides/contributing/#development-setup","title":"Development Setup","text":""},{"location":"guides/contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":"<pre><code># Fork the repository on GitHub, then clone your fork\ngit clone https://github.com/YOUR_USERNAME/vlm-ocr-pipeline.git\ncd vlm-ocr-pipeline\n</code></pre>"},{"location":"guides/contributing/#2-set-up-development-environment","title":"2. Set Up Development Environment","text":"<pre><code># Create virtual environment\nuv venv --python 3.11 .venv\nsource .venv/bin/activate\n\n# Install dependencies\nuv pip install -r requirements.txt\n\n# Install development dependencies\nuv pip install pytest pytest-cov ruff pyright\n\n# Run setup script\npython setup.py\n</code></pre>"},{"location":"guides/contributing/#3-create-a-branch","title":"3. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/your-bugfix-name\n</code></pre>"},{"location":"guides/contributing/#code-quality-standards","title":"Code Quality Standards","text":""},{"location":"guides/contributing/#type-annotations","title":"Type Annotations","text":"<p>All functions and methods must have type annotations:</p> <pre><code>def process_blocks(\n    self,\n    image: np.ndarray,\n    blocks: Sequence[Block]\n) -&gt; list[Block]:\n    \"\"\"Process blocks to extract text.\"\"\"\n    ...\n</code></pre>"},{"location":"guides/contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings for all public functions and classes:</p> <pre><code>def detect_layout(image: np.ndarray, confidence_threshold: float = 0.5) -&gt; list[Block]:\n    \"\"\"Detect layout blocks in an image.\n\n    Args:\n        image: Input image as numpy array (H, W, C)\n        confidence_threshold: Minimum confidence score for detection\n\n    Returns:\n        List of detected blocks with bounding boxes\n\n    Raises:\n        DetectionError: If detection fails\n\n    Example:\n        &gt;&gt;&gt; detector = DocLayoutYOLO()\n        &gt;&gt;&gt; blocks = detector.detect(image, confidence_threshold=0.7)\n        &gt;&gt;&gt; len(blocks)\n        15\n    \"\"\"\n</code></pre>"},{"location":"guides/contributing/#code-style","title":"Code Style","text":"<p>We use ruff for linting and formatting:</p> <pre><code># Format code\nuv run ruff format .\n\n# Check linting\nuv run ruff check .\n\n# Auto-fix linting issues\nuv run ruff check . --fix\n</code></pre> <p>Configuration (ruff.toml): - Line length: 120 characters - Import order: isort compatible - First-party modules: <code>[\"pipeline\", \"models\"]</code></p>"},{"location":"guides/contributing/#type-checking","title":"Type Checking","text":"<p>We use pyright for static type checking:</p> <pre><code># Type check entire project\nnpx pyright\n\n# Type check specific files\nnpx pyright pipeline/__init__.py\n</code></pre> <p>Note: Use <code>npx pyright</code>, not global <code>pyright</code></p>"},{"location":"guides/contributing/#testing","title":"Testing","text":""},{"location":"guides/contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run specific test file\nuv run pytest tests/test_types.py\n\n# Run with coverage\nuv run pytest --cov=pipeline --cov-report=term-missing\n\n# Run verbose\nuv run pytest -v\n</code></pre>"},{"location":"guides/contributing/#writing-tests","title":"Writing Tests","text":"<p>Place tests in <code>tests/</code> directory with naming convention <code>test_*.py</code>:</p> <pre><code>def test_bbox_from_yolo():\n    \"\"\"Test BBox conversion from YOLO format.\"\"\"\n    bbox = BBox.from_yolo([0.5, 0.5, 0.3, 0.4], 1000, 800)\n    assert bbox.x0 == 350\n    assert bbox.y0 == 240\n    assert bbox.x1 == 650\n    assert bbox.y1 == 640\n</code></pre> <p>Coverage Goal: 90%+ for new code</p>"},{"location":"guides/contributing/#test-fixtures","title":"Test Fixtures","text":"<p>Use fixtures in <code>tests/fixtures/</code>: - Sample images - Sample PDFs - Expected outputs</p>"},{"location":"guides/contributing/#bbox-handling-rules","title":"BBox Handling Rules","text":"<p>Critical: BBox Standards</p> <ol> <li>Always use BBox class - Never use raw lists/tuples</li> <li>Internal operations use xyxy - Access via <code>bbox.x0, bbox.y0, bbox.x1, bbox.y1</code></li> <li>JSON serialization uses xywh - Call <code>bbox.to_xywh_list()</code></li> <li>Accept floats, output integers - All BBox methods round to nearest integer</li> <li>PyPDF requires page height - Use <code>BBox.from_pypdf_rect(rect, page_height)</code></li> </ol>"},{"location":"guides/contributing/#adding-new-components","title":"Adding New Components","text":""},{"location":"guides/contributing/#adding-a-detector","title":"Adding a Detector","text":"<ol> <li>Create detector file in <code>pipeline/layout/detection/</code></li> <li>Implement <code>Detector</code> protocol from <code>pipeline/types.py</code></li> <li>Register in <code>create_detector()</code> factory</li> <li>Add validation rule in <code>validate_combination()</code> if needed</li> <li>Write tests in <code>tests/test_detectors.py</code></li> </ol> <p>Example:</p> <pre><code># pipeline/layout/detection/my_detector.py\nfrom pipeline.types import Block, Detector\n\nclass MyDetector:\n    \"\"\"My custom detector implementation.\"\"\"\n\n    def detect(self, image: np.ndarray) -&gt; list[Block]:\n        \"\"\"Detect layout blocks.\n\n        Args:\n            image: Input image (H, W, C)\n\n        Returns:\n            List of detected blocks\n        \"\"\"\n        # Your detection logic\n        return blocks\n\n# pipeline/layout/detection/__init__.py\ndef create_detector(name: str, **kwargs) -&gt; Detector:\n    if name == \"my-detector\":\n        from .my_detector import MyDetector  # noqa: PLC0415\n        return MyDetector(**kwargs)\n    ...\n</code></pre>"},{"location":"guides/contributing/#adding-a-sorter","title":"Adding a Sorter","text":"<p>Similar process in <code>pipeline/layout/ordering/</code>:</p> <pre><code>from pipeline.types import Block, Sorter\n\nclass MySorter:\n    \"\"\"My custom sorter implementation.\"\"\"\n\n    def sort(self, blocks: list[Block], image: np.ndarray, **kwargs) -&gt; list[Block]:\n        \"\"\"Sort blocks in reading order.\n\n        Args:\n            blocks: Detected blocks\n            image: Original image for context\n\n        Returns:\n            Sorted blocks with order field\n        \"\"\"\n        # Your sorting logic\n        return sorted_blocks\n</code></pre>"},{"location":"guides/contributing/#adding-prompts","title":"Adding Prompts","text":"<p>Place YAML prompts in <code>settings/prompts/{model}/</code>:</p> <pre><code># settings/prompts/my-model/text_extraction.yaml\nsystem: |\n  You are an expert OCR system.\n\nuser: |\n  Extract text from this image.\n  Preserve formatting and structure.\n\nfallback: |\n  [OCR failed]\n</code></pre>"},{"location":"guides/contributing/#error-handling","title":"Error Handling","text":"<p>Follow the error handling policy (see Error Handling Guide):</p>"},{"location":"guides/contributing/#custom-exceptions","title":"Custom Exceptions","text":"<p>Use specific exception types from <code>pipeline/exceptions.py</code>:</p> <pre><code>from pipeline.exceptions import DetectionError, InvalidConfigError\n\nif confidence &lt; 0 or confidence &gt; 1:\n    raise InvalidConfigError(f\"Confidence must be between 0 and 1, got {confidence}\")\n\ntry:\n    blocks = self.detector.detect(image)\nexcept Exception as e:\n    raise DetectionError(f\"Detection failed: {e}\") from e\n</code></pre>"},{"location":"guides/contributing/#error-logging","title":"Error Logging","text":"<p>Use proper logging with <code>%s</code> formatting (not f-strings):</p> <pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\n# \u2705 Good\nlogger.error(\"Failed to load file %s: %s\", file_path, error)\n\n# \u274c Bad\nlogger.error(f\"Failed to load file {file_path}: {error}\")\n</code></pre> <p>Add <code>exc_info=True</code> for unexpected errors:</p> <pre><code>except Exception as e:\n    logger.error(\"Unexpected error: %s\", e, exc_info=True)\n</code></pre>"},{"location":"guides/contributing/#commit-guidelines","title":"Commit Guidelines","text":""},{"location":"guides/contributing/#commit-messages","title":"Commit Messages","text":"<p>Follow conventional commits:</p> <pre><code>feat: add new detector for layout analysis\nfix: resolve type error in BBox conversion\ndocs: update installation guide\ntest: add tests for multi-column detection\nrefactor: simplify block sorting logic\nperf: optimize image preprocessing\n</code></pre>"},{"location":"guides/contributing/#before-committing","title":"Before Committing","text":"<pre><code># Format code\nuv run ruff format .\n\n# Check linting\nuv run ruff check .\n\n# Run tests\nuv run pytest\n\n# Type check\nnpx pyright\n</code></pre>"},{"location":"guides/contributing/#creating-a-pull-request","title":"Creating a Pull Request","text":"<ol> <li>Ensure all tests pass</li> <li>Update documentation if needed</li> <li>Add entry to CHANGELOG (if exists)</li> <li>Create PR with clear description:</li> </ol> <pre><code>## Summary\n\nBrief description of changes\n\n## Changes\n\n- Added feature X\n- Fixed bug Y\n- Updated documentation Z\n\n## Testing\n\n- Tested on Python 3.11\n- All existing tests pass\n- Added new tests for feature X\n\n## Breaking Changes\n\nNone (or list if applicable)\n</code></pre>"},{"location":"guides/contributing/#common-pitfalls","title":"Common Pitfalls","text":"<p>Avoid These Mistakes</p> <ul> <li>Don't use bare except: Catch specific exceptions</li> <li>Don't create empty <code>__init__.py</code>: Use PEP 420 namespace packages</li> <li>Don't install with <code>-e</code>: Never use editable mode from external directories</li> <li>Don't mix xywh/xyxy: Always convert via BBox methods</li> <li>Don't forget page_height for PyPDF: Y-axis flip required</li> </ul>"},{"location":"guides/contributing/#documentation","title":"Documentation","text":""},{"location":"guides/contributing/#building-docs-locally","title":"Building Docs Locally","text":"<pre><code># Install MkDocs\nuv pip install mkdocs mkdocs-material mkdocstrings[python]\n\n# Serve docs locally\nmkdocs serve\n\n# Build docs\nmkdocs build\n</code></pre>"},{"location":"guides/contributing/#writing-docs","title":"Writing Docs","text":"<ul> <li>Use Markdown with admonitions</li> <li>Include code examples</li> <li>Add mermaid diagrams where helpful</li> <li>Cross-reference related pages</li> </ul>"},{"location":"guides/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> <li>Documentation: This site</li> </ul>"},{"location":"guides/contributing/#code-review-process","title":"Code Review Process","text":"<ol> <li>All PRs require review</li> <li>Address review comments</li> <li>Keep PRs focused (one feature/fix per PR)</li> <li>Maintain backward compatibility when possible</li> <li>Update tests and docs</li> </ol>"},{"location":"guides/contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the same license as the project.</p>"},{"location":"guides/detector-block-types/","title":"Block Type System Documentation","text":"<p>This document describes the standardized block type system used throughout the VLM OCR Pipeline.</p>"},{"location":"guides/detector-block-types/#overview","title":"Overview","text":"<p>All detectors in this pipeline use a unified type system that supports 25+ block types. The type system is based on comprehensive document element categories, with detector-specific types automatically mapped to standardized types for consistent processing throughout the pipeline.</p>"},{"location":"guides/detector-block-types/#implementation","title":"Implementation","text":"<ul> <li>Type definitions: <code>pipeline/types.py</code> - <code>Block</code> dataclass with <code>type: str</code> field</li> <li>Detector integration: Applied in all detectors during block creation</li> <li>Output conversion: <code>pipeline/conversion/output/markdown/__init__.py</code></li> </ul>"},{"location":"guides/detector-block-types/#type-mapping-process","title":"Type Mapping Process","text":"<ol> <li>Detector returns blocks with native type names (e.g., <code>\"plain text\"</code>, <code>\"abandon\"</code>, <code>\"figure_caption\"</code>)</li> <li>Type normalization converts to standardized type:</li> <li><code>\"plain text\"</code> \u2192 <code>\"text\"</code></li> <li><code>\"abandon\"</code> \u2192 <code>\"discarded\"</code></li> <li><code>\"figure_caption\"</code> \u2192 <code>\"image_caption\"</code></li> <li>All downstream pipeline stages use standardized types</li> <li>Markdown conversion handles all 25+ standardized types</li> </ol>"},{"location":"guides/detector-block-types/#standardized-block-types","title":"Standardized Block Types","text":"<p>Based on MinerU 2.5 VLM (<code>external/MinerU/mineru/utils/enum_class.py</code>):</p>"},{"location":"guides/detector-block-types/#content-types","title":"Content Types","text":"<ul> <li><code>text</code> - Body text paragraphs</li> <li><code>title</code> - Document/section titles</li> </ul>"},{"location":"guides/detector-block-types/#figure-types","title":"Figure Types","text":"<ul> <li><code>image</code> - Image regions</li> <li><code>image_body</code> - Image content area</li> <li><code>image_caption</code> - Image captions</li> <li><code>image_footnote</code> - Image footnotes</li> </ul>"},{"location":"guides/detector-block-types/#table-types","title":"Table Types","text":"<ul> <li><code>table</code> - Table regions</li> <li><code>table_body</code> - Table content area</li> <li><code>table_caption</code> - Table captions</li> <li><code>table_footnote</code> - Table footnotes</li> </ul>"},{"location":"guides/detector-block-types/#equation-types","title":"Equation Types","text":"<ul> <li><code>interline_equation</code> - Display equations (block-level)</li> <li><code>inline_equation</code> - Inline equations</li> </ul>"},{"location":"guides/detector-block-types/#code-types","title":"Code Types","text":"<ul> <li><code>code</code> - Code blocks</li> <li><code>code_body</code> - Code content area</li> <li><code>code_caption</code> - Code captions</li> <li><code>algorithm</code> - Algorithm pseudocode</li> </ul>"},{"location":"guides/detector-block-types/#list-types","title":"List Types","text":"<ul> <li><code>list</code> - List items</li> </ul>"},{"location":"guides/detector-block-types/#page-elements","title":"Page Elements","text":"<ul> <li><code>header</code> - Page headers (not content headings)</li> <li><code>footer</code> - Page footers</li> <li><code>page_number</code> - Page numbers</li> <li><code>page_footnote</code> - Page-level footnotes</li> </ul>"},{"location":"guides/detector-block-types/#reference-types","title":"Reference Types","text":"<ul> <li><code>ref_text</code> - Reference text</li> <li><code>phonetic</code> - Phonetic annotations</li> <li><code>aside_text</code> - Aside/sidebar text</li> <li><code>index</code> - Index entries</li> </ul>"},{"location":"guides/detector-block-types/#special-types","title":"Special Types","text":"<ul> <li><code>discarded</code> - Discarded/invalid content</li> </ul>"},{"location":"guides/detector-block-types/#detector-support-matrix","title":"Detector Support Matrix","text":"Block Type DocLayoutYOLO MinerU DocLayoutYOLO MinerU VLM 2.5 PaddleOCR PP-DocLayoutV2 Content Types <code>text</code> <code>plain text</code> <code>plain text</code> <code>text</code> <code>text</code><code>vertical_text</code><code>abstract</code><code>contents</code> <code>title</code> <code>title</code> <code>title</code> <code>title</code> <code>doc_title</code><code>paragraph_title</code> Figure Types <code>image</code> <code>figure</code> <code>figure</code> <code>image</code> <code>image</code><code>chart</code><code>seal</code> <code>image_body</code> - - <code>image_body</code> - <code>image_caption</code> - <code>figure_caption</code><code>formula_caption</code> <code>image_caption</code> <code>figure_title</code> <code>image_footnote</code> - - <code>image_footnote</code> - Table Types <code>table</code> <code>table</code> <code>table</code> <code>table</code> <code>table</code> <code>table_body</code> - - <code>table_body</code> - <code>table_caption</code> - <code>table_caption</code> <code>table_caption</code> <code>figure_title</code> <code>table_footnote</code> - <code>table_footnote</code> <code>table_footnote</code> - Equation Types <code>interline_equation</code> <code>equation</code> <code>isolate_formula</code> <code>interline_equation</code> <code>display_formula</code><code>formula_number</code> <code>inline_equation</code> - - <code>inline_equation</code> <code>inline_formula</code> Code Types <code>code</code> - - <code>code</code> - <code>code_body</code> - - <code>code_body</code> - <code>code_caption</code> - - <code>code_caption</code> - <code>algorithm</code> - - <code>algorithm</code> <code>algorithm</code> List Types <code>list</code> <code>list</code><code>list_item</code> - <code>list</code> - Page Elements <code>header</code> - - <code>header</code> <code>header</code><code>header_image</code> <code>footer</code> - - <code>footer</code> <code>footer</code><code>footer_image</code> <code>page_number</code> - - <code>page_number</code> <code>page_number</code> <code>page_footnote</code> - - <code>page_footnote</code> <code>footnote</code> Reference Types <code>ref_text</code> - - <code>ref_text</code> <code>reference</code><code>reference_content</code> <code>phonetic</code> - - <code>phonetic</code> - <code>aside_text</code> - - <code>aside_text</code> <code>aside_text</code> <code>index</code> - - <code>index</code> - Special Types <code>discarded</code> - <code>abandon</code> <code>discarded</code> - <p>Legend: - Detector-specific type names shown in backticks - <code>-</code> = Not supported by detector - Multiple types per cell separated by line breaks</p>"},{"location":"guides/detector-block-types/#detector-specifications","title":"Detector Specifications","text":""},{"location":"guides/detector-block-types/#doclayout-yolo-project","title":"DocLayout-YOLO (Project)","text":"<p>Implementation: <code>pipeline/layout/detection/doclayout_yolo.py</code></p> <p>Type System: Model-dependent (loaded from YOLO model weights)</p> <p>Common Types: - <code>title</code>, <code>plain text</code>, <code>figure</code>, <code>table</code>, <code>equation</code>, <code>list</code>, <code>list_item</code></p> <p>Notes: - Actual types depend on model's <code>class_names</code> (determined at runtime) - DocStructBench model typically includes 6-8 types</p>"},{"location":"guides/detector-block-types/#paddleocr-pp-doclayoutv2","title":"PaddleOCR PP-DocLayoutV2","text":"<p>Implementation: <code>pipeline/layout/detection/paddleocr/doclayout_v2.py</code></p> <p>Type System: Fixed 25-type system with integrated reading order</p> <p>Supported Types: <pre><code>{\n    # Titles and text\n    \"doc_title\": \"title\",\n    \"paragraph_title\": \"title\",\n    \"text\": \"text\",\n    \"vertical_text\": \"text\",\n    \"aside_text\": \"aside_text\",\n\n    # Page elements\n    \"page_number\": \"page_number\",\n    \"header\": \"header\",\n    \"footer\": \"footer\",\n    \"header_image\": \"header\",\n    \"footer_image\": \"footer\",\n\n    # Structural elements\n    \"abstract\": \"text\",\n    \"contents\": \"text\",\n    \"reference\": \"ref_text\",\n    \"reference_content\": \"ref_text\",\n    \"footnote\": \"page_footnote\",\n\n    # Math and formulas\n    \"inline_formula\": \"inline_equation\",\n    \"display_formula\": \"interline_equation\",\n    \"formula_number\": \"interline_equation\",\n    \"algorithm\": \"algorithm\",\n\n    # Visual elements\n    \"image\": \"image\",\n    \"table\": \"table\",\n    \"chart\": \"image\",\n    \"seal\": \"image\",\n\n    # Captions (unified as figure_title)\n    \"figure_title\": \"image_caption\",  # Includes figure/table/chart captions\n}\n</code></pre></p> <p>Features: - Unified model: RT-DETR-L based PP-DocLayout_plus-L (81.4 mAP) - Reading order: Built-in pointer network (6 Transformer layers) - Output is pre-sorted: Blocks already have <code>order</code> field set - 25 categories: Most comprehensive category coverage - Model size: 203.8 MB - Supports: Chinese, English, Japanese, and vertical text documents - Trained on: Papers, magazines, PPTs, contracts, and diverse document types</p> <p>Notes: - No additional sorter needed - reading order is built-in - <code>figure_title</code> applies to all captions (images, tables, charts) - Distinguishes between doc-level and paragraph-level titles</p>"},{"location":"guides/detector-block-types/#mineru-doclayout-yolo","title":"MinerU DocLayout-YOLO","text":"<p>Implementation: <code>pipeline/layout/detection/mineru/doclayout_yolo.py</code></p> <p>Type System: Fixed 10-type system</p> <p>Supported Types: <pre><code>{\n    0: \"title\",\n    1: \"plain text\",\n    2: \"abandon\",           # \u2192 discarded\n    3: \"figure\",            # \u2192 image\n    4: \"figure_caption\",    # \u2192 image_caption\n    5: \"table\",\n    6: \"table_caption\",\n    7: \"table_footnote\",\n    8: \"isolate_formula\",   # \u2192 interline_equation\n    9: \"formula_caption\",   # \u2192 image_caption\n}\n</code></pre></p>"},{"location":"guides/detector-block-types/#mineru-vlm-25","title":"MinerU VLM 2.5","text":"<p>Implementation: <code>pipeline/layout/detection/mineru/vlm.py</code></p> <p>Type System: 25 standardized types (canonical reference)</p> <p>Features: - Most comprehensive type coverage - Includes code, algorithm, and specialized academic types - Distinguishes between body/caption/footnote for images, tables, and code - Separate page elements (header, footer, page_number) - Already uses standardized types (identity mapping)</p>"},{"location":"guides/detector-block-types/#olmocr-vlm","title":"olmOCR VLM","text":"<p>Implementation: Used via sorters in <code>pipeline/layout/ordering/olmocr/</code></p> <p>Type System: Single type (<code>text</code>)</p> <p>Special Behavior: - VLM generates complete Markdown directly - Output includes YAML front matter with metadata - No block-level type distinction - All content marked as <code>text</code> type</p> <p>Example Output: <pre><code>---\nprimary_language: en\nis_rotation_valid: true\nrotation_correction: 0\n---\n\n# Chapter 1\n\nContent with **formatting**, $$equations$$, and tables.\n</code></pre></p>"},{"location":"guides/detector-block-types/#markdown-conversion-rules","title":"Markdown Conversion Rules","text":"<p>Defined in <code>pipeline/conversion/output/markdown/__init__.py</code>:</p> Type Markdown Format Example <code>title</code> <code># {text}</code> <code># Introduction</code> <code>text</code> <code>{text}</code> Plain text <code>image</code>, <code>image_body</code> <code>**Figure:** {text}</code> <code>**Figure:** Chart showing...</code> <code>image_caption</code> <code>**Figure:** {text}</code> <code>**Figure:** Monthly sales</code> <code>image_footnote</code> <code>*{text}*</code> <code>*Source: 2024 report*</code> <code>table</code>, <code>table_body</code> <code>{text}</code> or <code>**Table:**\\n\\n{text}</code> Markdown table or formatted <code>table_caption</code> <code>**Table:** {text}</code> <code>**Table:** Q1 Results</code> <code>table_footnote</code> <code>*{text}*</code> <code>*n=100*</code> <code>interline_equation</code> <code>$${text}$$</code> <code>$$E = mc^2$$</code> <code>inline_equation</code> <code>${text}$</code> <code>$x^2$</code> <code>code</code>, <code>code_body</code>, <code>algorithm</code> <code>```\\n{text}\\n```</code> Code block <code>code_caption</code> <code>**Code:** {text}</code> <code>**Code:** Algorithm 1</code> <code>list</code> <code>- {text}</code> <code>- Item</code> <code>header</code>, <code>footer</code>, <code>page_number</code> (skipped) - <code>ref_text</code> <code>{text}</code> Plain text <code>phonetic</code>, <code>aside_text</code> <code>*{text}*</code> <code>*pronunciation*</code> <code>page_footnote</code> <code>*{text}*</code> <code>*footnote*</code> <code>index</code> <code>{text}</code> Plain text <code>discarded</code> (skipped) -"},{"location":"guides/detector-block-types/#adding-new-detectors","title":"Adding New Detectors","text":"<p>To integrate a new detector with the type system:</p> <ol> <li> <p>Implement the Detector protocol (see <code>pipeline/types.py</code>):    <pre><code>class MyDetector:\n    def detect(self, image: np.ndarray) -&gt; list[Block]:\n        # Return blocks with native type names\n        pass\n</code></pre></p> </li> <li> <p>Create type mapping dictionary (if detector uses non-standard types):    <pre><code>_TYPE_MAP = {\n    \"native_type_1\": \"text\",\n    \"native_type_2\": \"image\",\n    \"native_caption\": \"image_caption\",\n    # Map all detector-specific types to standardized types\n}\n</code></pre></p> </li> <li> <p>Apply mapping in detector's block creation:    <pre><code>def detect(self, image: np.ndarray) -&gt; list[Block]:\n    # Get detections from model\n    raw_detections = self.model.predict(image)\n\n    # Convert to Block objects with standardized types\n    blocks = []\n    for det in raw_detections:\n        # Map type using _TYPE_MAP\n        standardized_type = self._TYPE_MAP.get(det.type, det.type)\n\n        block = Block(\n            type=standardized_type,\n            bbox=BBox.from_xyxy(det.x0, det.y0, det.x1, det.y1),\n            detection_confidence=det.confidence,\n            order=None,  # Set by sorter\n            column_index=None,  # Set by sorter if multi-column\n            text=None,  # Set by recognizer\n            corrected_text=None,\n            correction_ratio=None,\n            source=\"my-detector\",\n        )\n        blocks.append(block)\n\n    return blocks\n</code></pre></p> </li> <li> <p>Register in factory (see <code>pipeline/layout/detection/__init__.py</code>):    <pre><code>def create_detector(name: str, **kwargs) -&gt; Detector:\n    if name == \"my-detector\":\n        from .my_detector import MyDetector  # noqa: PLC0415\n        return MyDetector(**kwargs)\n    # ...\n</code></pre></p> </li> </ol> <p>That's it! The rest of the pipeline will automatically handle your detector's output using standardized types.</p>"},{"location":"guides/error-handling/","title":"Error Handling Guidelines","text":"<p>This document defines the error handling policy for the VLM OCR Pipeline project.</p>"},{"location":"guides/error-handling/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Error Handling Guidelines</li> <li>Table of Contents</li> <li>1. Custom Exception Hierarchy</li> <li>2. When to Use Each Exception<ul> <li>ConfigurationError</li> <li>APIError</li> <li>ProcessingError</li> <li>FileError</li> <li>DependencyError</li> </ul> </li> <li>3. Exception Handling Best Practices<ul> <li>3.1. Catch Specific Exceptions</li> <li>3.2. Exception Chaining</li> <li>3.3. When to Use Broad Exception Handlers</li> <li>3.4. Re-raising Exceptions</li> </ul> </li> <li>4. Error Logging Standards<ul> <li>4.1. Logging Levels</li> <li>4.2. Log Message Format</li> <li>4.3. Exception Stack Traces</li> </ul> </li> <li>5. Error Recovery Strategies<ul> <li>5.1. Graceful Degradation</li> <li>5.2. Retry Logic</li> <li>5.3. Fallback Values</li> </ul> </li> <li>6. Testing Error Handling</li> <li>7. Migration Guide<ul> <li>Step 1: Identify the error type</li> <li>Step 2: Choose the appropriate custom exception</li> <li>Step 3: Add proper error context</li> <li>Step 4: Update logging</li> </ul> </li> </ul>"},{"location":"guides/error-handling/#1-custom-exception-hierarchy","title":"1. Custom Exception Hierarchy","text":"<p>All custom exceptions inherit from <code>PipelineError</code> and are defined in <code>pipeline/exceptions.py</code>:</p> <pre><code>PipelineError (base)\n\u251c\u2500\u2500 ConfigurationError\n\u2502   \u251c\u2500\u2500 InvalidConfigError\n\u2502   \u2514\u2500\u2500 MissingConfigError\n\u251c\u2500\u2500 APIError\n\u2502   \u251c\u2500\u2500 APIClientError\n\u2502   \u251c\u2500\u2500 APIAuthenticationError\n\u2502   \u251c\u2500\u2500 APIRateLimitError\n\u2502   \u2514\u2500\u2500 APITimeoutError\n\u251c\u2500\u2500 ProcessingError\n\u2502   \u251c\u2500\u2500 PageProcessingError\n\u2502   \u251c\u2500\u2500 DetectionError\n\u2502   \u251c\u2500\u2500 RecognitionError\n\u2502   \u2514\u2500\u2500 RenderingError\n\u251c\u2500\u2500 FileError\n\u2502   \u251c\u2500\u2500 FileLoadError\n\u2502   \u251c\u2500\u2500 FileSaveError\n\u2502   \u2514\u2500\u2500 FileFormatError\n\u2514\u2500\u2500 DependencyError\n</code></pre> <p>Import all exceptions from: <pre><code>from pipeline.exceptions import (\n    PipelineError,\n    ConfigurationError,\n    InvalidConfigError,\n    MissingConfigError,\n    APIError,\n    APIClientError,\n    APIAuthenticationError,\n    APIRateLimitError,\n    APITimeoutError,\n    ProcessingError,\n    PageProcessingError,\n    DetectionError,\n    RecognitionError,\n    RenderingError,\n    FileError,\n    FileLoadError,\n    FileSaveError,\n    FileFormatError,\n    DependencyError,\n)\n</code></pre></p>"},{"location":"guides/error-handling/#2-when-to-use-each-exception","title":"2. When to Use Each Exception","text":""},{"location":"guides/error-handling/#configurationerror","title":"ConfigurationError","text":"<p>Use when dealing with configuration files, settings, or initialization parameters.</p> <p>InvalidConfigError: <pre><code># Example: Invalid tier name\nif tier not in [\"free\", \"tier1\", \"tier2\", \"tier3\"]:\n    raise InvalidConfigError(f\"Invalid tier: {tier}. Must be one of: free, tier1, tier2, tier3\")\n\n# Example: Malformed YAML\ntry:\n    config = yaml.safe_load(f)\nexcept yaml.YAMLError as e:\n    raise InvalidConfigError(f\"Malformed YAML in {config_file}: {e}\") from e\n</code></pre></p> <p>MissingConfigError: <pre><code># Example: Missing API key\nif not api_key:\n    raise MissingConfigError(\"OpenAI API key not found. Set OPENAI_API_KEY environment variable.\")\n\n# Example: Required config file not found\nif not config_file.exists():\n    raise MissingConfigError(f\"Configuration file not found: {config_file}\")\n</code></pre></p>"},{"location":"guides/error-handling/#apierror","title":"APIError","text":"<p>Use when interacting with external APIs (OpenAI, Gemini, PaddleOCR-VL, etc.).</p> <p>APIClientError: <pre><code># Example: Client initialization failure\ntry:\n    self.client = OpenAI(api_key=api_key, base_url=base_url)\nexcept TypeError as e:\n    raise APIClientError(f\"Failed to initialize OpenAI client: {e}\") from e\n</code></pre></p> <p>APIAuthenticationError: <pre><code># Example: Invalid API key (from external library)\ntry:\n    response = self.client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    raise APIAuthenticationError(f\"OpenAI authentication failed: {e}\") from e\n</code></pre></p> <p>APIRateLimitError: <pre><code># Example: Rate limit exceeded\ntry:\n    response = self.client.chat.completions.create(...)\nexcept RateLimitError as e:\n    raise APIRateLimitError(f\"OpenAI rate limit exceeded: {e}\") from e\n</code></pre></p> <p>APITimeoutError: <pre><code># Example: Request timeout\ntry:\n    response = requests.get(url, timeout=30)\nexcept requests.Timeout as e:\n    raise APITimeoutError(f\"API request timed out: {e}\") from e\n</code></pre></p>"},{"location":"guides/error-handling/#processingerror","title":"ProcessingError","text":"<p>Use when processing documents through the pipeline stages.</p> <p>PageProcessingError: <pre><code># Example: Page rendering failure\ntry:\n    page_image = render_pdf_page(pdf_path, page_num)\nexcept Exception as e:\n    raise PageProcessingError(f\"Failed to process page {page_num}: {e}\") from e\n</code></pre></p> <p>DetectionError: <pre><code># Example: Layout detection failure\ntry:\n    blocks = self.detector.detect(page_image)\nexcept Exception as e:\n    raise DetectionError(f\"Layout detection failed: {e}\") from e\n</code></pre></p> <p>RecognitionError: <pre><code># Example: Text recognition failure\ntry:\n    text = self.recognizer.extract_text(block_image)\nexcept Exception as e:\n    raise RecognitionError(f\"Text recognition failed for block: {e}\") from e\n</code></pre></p> <p>RenderingError: <pre><code># Example: Markdown conversion error\ntry:\n    markdown = block_to_markdown(block)\nexcept Exception as e:\n    raise RenderingError(f\"Failed to render block to markdown: {e}\") from e\n</code></pre></p>"},{"location":"guides/error-handling/#fileerror","title":"FileError","text":"<p>Use for file I/O operations.</p> <p>FileLoadError: <pre><code># Example: File not found\nif not file_path.exists():\n    raise FileLoadError(f\"File not found: {file_path}\")\n\n# Example: PDF loading failure\ntry:\n    images = convert_from_path(str(pdf_path))\nexcept Exception as e:\n    raise FileLoadError(f\"Failed to load PDF: {e}\") from e\n</code></pre></p> <p>FileSaveError: <pre><code># Example: Permission denied\ntry:\n    with open(output_file, \"w\") as f:\n        json.dump(data, f)\nexcept PermissionError as e:\n    raise FileSaveError(f\"Permission denied writing to {output_file}: {e}\") from e\n\n# Example: Disk full\nexcept OSError as e:\n    raise FileSaveError(f\"Failed to save file {output_file}: {e}\") from e\n</code></pre></p> <p>FileFormatError: <pre><code># Example: Invalid PDF\nif not is_valid_pdf(file_path):\n    raise FileFormatError(f\"Invalid PDF file: {file_path}\")\n\n# Example: Unsupported image format\nif file_path.suffix not in [\".png\", \".jpg\", \".jpeg\"]:\n    raise FileFormatError(f\"Unsupported image format: {file_path.suffix}\")\n</code></pre></p>"},{"location":"guides/error-handling/#dependencyerror","title":"DependencyError","text":"<p>Use when optional dependencies are missing or incompatible.</p> <pre><code># Example: Missing optional dependency\ntry:\n    import fitz  # PyMuPDF\nexcept ImportError as e:\n    raise DependencyError(\"PyMuPDF is required for multi-column detection. Install with: uv pip install pymupdf\") from e\n\n# Example: Incompatible version\nif version.parse(mineru.__version__) &lt; version.parse(\"0.8.0\"):\n    raise DependencyError(f\"MinerU version {mineru.__version__} is not supported. Requires &gt;= 0.8.0\")\n</code></pre>"},{"location":"guides/error-handling/#3-exception-handling-best-practices","title":"3. Exception Handling Best Practices","text":""},{"location":"guides/error-handling/#31-catch-specific-exceptions","title":"3.1. Catch Specific Exceptions","text":"<p>\u2705 Good: <pre><code>try:\n    config = yaml.safe_load(f)\nexcept yaml.YAMLError as e:\n    raise InvalidConfigError(f\"Malformed YAML: {e}\") from e\nexcept OSError as e:\n    raise FileLoadError(f\"Failed to read config file: {e}\") from e\n</code></pre></p> <p>\u274c Bad: <pre><code>try:\n    config = yaml.safe_load(f)\nexcept Exception as e:  # Too broad!\n    logger.error(\"Error: %s\", e)\n</code></pre></p>"},{"location":"guides/error-handling/#32-exception-chaining","title":"3.2. Exception Chaining","text":"<p>Always use <code>from e</code> to preserve the original exception context:</p> <p>\u2705 Good: <pre><code>try:\n    response = self.client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    raise APIAuthenticationError(f\"Authentication failed: {e}\") from e\n</code></pre></p> <p>\u274c Bad: <pre><code>try:\n    response = self.client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    raise APIAuthenticationError(f\"Authentication failed: {e}\")  # Lost context!\n</code></pre></p>"},{"location":"guides/error-handling/#33-when-to-use-broad-exception-handlers","title":"3.3. When to Use Broad Exception Handlers","text":"<p>Broad exception handlers (<code>except Exception</code>) are only allowed in these cases:</p> <ol> <li> <p>Top-level CLI entry points (with <code># noqa: BLE001</code> comment):    <pre><code># main.py\ntry:\n    result = pipeline.process(pdf_path)\nexcept Exception as exc:  # noqa: BLE001 - retain broad logging for CLI\n    logger.error(\"Unexpected error: %s\", exc, exc_info=True)\n    return 1\n</code></pre></p> </li> <li> <p>Optional dependency guards (with <code># pragma: no cover</code> comment):    <pre><code>try:\n    import fitz\nexcept Exception:  # pragma: no cover - optional dependency guard\n    fitz = None\n</code></pre></p> </li> <li> <p>Fallback for unexpected errors (after catching specific errors):    <pre><code>try:\n    response = self.client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    raise APIAuthenticationError(f\"Authentication failed: {e}\") from e\nexcept RateLimitError as e:\n    raise APIRateLimitError(f\"Rate limit exceeded: {e}\") from e\nexcept Exception as e:\n    # Fallback for unexpected errors (document why!)\n    logger.error(\"Unexpected API error: %s\", e)\n    return {\"error\": \"api_error\", \"message\": str(e)}\n</code></pre></p> </li> </ol>"},{"location":"guides/error-handling/#34-re-raising-exceptions","title":"3.4. Re-raising Exceptions","text":"<p>When you want to log an error but still propagate it:</p> <pre><code>try:\n    page_result = self._process_pdf_page(pdf_path, page_num)\nexcept PageProcessingError as e:\n    logger.error(\"Page %d processing failed: %s\", page_num, e)\n    raise  # Re-raise the same exception\n</code></pre>"},{"location":"guides/error-handling/#4-error-logging-standards","title":"4. Error Logging Standards","text":""},{"location":"guides/error-handling/#41-logging-levels","title":"4.1. Logging Levels","text":"<p>Use appropriate logging levels:</p> Level When to Use Example <code>DEBUG</code> Detailed diagnostic information <code>logger.debug(\"Processing block %d of %d\", i, total)</code> <code>INFO</code> General informational messages <code>logger.info(\"Loaded %d pages from %s\", len(pages), pdf_path)</code> <code>WARNING</code> Recoverable errors, fallback used <code>logger.warning(\"PyMuPDF not available, using basic sorter\")</code> <code>ERROR</code> Serious errors, operation failed <code>logger.error(\"Failed to process page %d: %s\", page_num, e)</code> <code>CRITICAL</code> Critical errors, system cannot continue <code>logger.critical(\"API key not found, cannot proceed\")</code>"},{"location":"guides/error-handling/#42-log-message-format","title":"4.2. Log Message Format","text":"<p>Format: <code>\"Action failed: %s\", error_details</code></p> <p>\u2705 Good: <pre><code>logger.error(\"Failed to load config file %s: %s\", config_path, e)\nlogger.warning(\"Rate limit reached. Waiting %.2f seconds...\", wait_time)\nlogger.info(\"Processed %d pages in %.2f seconds\", page_count, elapsed)\n</code></pre></p> <p>\u274c Bad: <pre><code>logger.error(f\"Failed to load config file {config_path}: {e}\")  # Don't use f-strings!\nlogger.error(\"Error!\")  # Not descriptive!\nlogger.error(str(e))  # Missing context!\n</code></pre></p> <p>Why avoid f-strings in logging? - f-strings are evaluated before the log level check (performance overhead) - <code>%s</code> formatting is only evaluated if the log level is enabled - Better for structured logging</p>"},{"location":"guides/error-handling/#43-exception-stack-traces","title":"4.3. Exception Stack Traces","text":"<p>Use <code>exc_info=True</code> to include full stack trace:</p> <p>\u2705 Good: <pre><code>try:\n    result = process_page(page_num)\nexcept PageProcessingError as e:\n    logger.error(\"Error processing page %d: %s\", page_num, e, exc_info=True)\n</code></pre></p> <p>When to use <code>exc_info=True</code>? - For unexpected errors at top-level handlers - When debugging complex issues - For errors that should never happen</p> <p>When NOT to use <code>exc_info=True</code>? - For expected errors (rate limits, missing files) - For informational warnings - When stack trace would be too verbose</p>"},{"location":"guides/error-handling/#5-error-recovery-strategies","title":"5. Error Recovery Strategies","text":""},{"location":"guides/error-handling/#51-graceful-degradation","title":"5.1. Graceful Degradation","text":"<p>Continue processing with reduced functionality:</p> <pre><code>try:\n    import fitz  # PyMuPDF\nexcept ImportError:\n    logger.warning(\"PyMuPDF not available. Multi-column detection disabled.\")\n    fitz = None\n\n# Later in code\nif fitz is not None:\n    # Use advanced multi-column detection\n    layout = detect_multi_column_layout(page)\nelse:\n    # Fallback to basic processing\n    layout = None\n</code></pre>"},{"location":"guides/error-handling/#52-retry-logic","title":"5.2. Retry Logic","text":"<p>Retry on transient failures:</p> <pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n    reraise=True\n)\ndef api_call_with_retry():\n    try:\n        return self.client.chat.completions.create(...)\n    except APITimeoutError:\n        logger.warning(\"API timeout, retrying...\")\n        raise\n</code></pre>"},{"location":"guides/error-handling/#53-fallback-values","title":"5.3. Fallback Values","text":"<p>Return safe defaults on error:</p> <pre><code>def load_config(config_path: Path) -&gt; dict[str, Any]:\n    \"\"\"Load configuration file with fallback to empty dict.\"\"\"\n    try:\n        with open(config_path) as f:\n            return yaml.safe_load(f)\n    except FileNotFoundError:\n        logger.debug(\"Config file not found: %s\", config_path)\n        return {}\n    except yaml.YAMLError as e:\n        logger.warning(\"Failed to parse config file %s: %s\", config_path, e)\n        return {}\n</code></pre>"},{"location":"guides/error-handling/#6-testing-error-handling","title":"6. Testing Error Handling","text":"<p>Always test error paths:</p> <pre><code>def test_invalid_tier_raises_error():\n    \"\"\"Test that InvalidConfigError is raised for invalid tier.\"\"\"\n    with pytest.raises(InvalidConfigError, match=\"Invalid tier\"):\n        rate_limiter.set_tier_and_model(\"invalid_tier\", \"gemini-2.5-flash\")\n\ndef test_missing_api_key_raises_error():\n    \"\"\"Test that MissingConfigError is raised when API key is missing.\"\"\"\n    with pytest.raises(MissingConfigError, match=\"API key not found\"):\n        OpenAIClient(api_key=None)\n\ndef test_file_not_found_raises_error():\n    \"\"\"Test that FileLoadError is raised for non-existent file.\"\"\"\n    with pytest.raises(FileLoadError, match=\"File not found\"):\n        load_pdf(\"/nonexistent/file.pdf\")\n</code></pre>"},{"location":"guides/error-handling/#7-migration-guide","title":"7. Migration Guide","text":"<p>When migrating from <code>except Exception</code> to specific exceptions:</p>"},{"location":"guides/error-handling/#step-1-identify-the-error-type","title":"Step 1: Identify the error type","text":"<p>Look at what can go wrong in the try block: <pre><code># Before\ntry:\n    config = yaml.safe_load(f)\nexcept Exception as e:\n    logger.error(\"Error: %s\", e)\n    return {}\n</code></pre></p>"},{"location":"guides/error-handling/#step-2-choose-the-appropriate-custom-exception","title":"Step 2: Choose the appropriate custom exception","text":"<ul> <li>YAML parsing error \u2192 <code>InvalidConfigError</code></li> <li>File I/O error \u2192 <code>FileLoadError</code></li> </ul>"},{"location":"guides/error-handling/#step-3-add-proper-error-context","title":"Step 3: Add proper error context","text":"<pre><code># After\ntry:\n    config = yaml.safe_load(f)\nexcept yaml.YAMLError as e:\n    raise InvalidConfigError(f\"Malformed YAML in {config_file}: {e}\") from e\nexcept OSError as e:\n    raise FileLoadError(f\"Failed to read config file {config_file}: {e}\") from e\n</code></pre>"},{"location":"guides/error-handling/#step-4-update-logging","title":"Step 4: Update logging","text":"<pre><code># Caller code\ntry:\n    config = load_config(config_path)\nexcept InvalidConfigError as e:\n    logger.error(\"Configuration error: %s\", e)\n    return {}\nexcept FileLoadError as e:\n    logger.warning(\"Config file not found: %s\", e)\n    return {}\n</code></pre> <p>Last Updated: 2025-01-26</p> <p>See Also: - <code>pipeline/exceptions.py</code> - Custom exception definitions - <code>CLAUDE.md</code> - Project coding standards - <code>.cursorrules</code> - Detailed coding guidelines</p>"},{"location":"reference/cli/","title":"CLI Reference","text":"<p>Complete command-line interface documentation.</p>"},{"location":"reference/cli/#main-command","title":"Main Command","text":"<pre><code>python main.py [OPTIONS]\n</code></pre>"},{"location":"reference/cli/#options","title":"Options","text":"<p>For complete CLI documentation, see Basic Usage.</p>"},{"location":"reference/cli/#input","title":"Input","text":"<ul> <li><code>--input PATH</code>: Input file or directory</li> <li><code>--max-pages N</code>: Process first N pages</li> <li><code>--page-range START-END</code>: Process page range</li> <li><code>--pages LIST</code>: Process specific pages (comma-separated)</li> </ul>"},{"location":"reference/cli/#backend","title":"Backend","text":"<ul> <li><code>--backend {openai,gemini}</code>: VLM backend</li> <li><code>--model MODEL</code>: Model name</li> <li><code>--gemini-tier {free,tier1,tier2,tier3}</code>: Gemini API tier</li> </ul>"},{"location":"reference/cli/#components","title":"Components","text":"<ul> <li><code>--detector NAME</code>: Layout detector</li> <li><code>--sorter NAME</code>: Reading order sorter</li> <li><code>--recognizer NAME</code>: Text recognizer</li> </ul>"},{"location":"reference/cli/#output","title":"Output","text":"<ul> <li><code>--output DIR</code>: Output directory</li> <li><code>--cache-dir DIR</code>: Cache directory</li> <li><code>--temp-dir DIR</code>: Temporary files directory</li> <li><code>--no-cache</code>: Disable caching</li> </ul>"},{"location":"reference/cli/#other","title":"Other","text":"<ul> <li><code>--dpi N</code>: PDF rendering DPI (default: 200)</li> <li><code>-v, --verbose</code>: Verbose output</li> <li><code>-vv</code>: Very verbose (debug)</li> <li><code>--rate-limit-status</code>: Check Gemini rate limits</li> </ul> <p>Note</p> <p>Run <code>python main.py --help</code> for complete option list.</p>"}]}